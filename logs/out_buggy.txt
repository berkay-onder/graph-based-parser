bash-4.2$ 
bash-4.2$ pwd
/scratch/users/cgumeli/Nubik
bash-4.2$ cd ..
bash-4.2$ source .bashrc
[cgumeli@login03 cgumeli]$ srg
bash-4.2$ pwd
pwd
/scratch/users/cgumeli
bash-4.2$ cd Nubik
cd Nubik
bash-4.2$ julia
julia
               _
   _       _ _(_)_     |  A fresh approach to technical computing
  (_)     | (_) (_)    |  Documentation: https://docs.julialang.org
   _ _   _| |_  __ _   |  Type "?help" for help.
  | | | | | | |/ _` |  |
  | | |_| | | | (_| |  |  Version 0.6.2 (2017-12-13 18:08 UTC)
 _/ |\__'_|_|_|\__'_|  |  Official http://julialang.org/ release
|__/                   |  x86_64-pc-linux-gnu

WARNING: Terminal not fully functional
julia> include("main2.jl")
include("main2.jl")
INFO: Checking dependencies
INFO: All dependencies are installed
accuracy_bt

julia> 


julia> main("--datafiles $engtrn $engdev --lmfile $englm --feat 150 --lmdrop 0.33 --birnndrop 0.33 --decdrop 0.33 --tokens 500
main("--datafiles $engtrn $engdev --lmfile $englm --feat 150 --lmdrop 0.33 --birnndrop 0.33 --decdrop 0.33 --tokens 500")
")
Tuple{Symbol,Any}[(:feat, 150), (:birnn, 1), (:birnndrop, 0.33), (:lmfile, "/scratch/users/okirnap/ud-treebanks-v2.2/chmodel_converted/english_chmodel.jld"), (:min, 2), (:birnncell, "LSTM"), (:labelunits, 100), (:xpos, 0), (:batchsize, 8), (:max, 128), (:decdrop, 0.33), (:upos, 0), (:backupdir, "Backups_2018-06-08T19:14:56.213"), (:beta1, 0.9), (:remb, 350), (:algo, "edmonds"), (:beta2, 0.999), (:lmdrop, 0.33), (:proj, true), (:tokens, 500), (:epochs, 100), (:recover, ""), (:birnnhidden, 200), (:optim, "Adam"), (:lr, 0.001), (:datafiles, Any["/scratch/users/okirnap/ud-treebanks-v2.2/UD_English-EWT/en_ewt-ud-train.conllu", "/scratch/users/okirnap/ud-treebanks-v2.2/UD_English-EWT/en_ewt-ud-dev.conllu"]), (:arcunits, 400)]
INFO: Loading data
19:14:59 /scratch/users/okirnap/ud-treebanks-v2.2/chmodel_converted/english_chmodel.jld 
19:15:04 /scratch/users/okirnap/ud-treebanks-v2.2/UD_English-EWT/en_ewt-ud-train.conllu 
19:15:06 /scratch/users/okirnap/ud-treebanks-v2.2/UD_English-EWT/en_ewt-ud-dev.conllu 
19:16:03 Caching lm vectors... 
INFO: Bucketing
INFO: Initializing Model and Optimizers
ERROR: MethodError: no method matching rnninit(::Int64, ::Int64; usegpu=false, rnnType=:lstm, bidirectional=true, feat=150, upos=0, xpos=0, dropout=0.33f0, numLayers=1)
Closest candidates are:
  rnninit(::Any, ::Any; handle, numLayers, dropout, skipInput, bidirectional, rnnType, dataType, algo, seed, winit, binit, usegpu) at /scratch/users/cgumeli/.julia/v0.6/Knet/src/rnn.jl:361 got unsupported keyword arguments "feat", "upos", "xpos"
Stacktrace:
 [1] (::Knet.#kw##rnninit)(::Array{Any,1}, ::Knet.#rnninit, ::Int64, ::Int64) at ./<missing>:0
 [2] #LSTM#54(::Bool, ::Bool, ::Bool, ::Array{Any,1}, ::Type{T} where T, ::Int64, ::Int64) at /scratch/users/cgumeli/.julia/v0.6/KnetModules/src/rnn.jl:42
 [3] (::Core.#kw#Type)(::Array{Any,1}, ::Type{KnetModules.LSTM}, ::Int64, ::Int64) at ./<missing>:0
 [4] #FreshEncoder#100(::Int64, ::Bool, ::Int64, ::Int64, ::Int64, ::Int64, ::Float32, ::Type{T} where T, ::Array{Any,1}, ::Type{T} where T, ::ExtendedVocab) at /scratch/users/cgumeli/Nubik/./encoders/fresh.jl:19
 [5] (::Core.#kw#Type)(::Array{Any,1}, ::Type{FreshEncoder}, ::ExtendedVocab) at ./<missing>:0
 [6] train() at /scratch/users/cgumeli/Nubik/main2.jl:94
 [7] main(::String) at /scratch/users/cgumeli/Nubik/main2.jl:83

julia> main("--datafiles $engtrn $engdev --lmfile $englm --feat 150 --lmdrop 0.33 --birnndrop 0.33 --decdrop 0.33 --tokens 500  C-c C-c^C
julia> train()
train()
INFO: Initializing Model and Optimizers
ERROR: MethodError: no method matching rnninit(::Int64, ::Int64; usegpu=false, rnnType=:lstm, bidirectional=true, feat=150, upos=0, xpos=0, dropout=0.33f0, numLayers=1)
Closest candidates are:
  rnninit(::Any, ::Any; handle, numLayers, dropout, skipInput, bidirectional, rnnType, dataType, algo, seed, winit, binit, usegpu) at /scratch/users/cgumeli/.julia/v0.6/Knet/src/rnn.jl:361 got unsupported keyword arguments "feat", "upos", "xpos"
Stacktrace:
 [1] (::Knet.#kw##rnninit)(::Array{Any,1}, ::Knet.#rnninit, ::Int64, ::Int64) at ./<missing>:0
 [2] #LSTM#54(::Bool, ::Bool, ::Bool, ::Array{Any,1}, ::Type{T} where T, ::Int64, ::Int64) at /scratch/users/cgumeli/.julia/v0.6/KnetModules/src/rnn.jl:42
 [3] (::Core.#kw#Type)(::Array{Any,1}, ::Type{KnetModules.LSTM}, ::Int64, ::Int64) at ./<missing>:0
 [4] #FreshEncoder#100(::Int64, ::Bool, ::Int64, ::Int64, ::Int64, ::Int64, ::Float32, ::Type{T} where T, ::Array{Any,1}, ::Type{T} where T, ::ExtendedVocab) at /scratch/users/cgumeli/Nubik/./encoders/fresh.jl:19
 [5] (::Core.#kw#Type)(::Array{Any,1}, ::Type{FreshEncoder}, ::ExtendedVocab) at ./<missing>:0
 [6] train() at /scratch/users/cgumeli/Nubik/main2.jl:94

julia> include("main2.jl")
include("main2.jl")
INFO: Checking dependencies
INFO: All dependencies are installed
WARNING: redefining constant UPOSTAG
WARNING: redefining constant UDEPREL
WARNING: redefining constant lmdir
WARNING: redefining constant datadir
WARNING: redefining constant engtrn
WARNING: redefining constant engdev
WARNING: redefining constant englm
accuracy_bt

julia> train()
train()
INFO: Initializing Model and Optimizers
INFO: Transfering to gpu.
INFO: Setting momentums
INFO: Computing validation performance
ERROR: MethodError: no method matching *(::Knet.KnetArray{Float32,2}, ::Array{Float32,2})
Closest candidates are:
  *(::Any, ::Any, !Matched::Any, !Matched::Any...) at operators.jl:424
  *(!Matched::Type{AutoGrad.Grad{1}}, ::Any, !Matched::Any, !Matched::AutoGrad.Rec{##1109<:Number}, !Matched::AutoGrad.Rec{##1110<:Number}) where {##1109<:Number, ##1110<:Number} at :0
  *(!Matched::Type{AutoGrad.Grad{1}}, ::Any, !Matched::Any, !Matched::##1109<:Number, !Matched::AutoGrad.Rec{##1110<:Number}) where {##1109<:Number, ##1110<:Number} at :0
  ...
Stacktrace:
 [1] (::KnetModules.Linear)(::Array{Any,1}, ::Array{Float32,2}) at /scratch/users/cgumeli/.julia/v0.6/KnetModules/src/linear.jl:42
 [2] (::FreshEncoder)(::Array{Any,1}, ::Array{Any,1}) at /scratch/users/cgumeli/Nubik/./encoders/fresh.jl:38
 [3] predict at /scratch/users/cgumeli/Nubik/main2.jl:13 [inlined]
 [4] macro expansion at /scratch/users/cgumeli/Nubik/main2.jl:150 [inlined]
 [5] macro expansion at ./util.jl:237 [inlined]
 [6] train() at /scratch/users/cgumeli/Nubik/main2.jl:148

julia> include("main2.jl")
include("main2.jl")
INFO: Checking dependencies
INFO: All dependencies are installed
WARNING: redefining constant UPOSTAG
WARNING: redefining constant UDEPREL
WARNING: redefining constant lmdir
WARNING: redefining constant datadir
WARNING: redefining constant engtrn
WARNING: redefining constant engdev
WARNING: redefining constant englm
accuracy_bt

julia> train()
train()
INFO: Initializing Model and Optimizers
INFO: Transfering to gpu.
INFO: Setting momentums
INFO: Computing validation performance
ERROR: DimensionMismatch("((400, 500), (600, 274), (400, 274))")
Stacktrace:
 [1] gemm!(::Char, ::Char, ::Float32, ::Knet.KnetArray{Float32,2}, ::Knet.KnetArray{Float32,2}, ::Float32, ::Knet.KnetArray{Float32,2}) at /scratch/users/cgumeli/.julia/v0.6/Knet/src/linalg.jl:37
 [2] *(::Knet.KnetArray{Float32,2}, ::Knet.KnetArray{Float32,2}) at /scratch/users/cgumeli/.julia/v0.6/Knet/src/linalg.jl:10
 [3] (::KnetModules.Linear)(::Array{Any,1}, ::Knet.KnetArray{Float32,2}) at /scratch/users/cgumeli/.julia/v0.6/KnetModules/src/linear.jl:42
 [4] (::FreshEncoder)(::Array{Any,1}, ::Array{Any,1}) at /scratch/users/cgumeli/Nubik/./encoders/fresh.jl:38
 [5] predict at /scratch/users/cgumeli/Nubik/main2.jl:13 [inlined]
 [6] macro expansion at /scratch/users/cgumeli/Nubik/main2.jl:150 [inlined]
 [7] macro expansion at ./util.jl:237 [inlined]
 [8] train() at /scratch/users/cgumeli/Nubik/main2.jl:148

julia> include("main2.jl")
include("main2.jl")
INFO: Checking dependencies
INFO: All dependencies are installed
WARNING: redefining constant UPOSTAG
WARNING: redefining constant UDEPREL
WARNING: redefining constant lmdir
WARNING: redefining constant datadir
WARNING: redefining constant engtrn
WARNING: redefining constant engdev
WARNING: redefining constant englm
ERROR: LoadError: LoadError: LoadError: invalid redefinition of constant FreshEncoder
Stacktrace:
 [1] include_from_node1(::String) at ./loading.jl:576
 [2] include(::String) at ./sysimg.jl:14
 [3] include_from_node1(::String) at ./loading.jl:576
 [4] include(::String) at ./sysimg.jl:14
 [5] include_from_node1(::String) at ./loading.jl:576
 [6] include(::String) at ./sysimg.jl:14
while loading /scratch/users/cgumeli/Nubik/./encoders/fresh.jl, in expression starting on line 4
while loading /scratch/users/cgumeli/Nubik/./encoders/index.jl, in expression starting on line 3
while loading /scratch/users/cgumeli/Nubik/main2.jl, in expression starting on line 7

julia> exit()
exit()
bash-4.2$ julia
julia
               _
   _       _ _(_)_     |  A fresh approach to technical computing
  (_)     | (_) (_)    |  Documentation: https://docs.julialang.org
   _ _   _| |_  __ _   |  Type "?help" for help.
  | | | | | | |/ _` |  |
  | | |_| | | | (_| |  |  Version 0.6.2 (2017-12-13 18:08 UTC)
 _/ |\__'_|_|_|\__'_|  |  Official http://julialang.org/ release
|__/                   |  x86_64-pc-linux-gnu

WARNING: Terminal not fully functional
julia> readdir()
readdir()
12-element Array{String,1}:
 "Backups_2018-06-08T19:14:56.213"
 "data.jl"                        
 "decoders"                       
 "encoders"                       
 ".git"                           
 "install_deps.jl"                
 "ku-dependency-parser2"          
 "LICENSE"                        
 "main2.jl"                       
 "main2.jl~"                      
 "main.jl"                        
 "parsers"                        

julia> main("--datafiles $engtrn $engdev --lmfile $englm --feat 150 --lmdrop 0.33 --birnndrop 0.33 --decdrop 0.33 --tokens 500")
main("--datafiles $engtrn $engdev --lmfile $englm --feat 150 --lmdrop 0.33 --birnndrop 0.33 --decdrop 0.33 --tokens 500")
ERROR: UndefVarError: main not defined

julia> main("--datafiles $engtrn $engdev --lmfile $englm --feat 150 --lmdrop 0.33 --birnndrop 0.33 --decdrop 0.33 --tokens 500")  C-c C-c^C
julia> include("main2.jl")
include("main2.jl")
INFO: Checking dependencies
INFO: All dependencies are installed
accuracy_bt

julia> main("--datafiles $engtrn $engdev --lmfile $englm --feat 150 --lmdrop 0.33 --birnndrop 0.33 --decdrop 0.33 --tokens 500")
main("--datafiles $engtrn $engdev --lmfile $englm --feat 150 --lmdrop 0.33 --birnndrop 0.33 --decdrop 0.33 --tokens 500")
Tuple{Symbol,Any}[(:feat, 150), (:birnn, 1), (:birnndrop, 0.33), (:lmfile, "/scratch/users/okirnap/ud-treebanks-v2.2/chmodel_converted/english_chmodel.jld"), (:min, 2), (:birnncell, "LSTM"), (:labelunits, 100), (:xpos, 0), (:batchsize, 8), (:max, 128), (:decdrop, 0.33), (:upos, 0), (:backupdir, "Backups_2018-06-08T19:31:30.674"), (:beta1, 0.9), (:remb, 350), (:algo, "edmonds"), (:beta2, 0.999), (:lmdrop, 0.33), (:proj, true), (:tokens, 500), (:epochs, 100), (:recover, ""), (:birnnhidden, 200), (:optim, "Adam"), (:lr, 0.001), (:datafiles, Any["/scratch/users/okirnap/ud-treebanks-v2.2/UD_English-EWT/en_ewt-ud-train.conllu", "/scratch/users/okirnap/ud-treebanks-v2.2/UD_English-EWT/en_ewt-ud-dev.conllu"]), (:arcunits, 400)]
INFO: Loading data
19:31:34 /scratch/users/okirnap/ud-treebanks-v2.2/chmodel_converted/english_chmodel.jld 
19:31:39 /scratch/users/okirnap/ud-treebanks-v2.2/UD_English-EWT/en_ewt-ud-train.conllu 
19:31:40 /scratch/users/okirnap/ud-treebanks-v2.2/UD_English-EWT/en_ewt-ud-dev.conllu 
19:32:37 Caching lm vectors... 
INFO: Bucketing
INFO: Initializing Model and Optimizers
INFO: Transfering to gpu.
INFO: Setting momentums
INFO: Computing validation performance
parsers/mst.py:24: RuntimeWarning: divide by zero encountered in divide
  new_root = tokens[np.argmax(root_scores / head_scores)]
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 98.550588 seconds (33.23 M allocations: 3.713 GiB, 2.46% gc time)
Validation loss: 86.68212
Unlabeled attachment score: 3.8526030022357074
Labeled attachment score: 0.24752475247524752

Epoch 1
ERROR: MethodError: Cannot `convert` an object of type AutoGrad.Rec{Knet.KnetArray{Float32,3}} to an object of type Knet.KnetArray
This may have arisen from a call to the constructor Knet.KnetArray(...),
since type constructors fall back to convert methods.
Stacktrace:
 [1] Knet.KnetArray(::AutoGrad.Rec{Knet.KnetArray{Float32,3}}) at ./sysimg.jl:77
 [2] (::FreshEncoder)(::AutoGrad.Rec{Array{Any,1}}, ::Array{Any,1}) at /scratch/users/cgumeli/Nubik/./encoders/fresh.jl:41
 [3] predict at /scratch/users/cgumeli/Nubik/main2.jl:13 [inlined]
 [4] loss(::AutoGrad.Rec{Array{Any,1}}, ::FreshEncoder, ::BiaffineDecoder2, ::Array{Any,1}) at /scratch/users/cgumeli/Nubik/main2.jl:19
 [5] forward_pass(::Function, ::Tuple{Array{Any,1},FreshEncoder,BiaffineDecoder2,Array{Any,1}}, ::Array{Any,1}, ::Int64) at /scratch/users/cgumeli/.julia/v0.6/AutoGrad/src/core.jl:88
 [6] (::AutoGrad.##gradfun#4#6{#loss,Int64})(::Array{Any,1}, ::Function, ::Array{Any,1}, ::Vararg{Any,N} where N) at /scratch/users/cgumeli/.julia/v0.6/AutoGrad/src/core.jl:57
 [7] (::AutoGrad.#gradfun#5)(::Array{Any,1}, ::Vararg{Any,N} where N) at /scratch/users/cgumeli/.julia/v0.6/AutoGrad/src/core.jl:57
 [8] macro expansion at /scratch/users/cgumeli/Nubik/main2.jl:192 [inlined]
 [9] macro expansion at ./util.jl:237 [inlined]
 [10] train() at /scratch/users/cgumeli/Nubik/main2.jl:190
 [11] main(::String) at /scratch/users/cgumeli/Nubik/main2.jl:83

julia> include("main2.jl")
include("main2.jl")
INFO: Checking dependencies
INFO: All dependencies are installed
WARNING: redefining constant UPOSTAG
WARNING: redefining constant UDEPREL
WARNING: redefining constant lmdir
WARNING: redefining constant datadir
WARNING: redefining constant engtrn
WARNING: redefining constant engdev
WARNING: redefining constant englm
accuracy_bt

julia> train()
train()
INFO: Initializing Model and Optimizers
INFO: Transfering to gpu.
INFO: Setting momentums
INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
  9.367061 seconds (15.39 M allocations: 2.897 GiB, 11.55% gc time)
Validation loss: 86.772835
Unlabeled attachment score: 6.906739061002875
Labeled attachment score: 0.04391568189076972

Epoch 1
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
202.908455 seconds (172.18 M allocations: 26.030 GiB, 16.45% gc time)
Training loss: 39.68507


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 12.230328 seconds (15.01 M allocations: 2.879 GiB, 10.88% gc time)
Validation loss: 17.549297
Unlabeled attachment score: 69.00351325455127
Labeled attachment score: 63.40625998083679
INFO: Backing up

Epoch 2
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
179.624527 seconds (165.96 M allocations: 25.649 GiB, 19.44% gc time)
Training loss: 26.758057


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 18.485850 seconds (15.34 M allocations: 2.885 GiB, 24.11% gc time)
Validation loss: 14.737856
Unlabeled attachment score: 72.76030022357074
Labeled attachment score: 68.70009581603321
INFO: Backing up

Epoch 3
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
219.827470 seconds (166.00 M allocations: 25.647 GiB, 29.55% gc time)
Training loss: 23.648636


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 12.776392 seconds (15.01 M allocations: 2.879 GiB, 11.02% gc time)
Validation loss: 13.916336
Unlabeled attachment score: 75.44714148834238
Labeled attachment score: 71.49073778345577
INFO: Backing up

Epoch 4
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
218.674921 seconds (165.91 M allocations: 25.654 GiB, 27.56% gc time)
Training loss: 22.0664


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 23.454878 seconds (15.34 M allocations: 2.885 GiB, 32.01% gc time)
Validation loss: 12.857707
Unlabeled attachment score: 76.82050463110828
Labeled attachment score: 73.01580964548067
INFO: Backing up

Epoch 5
10 / 380
  C-c C-c^CERROR: InterruptException:
Stacktrace:
 [1] Knet.KnetPtr(::Int64) at /scratch/users/cgumeli/.julia/v0.6/Knet/src/kptr.jl:80
 [2] Type at /scratch/users/cgumeli/.julia/v0.6/Knet/src/karray.jl:104 [inlined]
 [3] Type at /scratch/users/cgumeli/.julia/v0.6/Knet/src/karray.jl:116 [inlined]
 [4] similar at /scratch/users/cgumeli/.julia/v0.6/Knet/src/karray.jl:172 [inlined]
 [5] similar at /scratch/users/cgumeli/.julia/v0.6/Knet/src/karray.jl:175 [inlined]
 [6] zeros(::Knet.KnetArray{Float32,2}) at /scratch/users/cgumeli/.julia/v0.6/Knet/src/karray.jl:182
 [7] zeroslike(::Knet.KnetArray{Float32,2}) at /scratch/users/cgumeli/.julia/v0.6/Knet/src/karray.jl:1027
 [8] sum_outgrads(::Void, ::AutoGrad.UngetIndex) at /scratch/users/cgumeli/.julia/v0.6/AutoGrad/src/interfaces.jl:73
 [9] backward_pass(::AutoGrad.Rec{Array{Any,1}}, ::AutoGrad.Rec{Float32}, ::Array{AutoGrad.Node,1}) at /scratch/users/cgumeli/.julia/v0.6/AutoGrad/src/core.jl:258
 [10] (::AutoGrad.##gradfun#4#6{#loss,Int64})(::Array{Any,1}, ::Function, ::Array{Any,1}, ::Vararg{Any,N} where N) at /scratch/users/cgumeli/.julia/v0.6/AutoGrad/src/core.jl:62
 [11] (::AutoGrad.#gradfun#5)(::Array{Any,1}, ::Vararg{Any,N} where N) at /scratch/users/cgumeli/.julia/v0.6/AutoGrad/src/core.jl:57
 [12] macro expansion at /scratch/users/cgumeli/Nubik/main2.jl:192 [inlined]
 [13] macro expansion at ./util.jl:237 [inlined]
 [14] train() at /scratch/users/cgumeli/Nubik/main2.jl:190

julia> main("--datafiles $engtrn $engdev --lmfile $englm --feat 150 --lmdrop 0.33 --birnndrop 0.33 --decdrop 0.33 --tokens 500 --birnn 3")
main("--datafiles $engtrn $engdev --lmfile $englm --feat 150 --lmdrop 0.33 --birnndrop 0.33 --decdrop 0.33 --tokens 500 --birnn 3")
Tuple{Symbol,Any}[(:feat, 150), (:birnn, 3), (:birnndrop, 0.33), (:lmfile, "/scratch/users/okirnap/ud-treebanks-v2.2/chmodel_converted/english_chmodel.jld"), (:min, 2), (:birnncell, "LSTM"), (:labelunits, 100), (:xpos, 0), (:batchsize, 8), (:max, 128), (:decdrop, 0.33), (:upos, 0), (:backupdir, "Backups_2018-06-08T19:51:17.916"), (:beta1, 0.9), (:remb, 350), (:algo, "edmonds"), (:beta2, 0.999), (:lmdrop, 0.33), (:proj, true), (:tokens, 500), (:epochs, 100), (:recover, ""), (:birnnhidden, 200), (:optim, "Adam"), (:lr, 0.001), (:datafiles, Any["/scratch/users/okirnap/ud-treebanks-v2.2/UD_English-EWT/en_ewt-ud-train.conllu", "/scratch/users/okirnap/ud-treebanks-v2.2/UD_English-EWT/en_ewt-ud-dev.conllu"]), (:arcunits, 400)]
INFO: Loading data
19:51:18 /scratch/users/okirnap/ud-treebanks-v2.2/chmodel_converted/english_chmodel.jld 
19:51:20 /scratch/users/okirnap/ud-treebanks-v2.2/UD_English-EWT/en_ewt-ud-train.conllu 
19:51:22 /scratch/users/okirnap/ud-treebanks-v2.2/UD_English-EWT/en_ewt-ud-dev.conllu 
19:52:45 Caching lm vectors... 
INFO: Bucketing
INFO: Initializing Model and Optimizers
INFO: Transfering to gpu.
INFO: Setting momentums
INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 17.076608 seconds (15.01 M allocations: 2.879 GiB, 25.72% gc time)
Validation loss: 86.59675
Unlabeled attachment score: 5.309805174065794
Labeled attachment score: 0.09182369849888215

Epoch 1
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
232.912381 seconds (165.74 M allocations: 25.636 GiB, 25.79% gc time)
Training loss: 39.149445


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 26.982086 seconds (15.34 M allocations: 2.885 GiB, 37.63% gc time)
Validation loss: 17.113985
Unlabeled attachment score: 71.49473011817311
Labeled attachment score: 65.85755349728521
INFO: Backing up

Epoch 2
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
ERROR: ReadOnlyMemoryError()
Stacktrace:
 [1] #rnnforw#487(::Ptr{Void}, ::Bool, ::Void, ::Bool, ::Bool, ::Function, ::Knet.RNN, ::Knet.KnetArray{Float32,3}, ::Knet.KnetArray{Float32,3}, ::Void, ::Void) at /scratch/users/cgumeli/.julia/v0.6/Knet/src/rnn.jl:497
 [2] (::Knet.#kw##rnnforw)(::Array{Any,1}, ::Knet.#rnnforw, ::Knet.RNN, ::Knet.KnetArray{Float32,3}, ::Knet.KnetArray{Float32,3}, ::Void, ::Void) at ./<missing>:0
 [3] (::AutoGrad.##rfun#7#10{Knet.#rnnforw})(::Array{Any,1}, ::Function, ::Knet.RNN, ::Vararg{Any,N} where N) at /scratch/users/cgumeli/.julia/v0.6/AutoGrad/src/core.jl:124
 [4] (::AutoGrad.#kw##rfun#9)(::Array{Any,1}, ::AutoGrad.#rfun#9, ::Knet.RNN, ::Vararg{Any,N} where N) at ./<missing>:0
 [5] (::Knet.##rnnforw#489#490{AutoGrad.#rfun#9})(::Array{Any,1}, ::Function, ::Knet.RNN, ::AutoGrad.Rec{Knet.KnetArray{Float32,3}}, ::AutoGrad.Rec{Knet.KnetArray{Float32,3}}, ::Vararg{Any,N} where N) at /scratch/users/cgumeli/.julia/v0.6/Knet/src/rnn.jl:550
 [6] (::Knet.#kw##rnnforw)(::Array{Any,1}, ::Knet.#rnnforw, ::Knet.RNN, ::AutoGrad.Rec{Knet.KnetArray{Float32,3}}, ::AutoGrad.Rec{Knet.KnetArray{Float32,3}}, ::Void, ::Void) at ./<missing>:0
 [7] #_forw#59(::Array{Any,1}, ::Function, ::AutoGrad.Rec{Array{Any,1}}, ::KnetModules.LSTM, ::AutoGrad.Rec{Knet.KnetArray{Float32,3}}, ::Void, ::Void) at /scratch/users/cgumeli/.julia/v0.6/KnetModules/src/rnn.jl:59
 [8] #call#55 at /scratch/users/cgumeli/.julia/v0.6/KnetModules/src/rnn.jl:47 [inlined]
 [9] (::KnetModules.LSTM)(::AutoGrad.Rec{Array{Any,1}}, ::AutoGrad.Rec{Knet.KnetArray{Float32,3}}, ::Void, ::Void) at /scratch/users/cgumeli/.julia/v0.6/KnetModules/src/rnn.jl:47 (repeats 2 times)
 [10] (::FreshEncoder)(::AutoGrad.Rec{Array{Any,1}}, ::Array{Any,1}) at /scratch/users/cgumeli/Nubik/./encoders/fresh.jl:34
 [11] predict at /scratch/users/cgumeli/Nubik/main2.jl:13 [inlined]
 [12] loss(::AutoGrad.Rec{Array{Any,1}}, ::FreshEncoder, ::BiaffineDecoder2, ::Array{Any,1}) at /scratch/users/cgumeli/Nubik/main2.jl:19
 [13] forward_pass(::Function, ::Tuple{Array{Any,1},FreshEncoder,BiaffineDecoder2,Array{Any,1}}, ::Array{Any,1}, ::Int64) at /scratch/users/cgumeli/.julia/v0.6/AutoGrad/src/core.jl:88
 [14] (::AutoGrad.##gradfun#4#6{#loss,Int64})(::Array{Any,1}, ::Function, ::Array{Any,1}, ::Vararg{Any,N} where N) at /scratch/users/cgumeli/.julia/v0.6/AutoGrad/src/core.jl:57
 [15] (::AutoGrad.#gradfun#5)(::Array{Any,1}, ::Vararg{Any,N} where N) at /scratch/users/cgumeli/.julia/v0.6/AutoGrad/src/core.jl:57
 [16] macro expansion at /scratch/users/cgumeli/Nubik/main2.jl:192 [inlined]
 [17] macro expansion at ./util.jl:237 [inlined]
 [18] train() at /scratch/users/cgumeli/Nubik/main2.jl:190
 [19] main(::String) at /scratch/users/cgumeli/Nubik/main2.jl:83

julia> exit()
exit()
bash-4.2$ julia
julia
               _
   _       _ _(_)_     |  A fresh approach to technical computing
  (_)     | (_) (_)    |  Documentation: https://docs.julialang.org
   _ _   _| |_  __ _   |  Type "?help" for help.
  | | | | | | |/ _` |  |
  | | |_| | | | (_| |  |  Version 0.6.2 (2017-12-13 18:08 UTC)
 _/ |\__'_|_|_|\__'_|  |  Official http://julialang.org/ release
|__/                   |  x86_64-pc-linux-gnu

WARNING: Terminal not fully functional
julia> include("main2.jl")
include("main2.jl")
INFO: Checking dependencies
INFO: All dependencies are installed
accuracy_bt

julia> main("--datafiles $engtrn $engdev --lmfile $englm --feat 150 --lmdrop 0.33 --birnndrop 0.33 --decdrop 0.33 --tokens 500 --birnn 3")
main("--datafiles $engtrn $engdev --lmfile $englm --feat 150 --lmdrop 0.33 --birnndrop 0.33 --decdrop 0.33 --tokens 500 --birnn 3")
Tuple{Symbol,Any}[(:feat, 150), (:birnn, 3), (:birnndrop, 0.33), (:lmfile, "/scratch/users/okirnap/ud-treebanks-v2.2/chmodel_converted/english_chmodel.jld"), (:min, 2), (:birnncell, "LSTM"), (:labelunits, 100), (:xpos, 0), (:batchsize, 8), (:max, 128), (:decdrop, 0.33), (:upos, 0), (:backupdir, "Backups_2018-06-08T20:01:00.764"), (:beta1, 0.9), (:remb, 350), (:algo, "edmonds"), (:beta2, 0.999), (:lmdrop, 0.33), (:proj, true), (:tokens, 500), (:epochs, 100), (:recover, ""), (:birnnhidden, 200), (:optim, "Adam"), (:lr, 0.001), (:datafiles, Any["/scratch/users/okirnap/ud-treebanks-v2.2/UD_English-EWT/en_ewt-ud-train.conllu", "/scratch/users/okirnap/ud-treebanks-v2.2/UD_English-EWT/en_ewt-ud-dev.conllu"]), (:arcunits, 400)]
  C-c C-c^CERROR: InterruptException:
Stacktrace:
 [1] (::Base.#kw##info)(::Array{Any,1}, ::Base.#info, ::Base.TTY, ::String) at ./<missing>:0
 [2] main(::String) at /scratch/users/cgumeli/Nubik/main2.jl:71

julia> main("--datafiles $engtrn $engdev --lmfile $englm --upos 150 --lmdrop 0.33 --birnndrop 0.33 --decdrop 0.33 --tokens 500 --birnn 2")
main("--datafiles $engtrn $engdev --lmfile $englm --upos 150 --lmdrop 0.33 --birnndrop 0.33 --decdrop 0.33 --tokens 500 --birnn 2")
Tuple{Symbol,Any}[(:feat, 0), (:birnn, 2), (:birnndrop, 0.33), (:lmfile, "/scratch/users/okirnap/ud-treebanks-v2.2/chmodel_converted/english_chmodel.jld"), (:min, 2), (:birnncell, "LSTM"), (:labelunits, 100), (:xpos, 0), (:batchsize, 8), (:max, 128), (:decdrop, 0.33), (:upos, 150), (:backupdir, "Backups_2018-06-08T20:01:40.725"), (:beta1, 0.9), (:remb, 350), (:algo, "edmonds"), (:beta2, 0.999), (:lmdrop, 0.33), (:proj, true), (:tokens, 500), (:epochs, 100), (:recover, ""), (:birnnhidden, 200), (:optim, "Adam"), (:lr, 0.001), (:datafiles, Any["/scratch/users/okirnap/ud-treebanks-v2.2/UD_English-EWT/en_ewt-ud-train.conllu", "/scratch/users/okirnap/ud-treebanks-v2.2/UD_English-EWT/en_ewt-ud-dev.conllu"]), (:arcunits, 400)]
INFO: Loading data
20:01:41 /scratch/users/okirnap/ud-treebanks-v2.2/chmodel_converted/english_chmodel.jld 
20:01:46 /scratch/users/okirnap/ud-treebanks-v2.2/UD_English-EWT/en_ewt-ud-train.conllu 
20:01:48 /scratch/users/okirnap/ud-treebanks-v2.2/UD_English-EWT/en_ewt-ud-dev.conllu 
20:02:43 Caching lm vectors... 
INFO: Bucketing
INFO: Initializing Model and Optimizers
INFO: Transfering to gpu.
INFO: Setting momentums
INFO: Computing validation performance
parsers/mst.py:24: RuntimeWarning: divide by zero encountered in divide
  new_root = tokens[np.argmax(root_scores / head_scores)]
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 58.589167 seconds (18.64 M allocations: 1.640 GiB, 0.60% gc time)
Validation loss: 87.71294
Unlabeled attachment score: 3.325614819546471
Labeled attachment score: 0.047908016608112426

Epoch 1
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380

signal (11): Segmentation fault
while loading no file, in expression starting on line 0
unknown function (ip: 0x7f5d52cdb974)
unknown function (ip: 0x7f5d52c02635)
cuEventDestroy_v2 at /usr/lib64/nvidia/libcuda.so.1 (unknown line)
unknown function (ip: 0x7f5a99c465e3)
unknown function (ip: 0x7f5a99c7efc6)
unknown function (ip: 0x7f5a999d1ed1)
cudnnRNNBackwardData at /kuacc/apps/cudnn/v7.0.4_CUDA_9.0/lib64/libcudnn.so.7.0.4 (unknown line)
#rnnback#495 at /scratch/users/cgumeli/.julia/v0.6/Knet/src/rnn.jl:593
unknown function (ip: 0x7f5d2099a49a)
jl_call_fptr_internal at /buildworker/worker/package_linux64/build/src/julia_internal.h:339 [inlined]
jl_call_method_internal at /buildworker/worker/package_linux64/build/src/julia_internal.h:358 [inlined]
jl_apply_generic at /buildworker/worker/package_linux64/build/src/gf.c:1926
#rnnback at ./<missing>:0
#rnnforw#491 at /scratch/users/cgumeli/.julia/v0.6/Knet/src/rnn.jl:557
unknown function (ip: 0x7f5d20998b40)
jl_call_fptr_internal at /buildworker/worker/package_linux64/build/src/julia_internal.h:339 [inlined]
jl_call_method_internal at /buildworker/worker/package_linux64/build/src/julia_internal.h:358 [inlined]
jl_invoke at /buildworker/worker/package_linux64/build/src/gf.c:41
#rnnforw at ./<missing>:0
unknown function (ip: 0x7f5d20998866)
jl_call_fptr_internal at /buildworker/worker/package_linux64/build/src/julia_internal.h:339 [inlined]
jl_call_method_internal at /buildworker/worker/package_linux64/build/src/julia_internal.h:358 [inlined]
jl_apply_generic at /buildworker/worker/package_linux64/build/src/gf.c:1926
jl_apply at /buildworker/worker/package_linux64/build/src/julia.h:1424 [inlined]
jl_f__apply at /buildworker/worker/package_linux64/build/src/builtins.c:426
backward_pass at /scratch/users/cgumeli/.julia/v0.6/AutoGrad/src/core.jl:256
unknown function (ip: 0x7f5d20963d9d)
jl_call_fptr_internal at /buildworker/worker/package_linux64/build/src/julia_internal.h:339 [inlined]
jl_call_method_internal at /buildworker/worker/package_linux64/build/src/julia_internal.h:358 [inlined]
jl_apply_generic at /buildworker/worker/package_linux64/build/src/gf.c:1926
#gradfun#4 at /scratch/users/cgumeli/.julia/v0.6/AutoGrad/src/core.jl:62
jl_call_fptr_internal at /buildworker/worker/package_linux64/build/src/julia_internal.h:339 [inlined]
jl_call_method_internal at /buildworker/worker/package_linux64/build/src/julia_internal.h:358 [inlined]
jl_apply_generic at /buildworker/worker/package_linux64/build/src/gf.c:1926
jl_apply at /buildworker/worker/package_linux64/build/src/julia.h:1424 [inlined]
jl_f__apply at /buildworker/worker/package_linux64/build/src/builtins.c:426
gradfun at /scratch/users/cgumeli/.julia/v0.6/AutoGrad/src/core.jl:57
jl_call_fptr_internal at /buildworker/worker/package_linux64/build/src/julia_internal.h:339 [inlined]
jl_call_method_internal at /buildworker/worker/package_linux64/build/src/julia_internal.h:358 [inlined]
jl_apply_generic at /buildworker/worker/package_linux64/build/src/gf.c:1926
macro expansion at /scratch/users/cgumeli/Nubik/main2.jl:192 [inlined]
macro expansion at ./util.jl:237 [inlined]
train at /scratch/users/cgumeli/Nubik/main2.jl:190
main at /scratch/users/cgumeli/Nubik/main2.jl:83
unknown function (ip: 0x7f5d5cc8f272)
jl_call_fptr_internal at /buildworker/worker/package_linux64/build/src/julia_internal.h:339 [inlined]
jl_call_method_internal at /buildworker/worker/package_linux64/build/src/julia_internal.h:358 [inlined]
jl_apply_generic at /buildworker/worker/package_linux64/build/src/gf.c:1926
do_call at /buildworker/worker/package_linux64/build/src/interpreter.c:75
eval at /buildworker/worker/package_linux64/build/src/interpreter.c:242
jl_interpret_toplevel_expr at /buildworker/worker/package_linux64/build/src/interpreter.c:34
jl_toplevel_eval_flex at /buildworker/worker/package_linux64/build/src/toplevel.c:577
jl_toplevel_eval_in at /buildworker/worker/package_linux64/build/src/builtins.c:496
eval at ./boot.jl:235
unknown function (ip: 0x7f5d8d814d2f)
jl_call_fptr_internal at /buildworker/worker/package_linux64/build/src/julia_internal.h:339 [inlined]
jl_call_method_internal at /buildworker/worker/package_linux64/build/src/julia_internal.h:358 [inlined]
jl_apply_generic at /buildworker/worker/package_linux64/build/src/gf.c:1926
eval_user_input at ./REPL.jl:66
unknown function (ip: 0x7f5d8d89618f)
jl_call_fptr_internal at /buildworker/worker/package_linux64/build/src/julia_internal.h:339 [inlined]
jl_call_method_internal at /buildworker/worker/package_linux64/build/src/julia_internal.h:358 [inlined]
jl_apply_generic at /buildworker/worker/package_linux64/build/src/gf.c:1926
macro expansion at ./REPL.jl:97 [inlined]
#1 at ./event.jl:73
unknown function (ip: 0x7f5d5cc16d4f)
jl_call_fptr_internal at /buildworker/worker/package_linux64/build/src/julia_internal.h:339 [inlined]
jl_call_method_internal at /buildworker/worker/package_linux64/build/src/julia_internal.h:358 [inlined]
jl_apply_generic at /buildworker/worker/package_linux64/build/src/gf.c:1926
jl_apply at /buildworker/worker/package_linux64/build/src/julia.h:1424 [inlined]
start_task at /buildworker/worker/package_linux64/build/src/task.c:267
unknown function (ip: 0xffffffffffffffff)
Allocations: 122769218 (Pool: 122035537; Big: 733681); GC: 371
Segmentation fault
bash-4.2$ julia
julia
               _
   _       _ _(_)_     |  A fresh approach to technical computing
  (_)     | (_) (_)    |  Documentation: https://docs.julialang.org
   _ _   _| |_  __ _   |  Type "?help" for help.
  | | | | | | |/ _` |  |
  | | |_| | | | (_| |  |  Version 0.6.2 (2017-12-13 18:08 UTC)
 _/ |\__'_|_|_|\__'_|  |  Official http://julialang.org/ release
|__/                   |  x86_64-pc-linux-gnu

WARNING: Terminal not fully functional
julia> include("main2.jl")
include("main2.jl")
INFO: Checking dependencies
INFO: All dependencies are installed
accuracy_bt

julia> main("--datafiles $engtrn $engdev --lmfile $englm --upos 150 --lmdrop 0.33 --birnndrop 0.33 --decdrop 0.33 --tokens 500 --birnn 2")
main("--datafiles $engtrn $engdev --lmfile $englm --upos 150 --lmdrop 0.33 --birnndrop 0.33 --decdrop 0.33 --tokens 500 --birnn 2")
Tuple{Symbol,Any}[(:feat, 0), (:birnn, 2), (:birnndrop, 0.33), (:lmfile, "/scratch/users/okirnap/ud-treebanks-v2.2/chmodel_converted/english_chmodel.jld"), (:min, 2), (:birnncell, "LSTM"), (:labelunits, 100), (:xpos, 0), (:batchsize, 8), (:max, 128), (:decdrop, 0.33), (:upos, 150), (:backupdir, "Backups_2018-06-08T20:06:26.275"), (:beta1, 0.9), (:remb, 350), (:algo, "edmonds"), (:beta2, 0.999), (:lmdrop, 0.33), (:proj, true), (:tokens, 500), (:epochs, 100), (:recover, ""), (:birnnhidden, 200), (:optim, "Adam"), (:lr, 0.001), (:datafiles, Any["/scratch/users/okirnap/ud-treebanks-v2.2/UD_English-EWT/en_ewt-ud-train.conllu", "/scratch/users/okirnap/ud-treebanks-v2.2/UD_English-EWT/en_ewt-ud-dev.conllu"]), (:arcunits, 400)]
INFO: Loading data
20:06:30 /scratch/users/okirnap/ud-treebanks-v2.2/chmodel_converted/english_chmodel.jld 
20:06:35 /scratch/users/okirnap/ud-treebanks-v2.2/UD_English-EWT/en_ewt-ud-train.conllu 
20:06:36 /scratch/users/okirnap/ud-treebanks-v2.2/UD_English-EWT/en_ewt-ud-dev.conllu 
20:07:30 Caching lm vectors... 
INFO: Bucketing
INFO: Initializing Model and Optimizers
INFO: Transfering to gpu.
INFO: Setting momentums
INFO: Computing validation performance
parsers/mst.py:24: RuntimeWarning: divide by zero encountered in divide
  new_root = tokens[np.argmax(root_scores / head_scores)]
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 59.266977 seconds (18.64 M allocations: 1.639 GiB, 0.60% gc time)
Validation loss: 87.54605
Unlabeled attachment score: 3.0661130629191953
Labeled attachment score: 0.027946343021398912

Epoch 1
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380

signal (11): Segmentation fault
while loading no file, in expression starting on line 0
unknown function (ip: 0x7f415dd09974)
unknown function (ip: 0x7f415dc30635)
cuEventDestroy_v2 at /usr/lib64/nvidia/libcuda.so.1 (unknown line)
unknown function (ip: 0x7f3ea88465e3)
unknown function (ip: 0x7f3ea887efc6)
unknown function (ip: 0x7f3ea85d1ed1)
cudnnRNNBackwardData at /kuacc/apps/cudnn/v7.0.4_CUDA_9.0/lib64/libcudnn.so.7.0.4 (unknown line)
#rnnback#495 at /scratch/users/cgumeli/.julia/v0.6/Knet/src/rnn.jl:593
unknown function (ip: 0x7f412d18e7ea)
jl_call_fptr_internal at /buildworker/worker/package_linux64/build/src/julia_internal.h:339 [inlined]
jl_call_method_internal at /buildworker/worker/package_linux64/build/src/julia_internal.h:358 [inlined]
jl_apply_generic at /buildworker/worker/package_linux64/build/src/gf.c:1926
#rnnback at ./<missing>:0
#rnnforw#491 at /scratch/users/cgumeli/.julia/v0.6/Knet/src/rnn.jl:557
unknown function (ip: 0x7f412d18cea0)
jl_call_fptr_internal at /buildworker/worker/package_linux64/build/src/julia_internal.h:339 [inlined]
jl_call_method_internal at /buildworker/worker/package_linux64/build/src/julia_internal.h:358 [inlined]
jl_invoke at /buildworker/worker/package_linux64/build/src/gf.c:41
#rnnforw at ./<missing>:0
unknown function (ip: 0x7f412d18cbc6)
jl_call_fptr_internal at /buildworker/worker/package_linux64/build/src/julia_internal.h:339 [inlined]
jl_call_method_internal at /buildworker/worker/package_linux64/build/src/julia_internal.h:358 [inlined]
jl_apply_generic at /buildworker/worker/package_linux64/build/src/gf.c:1926
jl_apply at /buildworker/worker/package_linux64/build/src/julia.h:1424 [inlined]
jl_f__apply at /buildworker/worker/package_linux64/build/src/builtins.c:426
backward_pass at /scratch/users/cgumeli/.julia/v0.6/AutoGrad/src/core.jl:256
unknown function (ip: 0x7f412d15800d)
jl_call_fptr_internal at /buildworker/worker/package_linux64/build/src/julia_internal.h:339 [inlined]
jl_call_method_internal at /buildworker/worker/package_linux64/build/src/julia_internal.h:358 [inlined]
jl_apply_generic at /buildworker/worker/package_linux64/build/src/gf.c:1926
#gradfun#4 at /scratch/users/cgumeli/.julia/v0.6/AutoGrad/src/core.jl:62
jl_call_fptr_internal at /buildworker/worker/package_linux64/build/src/julia_internal.h:339 [inlined]
jl_call_method_internal at /buildworker/worker/package_linux64/build/src/julia_internal.h:358 [inlined]
jl_apply_generic at /buildworker/worker/package_linux64/build/src/gf.c:1926
jl_apply at /buildworker/worker/package_linux64/build/src/julia.h:1424 [inlined]
jl_f__apply at /buildworker/worker/package_linux64/build/src/builtins.c:426
gradfun at /scratch/users/cgumeli/.julia/v0.6/AutoGrad/src/core.jl:57
jl_call_fptr_internal at /buildworker/worker/package_linux64/build/src/julia_internal.h:339 [inlined]
jl_call_method_internal at /buildworker/worker/package_linux64/build/src/julia_internal.h:358 [inlined]
jl_apply_generic at /buildworker/worker/package_linux64/build/src/gf.c:1926
macro expansion at /scratch/users/cgumeli/Nubik/main2.jl:192 [inlined]
macro expansion at ./util.jl:237 [inlined]
train at /scratch/users/cgumeli/Nubik/main2.jl:190
main at /scratch/users/cgumeli/Nubik/main2.jl:83
unknown function (ip: 0x7f4167cbd2f2)
jl_call_fptr_internal at /buildworker/worker/package_linux64/build/src/julia_internal.h:339 [inlined]
jl_call_method_internal at /buildworker/worker/package_linux64/build/src/julia_internal.h:358 [inlined]
jl_apply_generic at /buildworker/worker/package_linux64/build/src/gf.c:1926
do_call at /buildworker/worker/package_linux64/build/src/interpreter.c:75
eval at /buildworker/worker/package_linux64/build/src/interpreter.c:242
jl_interpret_toplevel_expr at /buildworker/worker/package_linux64/build/src/interpreter.c:34
jl_toplevel_eval_flex at /buildworker/worker/package_linux64/build/src/toplevel.c:577
jl_toplevel_eval_in at /buildworker/worker/package_linux64/build/src/builtins.c:496
eval at ./boot.jl:235
unknown function (ip: 0x7f4198942d2f)
jl_call_fptr_internal at /buildworker/worker/package_linux64/build/src/julia_internal.h:339 [inlined]
jl_call_method_internal at /buildworker/worker/package_linux64/build/src/julia_internal.h:358 [inlined]
jl_apply_generic at /buildworker/worker/package_linux64/build/src/gf.c:1926
eval_user_input at ./REPL.jl:66
unknown function (ip: 0x7f41989c418f)
jl_call_fptr_internal at /buildworker/worker/package_linux64/build/src/julia_internal.h:339 [inlined]
jl_call_method_internal at /buildworker/worker/package_linux64/build/src/julia_internal.h:358 [inlined]
jl_apply_generic at /buildworker/worker/package_linux64/build/src/gf.c:1926
macro expansion at ./REPL.jl:97 [inlined]
#1 at ./event.jl:73
unknown function (ip: 0x7f4167c44d4f)
jl_call_fptr_internal at /buildworker/worker/package_linux64/build/src/julia_internal.h:339 [inlined]
jl_call_method_internal at /buildworker/worker/package_linux64/build/src/julia_internal.h:358 [inlined]
jl_apply_generic at /buildworker/worker/package_linux64/build/src/gf.c:1926
jl_apply at /buildworker/worker/package_linux64/build/src/julia.h:1424 [inlined]
start_task at /buildworker/worker/package_linux64/build/src/task.c:267
unknown function (ip: 0xffffffffffffffff)
Allocations: 122544657 (Pool: 121810998; Big: 733659); GC: 372
Segmentation fault
bash-4.2$ main("--datafiles $engtrn $engdev --lmfile $englm --upos 150 --lmdrop 0.33 --birnndrop 0.33 --decdrop 0.33 --tokens 500 --birnn 2")  C-c C-c^C
bash-4.2$ julia
julia
               _
   _       _ _(_)_     |  A fresh approach to technical computing
  (_)     | (_) (_)    |  Documentation: https://docs.julialang.org
   _ _   _| |_  __ _   |  Type "?help" for help.
  | | | | | | |/ _` |  |
  | | |_| | | | (_| |  |  Version 0.6.2 (2017-12-13 18:08 UTC)
 _/ |\__'_|_|_|\__'_|  |  Official http://julialang.org/ release
|__/                   |  x86_64-pc-linux-gnu

WARNING: Terminal not fully functional
julia> include("main2.jl")
include("main2.jl")
INFO: Checking dependencies
INFO: All dependencies are installed
accuracy_bt

julia> include("main2.jl")  C-c C-c^C
julia> main("--datafiles $engtrn $engdev --lmfile $englm --upos 150 --lmdrop 0.33 --birnndrop 0.33 --decdrop 0.33 --tokens 500 --birnn 1")
main("--datafiles $engtrn $engdev --lmfile $englm --upos 150 --lmdrop 0.33 --birnndrop 0.33 --decdrop 0.33 --tokens 500 --birnn 1")
Tuple{Symbol,Any}[(:feat, 0), (:birnn, 1), (:birnndrop, 0.33), (:lmfile, "/scratch/users/okirnap/ud-treebanks-v2.2/chmodel_converted/english_chmodel.jld"), (:min, 2), (:birnncell, "LSTM"), (:labelunits, 100), (:xpos, 0), (:batchsize, 8), (:max, 128), (:decdrop, 0.33), (:upos, 150), (:backupdir, "Backups_2018-06-08T20:11:27.733"), (:beta1, 0.9), (:remb, 350), (:algo, "edmonds"), (:beta2, 0.999), (:lmdrop, 0.33), (:proj, true), (:tokens, 500), (:epochs, 100), (:recover, ""), (:birnnhidden, 200), (:optim, "Adam"), (:lr, 0.001), (:datafiles, Any["/scratch/users/okirnap/ud-treebanks-v2.2/UD_English-EWT/en_ewt-ud-train.conllu", "/scratch/users/okirnap/ud-treebanks-v2.2/UD_English-EWT/en_ewt-ud-dev.conllu"]), (:arcunits, 400)]
INFO: Loading data
20:11:31 /scratch/users/okirnap/ud-treebanks-v2.2/chmodel_converted/english_chmodel.jld 
20:11:36 /scratch/users/okirnap/ud-treebanks-v2.2/UD_English-EWT/en_ewt-ud-train.conllu 
20:11:37 /scratch/users/okirnap/ud-treebanks-v2.2/UD_English-EWT/en_ewt-ud-dev.conllu 
20:12:33 Caching lm vectors... 
INFO: Bucketing
INFO: Initializing Model and Optimizers
INFO: Transfering to gpu.
INFO: Setting momentums
INFO: Computing validation performance
parsers/mst.py:24: RuntimeWarning: divide by zero encountered in divide
  new_root = tokens[np.argmax(root_scores / head_scores)]
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 58.531572 seconds (18.64 M allocations: 1.639 GiB, 0.66% gc time)
Validation loss: 86.32751
Unlabeled attachment score: 5.050303417438518
Labeled attachment score: 0.13573938038965186

Epoch 1
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
103.413292 seconds (86.04 M allocations: 10.851 GiB, 8.43% gc time)
Training loss: 33.9769


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
  7.586199 seconds (6.77 M allocations: 1.083 GiB, 5.72% gc time)
Validation loss: 14.797262
Unlabeled attachment score: 70.81603321622485
Labeled attachment score: 67.3147556691153
INFO: Backing up

Epoch 2
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 75.331090 seconds (81.18 M allocations: 10.584 GiB, 15.10% gc time)
Training loss: 23.499598


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
  8.333202 seconds (6.77 M allocations: 1.083 GiB, 5.47% gc time)
Validation loss: 13.124895
Unlabeled attachment score: 76.26956244011498
Labeled attachment score: 72.54072181411689
INFO: Backing up

Epoch 3
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 74.353340 seconds (81.21 M allocations: 10.585 GiB, 14.52% gc time)
Training loss: 20.84882


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
  8.143343 seconds (6.77 M allocations: 1.083 GiB, 5.39% gc time)
Validation loss: 12.149923
Unlabeled attachment score: 79.09214308527626
Labeled attachment score: 75.8783136378154
INFO: Backing up

Epoch 4
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 72.833469 seconds (81.25 M allocations: 10.586 GiB, 13.51% gc time)
Training loss: 19.62743


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
  8.286877 seconds (6.78 M allocations: 1.083 GiB, 5.17% gc time)
Validation loss: 11.819311
Unlabeled attachment score: 78.7128712871287
Labeled attachment score: 75.64675822420952
INFO: Backing up

Epoch 5
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 74.077395 seconds (81.29 M allocations: 10.587 GiB, 12.88% gc time)
Training loss: 18.475796


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
  8.540419 seconds (6.77 M allocations: 1.083 GiB, 4.76% gc time)
Validation loss: 11.562407
Unlabeled attachment score: 80.62120728201853
Labeled attachment score: 77.24768444586394
INFO: Backing up

Epoch 6
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 66.461116 seconds (80.97 M allocations: 10.580 GiB, 9.88% gc time)
Training loss: 17.748272


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 15.844720 seconds (7.04 M allocations: 1.088 GiB, 13.71% gc time)
Validation loss: 10.894555
Unlabeled attachment score: 81.24001916320664
Labeled attachment score: 78.25375279463431
INFO: Backing up

Epoch 7
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 68.320267 seconds (80.96 M allocations: 10.580 GiB, 11.36% gc time)
Training loss: 17.228525


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83


 15.330723 seconds (7.04 M allocations: 1.088 GiB, 14.58% gc time)
Validation loss: 10.665053
Unlabeled attachment score: 82.18620249121686
Labeled attachment score: 79.11609709358032
INFO: Backing up

Epoch 8
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 67.849548 seconds (80.98 M allocations: 10.581 GiB, 9.30% gc time)
Training loss: 16.71394


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
  9.876268 seconds (6.77 M allocations: 1.083 GiB, 4.90% gc time)
Validation loss: 10.762069
Unlabeled attachment score: 81.68316831683168
Labeled attachment score: 78.6729479399553
INFO: Backing up

Epoch 9
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 76.596373 seconds (81.23 M allocations: 10.585 GiB, 16.45% gc time)
Training loss: 16.535744


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
  9.671825 seconds (6.77 M allocations: 1.083 GiB, 4.92% gc time)
Validation loss: 11.261858
Unlabeled attachment score: 81.69913765570105
Labeled attachment score: 78.6370169274992
INFO: Backing up

Epoch 10
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 72.631889 seconds (81.20 M allocations: 10.585 GiB, 12.31% gc time)
Training loss: 16.241854


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 10.122746 seconds (6.77 M allocations: 1.083 GiB, 4.52% gc time)
Validation loss: 10.432901
Unlabeled attachment score: 81.57138294474609
Labeled attachment score: 78.64100926221654
INFO: Backing up

Epoch 11
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 75.658434 seconds (81.21 M allocations: 10.585 GiB, 14.79% gc time)
Training loss: 15.629705


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 10.604903 seconds (6.77 M allocations: 1.083 GiB, 4.40% gc time)
Validation loss: 10.30245
Unlabeled attachment score: 82.9287767486426
Labeled attachment score: 80.16208878952412
INFO: Backing up

Epoch 12
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 73.898502 seconds (81.23 M allocations: 10.585 GiB, 11.40% gc time)
Training loss: 15.18032


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 10.410969 seconds (6.77 M allocations: 1.083 GiB, 4.33% gc time)
Validation loss: 10.478468
Unlabeled attachment score: 82.46167358671352
Labeled attachment score: 79.67103161929096
INFO: Backing up

Epoch 13
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 75.239142 seconds (81.25 M allocations: 10.586 GiB, 12.25% gc time)
Training loss: 15.372194


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 10.413578 seconds (6.77 M allocations: 1.083 GiB, 4.05% gc time)
Validation loss: 10.398203
Unlabeled attachment score: 82.72916001277547
Labeled attachment score: 79.85467901628873
INFO: Backing up

Epoch 14
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 78.406519 seconds (81.26 M allocations: 10.586 GiB, 15.40% gc time)
Training loss: 14.862793


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 10.592985 seconds (6.77 M allocations: 1.083 GiB, 4.06% gc time)
Validation loss: 10.248979
Unlabeled attachment score: 83.1244011497924
Labeled attachment score: 80.15410412008943
INFO: Backing up

Epoch 15
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 68.056761 seconds (80.97 M allocations: 10.580 GiB, 9.71% gc time)
Training loss: 14.904693


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 19.729366 seconds (7.04 M allocations: 1.088 GiB, 17.40% gc time)
Validation loss: 10.195398
Unlabeled attachment score: 83.89492175023955
Labeled attachment score: 80.78489300542958
INFO: Backing up

Epoch 16
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 68.292484 seconds (80.97 M allocations: 10.581 GiB, 9.72% gc time)
Training loss: 14.680469


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 17.111017 seconds (7.04 M allocations: 1.088 GiB, 15.12% gc time)
Validation loss: 10.208907
Unlabeled attachment score: 82.87687639731715
Labeled attachment score: 80.14611945065474
INFO: Backing up

Epoch 17
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 67.064629 seconds (80.97 M allocations: 10.581 GiB, 11.39% gc time)
Training loss: 14.305265


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 11.829551 seconds (6.77 M allocations: 1.083 GiB, 4.17% gc time)
Validation loss: 10.379316
Unlabeled attachment score: 83.04455445544555
Labeled attachment score: 80.0303417438518
INFO: Backing up

Epoch 18
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 75.954865 seconds (81.26 M allocations: 10.586 GiB, 16.04% gc time)
Training loss: 14.533309


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 11.316872 seconds (6.77 M allocations: 1.083 GiB, 4.19% gc time)
Validation loss: 10.334465
Unlabeled attachment score: 82.98866176940275
Labeled attachment score: 80.22995847971895
INFO: Backing up

Epoch 19
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 74.495523 seconds (81.23 M allocations: 10.585 GiB, 16.46% gc time)
Training loss: 13.917712


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 12.645898 seconds (6.77 M allocations: 1.083 GiB, 4.07% gc time)
Validation loss: 10.218175
Unlabeled attachment score: 83.4797189396359
Labeled attachment score: 80.3856595336953
INFO: Backing up

Epoch 20
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 71.737204 seconds (81.20 M allocations: 10.585 GiB, 12.52% gc time)
Training loss: 13.722954


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 12.341233 seconds (6.77 M allocations: 1.083 GiB, 4.04% gc time)
Validation loss: 10.226625
Unlabeled attachment score: 83.22021718300863
Labeled attachment score: 80.20600447141489
INFO: Backing up

Epoch 21
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 73.729543 seconds (81.21 M allocations: 10.585 GiB, 12.22% gc time)
Training loss: 13.870107


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 12.439620 seconds (6.77 M allocations: 1.083 GiB, 3.72% gc time)
Validation loss: 10.245865
Unlabeled attachment score: 82.9287767486426
Labeled attachment score: 80.16608112424146
INFO: Backing up

Epoch 22
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 72.668362 seconds (81.25 M allocations: 10.585 GiB, 14.57% gc time)
Training loss: 13.730931


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 13.219310 seconds (6.78 M allocations: 1.083 GiB, 3.33% gc time)
Validation loss: 10.195465
Unlabeled attachment score: 83.12839348450974
Labeled attachment score: 80.22596614500159
INFO: Backing up

Epoch 23
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 72.524876 seconds (81.29 M allocations: 10.587 GiB, 12.87% gc time)
Training loss: 13.642584


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 12.780801 seconds (6.77 M allocations: 1.083 GiB, 3.23% gc time)
Validation loss: 10.282513
Unlabeled attachment score: 83.40386458000638
Labeled attachment score: 80.43755988502076
INFO: Backing up

Epoch 24
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
  C-c C-c^CERROR: InterruptException:
Stacktrace:
 [1] uncat(::Knet.KnetArray{Float32,2}, ::Int64, ::Int64, ::AutoGrad.Rec{Knet.KnetArray{Float32,1}}, ::AutoGrad.Rec{Knet.KnetArray{Float32,1}}, ::Vararg{AutoGrad.Rec{Knet.KnetArray{Float32,1}},N} where N) at /scratch/users/cgumeli/.julia/v0.6/AutoGrad/src/base/abstractarray.jl:113
 [2] cat(::Type{AutoGrad.Grad{207}}, ::Knet.KnetArray{Float32,2}, ::Knet.KnetArray{Float32,2}, ::Int64, ::AutoGrad.Rec{Knet.KnetArray{Float32,1}}, ::Vararg{AutoGrad.Rec{Knet.KnetArray{Float32,1}},N} where N) at /scratch/users/cgumeli/.julia/v0.6/Knet/src/karray.jl:202
 [3] backward_pass(::AutoGrad.Rec{Array{Any,1}}, ::AutoGrad.Rec{Float32}, ::Array{AutoGrad.Node,1}) at /scratch/users/cgumeli/.julia/v0.6/AutoGrad/src/core.jl:256
 [4] (::AutoGrad.##gradfun#4#6{#loss,Int64})(::Array{Any,1}, ::Function, ::Array{Any,1}, ::Vararg{Any,N} where N) at /scratch/users/cgumeli/.julia/v0.6/AutoGrad/src/core.jl:62
 [5] (::AutoGrad.#gradfun#5)(::Array{Any,1}, ::Vararg{Any,N} where N) at /scratch/users/cgumeli/.julia/v0.6/AutoGrad/src/core.jl:57
 [6] macro expansion at /scratch/users/cgumeli/Nubik/main2.jl:192 [inlined]
 [7] macro expansion at ./util.jl:237 [inlined]
 [8] train() at /scratch/users/cgumeli/Nubik/main2.jl:190
 [9] main(::String) at /scratch/users/cgumeli/Nubik/main2.jl:83

julia> exit()
exit()
bash-4.2$ julia
julia
               _
   _       _ _(_)_     |  A fresh approach to technical computing
  (_)     | (_) (_)    |  Documentation: https://docs.julialang.org
   _ _   _| |_  __ _   |  Type "?help" for help.
  | | | | | | |/ _` |  |
  | | |_| | | | (_| |  |  Version 0.6.2 (2017-12-13 18:08 UTC)
 _/ |\__'_|_|_|\__'_|  |  Official http://julialang.org/ release
|__/                   |  x86_64-pc-linux-gnu

WARNING: Terminal not fully functional
julia> include("main2.jl")
include("main2.jl")
INFO: Checking dependencies
INFO: All dependencies are installed
accuracy_bt

julia> 


julia> include("main2.jl")
include("main2.jl")
INFO: Checking dependencies
INFO: All dependencies are installed
WARNING: redefining constant UPOSTAG
WARNING: redefining constant UDEPREL
WARNING: redefining constant lmdir
WARNING: redefining constant datadir
WARNING: redefining constant engtrn
WARNING: redefining constant engdev
WARNING: redefining constant englm
ERROR: LoadError: LoadError: LoadError: invalid redefinition of constant FreshEncoder
Stacktrace:
 [1] include_from_node1(::String) at ./loading.jl:576
 [2] include(::String) at ./sysimg.jl:14
 [3] include_from_node1(::String) at ./loading.jl:576
 [4] include(::String) at ./sysimg.jl:14
 [5] include_from_node1(::String) at ./loading.jl:576
 [6] include(::String) at ./sysimg.jl:14
while loading /scratch/users/cgumeli/Nubik/./encoders/fresh.jl, in expression starting on line 4
while loading /scratch/users/cgumeli/Nubik/./encoders/index.jl, in expression starting on line 3
while loading /scratch/users/cgumeli/Nubik/main2.jl, in expression starting on line 7

julia> exit()
exit()
bash-4.2$ julia
julia
               _
   _       _ _(_)_     |  A fresh approach to technical computing
  (_)     | (_) (_)    |  Documentation: https://docs.julialang.org
   _ _   _| |_  __ _   |  Type "?help" for help.
  | | | | | | |/ _` |  |
  | | |_| | | | (_| |  |  Version 0.6.2 (2017-12-13 18:08 UTC)
 _/ |\__'_|_|_|\__'_|  |  Official http://julialang.org/ release
|__/                   |  x86_64-pc-linux-gnu

WARNING: Terminal not fully functional
julia> include("main2.jl")
include("main2.jl")
INFO: Checking dependencies
INFO: All dependencies are installed
accuracy_bt

julia> main("--datafiles $engtrn $engdev --lmfile $englm --upos 150 --lmdrop 0.33 --birnndrop 0.33 --decdrop 0.33 --tokens 500 --birnn 2")
main("--datafiles $engtrn $engdev --lmfile $englm --upos 150 --lmdrop 0.33 --birnndrop 0.33 --decdrop 0.33 --tokens 500 --birnn 2")
Tuple{Symbol,Any}[(:feat, 0), (:birnn, 2), (:birnndrop, 0.33), (:lmfile, "/scratch/users/okirnap/ud-treebanks-v2.2/chmodel_converted/english_chmodel.jld"), (:min, 2), (:birnncell, "LSTM"), (:labelunits, 100), (:xpos, 0), (:batchsize, 8), (:max, 128), (:decdrop, 0.33), (:upos, 150), (:backupdir, "Backups_2018-06-08T20:55:58.726"), (:beta1, 0.9), (:remb, 350), (:algo, "edmonds"), (:beta2, 0.999), (:lmdrop, 0.33), (:proj, true), (:tokens, 500), (:epochs, 100), (:recover, ""), (:birnnhidden, 200), (:optim, "Adam"), (:lr, 0.001), (:datafiles, Any["/scratch/users/okirnap/ud-treebanks-v2.2/UD_English-EWT/en_ewt-ud-train.conllu", "/scratch/users/okirnap/ud-treebanks-v2.2/UD_English-EWT/en_ewt-ud-dev.conllu"]), (:arcunits, 400)]
INFO: Loading data
20:56:02 /scratch/users/okirnap/ud-treebanks-v2.2/chmodel_converted/english_chmodel.jld 
20:56:07 /scratch/users/okirnap/ud-treebanks-v2.2/UD_English-EWT/en_ewt-ud-train.conllu 
20:56:08 /scratch/users/okirnap/ud-treebanks-v2.2/UD_English-EWT/en_ewt-ud-dev.conllu 
20:56:59 Caching lm vectors... 
INFO: Bucketing
INFO: Initializing Model and Optimizers
INFO: Transfering to gpu.
INFO: Setting momentums
INFO: Computing validation performance
parsers/mst.py:24: RuntimeWarning: divide by zero encountered in divide
  new_root = tokens[np.argmax(root_scores / head_scores)]
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 57.744281 seconds (18.65 M allocations: 1.641 GiB, 0.58% gc time)
Validation loss: 86.40217
Unlabeled attachment score: 3.816671989779623
Labeled attachment score: 0.17167039284573618

Epoch 1
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
106.119251 seconds (86.16 M allocations: 10.856 GiB, 9.05% gc time)
Training loss: 35.03168


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
  8.273307 seconds (6.77 M allocations: 1.083 GiB, 5.45% gc time)
Validation loss: 15.335308
Unlabeled attachment score: 71.50271478760779
Labeled attachment score: 67.18300862344299
INFO: Backing up

Epoch 2
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 74.720872 seconds (81.35 M allocations: 10.592 GiB, 10.51% gc time)
Training loss: 24.766106


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
  8.585442 seconds (6.77 M allocations: 1.083 GiB, 5.41% gc time)
Validation loss: 13.158031
Unlabeled attachment score: 74.64867454487384
Labeled attachment score: 71.34302139891409
INFO: Backing up

Epoch 3
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 70.205208 seconds (81.36 M allocations: 10.591 GiB, 10.51% gc time)
Training loss: 21.902515


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
  8.016417 seconds (6.77 M allocations: 1.083 GiB, 5.41% gc time)
Validation loss: 12.205494
Unlabeled attachment score: 78.75678696901949
Labeled attachment score: 75.36330245927819
INFO: Backing up

Epoch 4
10 / 380
20 / 380
30 / 380
40 / 380

signal (11): Segmentation fault
while loading no file, in expression starting on line 0
unknown function (ip: 0x7f7c840d8974)
unknown function (ip: 0x7f7c83fff635)
cuEventDestroy_v2 at /usr/lib64/nvidia/libcuda.so.1 (unknown line)
unknown function (ip: 0x7f79cc6465e3)
unknown function (ip: 0x7f79cc67efc6)
unknown function (ip: 0x7f79cc3d1ed1)
cudnnRNNBackwardData at /kuacc/apps/cudnn/v7.0.4_CUDA_9.0/lib64/libcudnn.so.7.0.4 (unknown line)
#rnnback#495 at /scratch/users/cgumeli/.julia/v0.6/Knet/src/rnn.jl:593
unknown function (ip: 0x7f7c5c192fba)
jl_call_fptr_internal at /buildworker/worker/package_linux64/build/src/julia_internal.h:339 [inlined]
jl_call_method_internal at /buildworker/worker/package_linux64/build/src/julia_internal.h:358 [inlined]
jl_apply_generic at /buildworker/worker/package_linux64/build/src/gf.c:1926
#rnnback at ./<missing>:0
#rnnforw#491 at /scratch/users/cgumeli/.julia/v0.6/Knet/src/rnn.jl:557
unknown function (ip: 0x7f7c5c191670)
jl_call_fptr_internal at /buildworker/worker/package_linux64/build/src/julia_internal.h:339 [inlined]
jl_call_method_internal at /buildworker/worker/package_linux64/build/src/julia_internal.h:358 [inlined]
jl_invoke at /buildworker/worker/package_linux64/build/src/gf.c:41
#rnnforw at ./<missing>:0
unknown function (ip: 0x7f7c5c191396)
jl_call_fptr_internal at /buildworker/worker/package_linux64/build/src/julia_internal.h:339 [inlined]
jl_call_method_internal at /buildworker/worker/package_linux64/build/src/julia_internal.h:358 [inlined]
jl_apply_generic at /buildworker/worker/package_linux64/build/src/gf.c:1926
jl_apply at /buildworker/worker/package_linux64/build/src/julia.h:1424 [inlined]
jl_f__apply at /buildworker/worker/package_linux64/build/src/builtins.c:426
backward_pass at /scratch/users/cgumeli/.julia/v0.6/AutoGrad/src/core.jl:256
unknown function (ip: 0x7f7c5c15c1ad)
jl_call_fptr_internal at /buildworker/worker/package_linux64/build/src/julia_internal.h:339 [inlined]
jl_call_method_internal at /buildworker/worker/package_linux64/build/src/julia_internal.h:358 [inlined]
jl_apply_generic at /buildworker/worker/package_linux64/build/src/gf.c:1926
#gradfun#4 at /scratch/users/cgumeli/.julia/v0.6/AutoGrad/src/core.jl:62
jl_call_fptr_internal at /buildworker/worker/package_linux64/build/src/julia_internal.h:339 [inlined]
jl_call_method_internal at /buildworker/worker/package_linux64/build/src/julia_internal.h:358 [inlined]
jl_apply_generic at /buildworker/worker/package_linux64/build/src/gf.c:1926
jl_apply at /buildworker/worker/package_linux64/build/src/julia.h:1424 [inlined]
jl_f__apply at /buildworker/worker/package_linux64/build/src/builtins.c:426
gradfun at /scratch/users/cgumeli/.julia/v0.6/AutoGrad/src/core.jl:57
jl_call_fptr_internal at /buildworker/worker/package_linux64/build/src/julia_internal.h:339 [inlined]
jl_call_method_internal at /buildworker/worker/package_linux64/build/src/julia_internal.h:358 [inlined]
jl_apply_generic at /buildworker/worker/package_linux64/build/src/gf.c:1926
macro expansion at /scratch/users/cgumeli/Nubik/main2.jl:192 [inlined]
macro expansion at ./util.jl:237 [inlined]
train at /scratch/users/cgumeli/Nubik/main2.jl:190
main at /scratch/users/cgumeli/Nubik/main2.jl:83
unknown function (ip: 0x7f7c8e08c272)
jl_call_fptr_internal at /buildworker/worker/package_linux64/build/src/julia_internal.h:339 [inlined]
jl_call_method_internal at /buildworker/worker/package_linux64/build/src/julia_internal.h:358 [inlined]
jl_apply_generic at /buildworker/worker/package_linux64/build/src/gf.c:1926
do_call at /buildworker/worker/package_linux64/build/src/interpreter.c:75
eval at /buildworker/worker/package_linux64/build/src/interpreter.c:242
jl_interpret_toplevel_expr at /buildworker/worker/package_linux64/build/src/interpreter.c:34
jl_toplevel_eval_flex at /buildworker/worker/package_linux64/build/src/toplevel.c:577
jl_toplevel_eval_in at /buildworker/worker/package_linux64/build/src/builtins.c:496
eval at ./boot.jl:235
unknown function (ip: 0x7f7cbec11d2f)
jl_call_fptr_internal at /buildworker/worker/package_linux64/build/src/julia_internal.h:339 [inlined]
jl_call_method_internal at /buildworker/worker/package_linux64/build/src/julia_internal.h:358 [inlined]
jl_apply_generic at /buildworker/worker/package_linux64/build/src/gf.c:1926
eval_user_input at ./REPL.jl:66
unknown function (ip: 0x7f7cbec9318f)
jl_call_fptr_internal at /buildworker/worker/package_linux64/build/src/julia_internal.h:339 [inlined]
jl_call_method_internal at /buildworker/worker/package_linux64/build/src/julia_internal.h:358 [inlined]
jl_apply_generic at /buildworker/worker/package_linux64/build/src/gf.c:1926
macro expansion at ./REPL.jl:97 [inlined]
#1 at ./event.jl:73
unknown function (ip: 0x7f7c8e013d4f)
jl_call_fptr_internal at /buildworker/worker/package_linux64/build/src/julia_internal.h:339 [inlined]
jl_call_method_internal at /buildworker/worker/package_linux64/build/src/julia_internal.h:358 [inlined]
jl_apply_generic at /buildworker/worker/package_linux64/build/src/gf.c:1926
jl_apply at /buildworker/worker/package_linux64/build/src/julia.h:1424 [inlined]
start_task at /buildworker/worker/package_linux64/build/src/task.c:267
unknown function (ip: 0xffffffffffffffff)
Allocations: 352513046 (Pool: 349843413; Big: 2669633); GC: 1805
Segmentation fault
bash-4.2$ julia
julia
               _
   _       _ _(_)_     |  A fresh approach to technical computing
  (_)     | (_) (_)    |  Documentation: https://docs.julialang.org
   _ _   _| |_  __ _   |  Type "?help" for help.
  | | | | | | |/ _` |  |
  | | |_| | | | (_| |  |  Version 0.6.2 (2017-12-13 18:08 UTC)
 _/ |\__'_|_|_|\__'_|  |  Official http://julialang.org/ release
|__/                   |  x86_64-pc-linux-gnu

WARNING: Terminal not fully functional
julia> include("main2.jl")
include("main2.jl")
INFO: Checking dependencies
INFO: All dependencies are installed
accuracy_bt

julia> main("--datafiles $engtrn $engdev --lmfile $englm --upos 150 --lmdrop 0.33 --birnndrop 0.33 --decdrop 0.33 --tokens 500 --birnn 1")
main("--datafiles $engtrn $engdev --lmfile $englm --upos 150 --lmdrop 0.33 --birnndrop 0.33 --decdrop 0.33 --tokens 500 --birnn 1")
Tuple{Symbol,Any}[(:feat, 0), (:birnn, 1), (:birnndrop, 0.33), (:lmfile, "/scratch/users/okirnap/ud-treebanks-v2.2/chmodel_converted/english_chmodel.jld"), (:min, 2), (:birnncell, "LSTM"), (:labelunits, 100), (:xpos, 0), (:batchsize, 8), (:max, 128), (:decdrop, 0.33), (:upos, 150), (:backupdir, "Backups_2018-06-08T21:05:52.554"), (:beta1, 0.9), (:remb, 350), (:algo, "edmonds"), (:beta2, 0.999), (:lmdrop, 0.33), (:proj, true), (:tokens, 500), (:epochs, 100), (:recover, ""), (:birnnhidden, 200), (:optim, "Adam"), (:lr, 0.001), (:datafiles, Any["/scratch/users/okirnap/ud-treebanks-v2.2/UD_English-EWT/en_ewt-ud-train.conllu", "/scratch/users/okirnap/ud-treebanks-v2.2/UD_English-EWT/en_ewt-ud-dev.conllu"]), (:arcunits, 400)]
INFO: Loading data
21:05:56 /scratch/users/okirnap/ud-treebanks-v2.2/chmodel_converted/english_chmodel.jld 
21:06:01 /scratch/users/okirnap/ud-treebanks-v2.2/UD_English-EWT/en_ewt-ud-train.conllu 
21:06:02 /scratch/users/okirnap/ud-treebanks-v2.2/UD_English-EWT/en_ewt-ud-dev.conllu 
21:07:01 Caching lm vectors... 
INFO: Bucketing
INFO: Initializing Model and Optimizers
INFO: Transfering to gpu.
INFO: Setting momentums
INFO: Computing validation performance
parsers/mst.py:24: RuntimeWarning: divide by zero encountered in divide
  new_root = tokens[np.argmax(root_scores / head_scores)]
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 58.995097 seconds (18.64 M allocations: 1.639 GiB, 0.62% gc time)
Validation loss: 85.519455
Unlabeled attachment score: 12.01293516448419
Labeled attachment score: 0.32737144682210156

Epoch 1
  C-c C-c^CERROR: InterruptException:
Stacktrace:
 [1] broadcast(::Function, ::AutoGrad.Rec{Knet.KnetArray{Float32,2}}, ::AutoGrad.Rec{Knet.KnetArray{Float32,2}}) at /scratch/users/cgumeli/.julia/v0.6/AutoGrad/src/unfuse.jl:37
 [2] (::KnetModules.Linear)(::AutoGrad.Rec{Array{Any,1}}, ::AutoGrad.Rec{Knet.KnetArray{Float32,2}}) at /scratch/users/cgumeli/.julia/v0.6/KnetModules/src/linear.jl:44
 [3] (::FreshEncoder)(::AutoGrad.Rec{Array{Any,1}}, ::Array{Any,1}) at /scratch/users/cgumeli/Nubik/./encoders/fresh.jl:53
 [4] predict at /scratch/users/cgumeli/Nubik/main2.jl:13 [inlined]
 [5] loss(::AutoGrad.Rec{Array{Any,1}}, ::FreshEncoder, ::BiaffineDecoder2, ::Array{Any,1}) at /scratch/users/cgumeli/Nubik/main2.jl:19
 [6] forward_pass(::Function, ::Tuple{Array{Any,1},FreshEncoder,BiaffineDecoder2,Array{Any,1}}, ::Array{Any,1}, ::Int64) at /scratch/users/cgumeli/.julia/v0.6/AutoGrad/src/core.jl:88
 [7] (::AutoGrad.##gradfun#4#6{#loss,Int64})(::Array{Any,1}, ::Function, ::Array{Any,1}, ::Vararg{Any,N} where N) at /scratch/users/cgumeli/.julia/v0.6/AutoGrad/src/core.jl:57
 [8] (::AutoGrad.#gradfun#5)(::Array{Any,1}, ::Vararg{Any,N} where N) at /scratch/users/cgumeli/.julia/v0.6/AutoGrad/src/core.jl:57
 [9] macro expansion at /scratch/users/cgumeli/Nubik/main2.jl:192 [inlined]
 [10] macro expansion at ./util.jl:237 [inlined]
 [11] train() at /scratch/users/cgumeli/Nubik/main2.jl:190
 [12] main(::String) at /scratch/users/cgumeli/Nubik/main2.jl:83

julia> main("--datafiles $engtrn $engdev --lmfile $englm --upos 50 --birnnhidden 150 --lmdrop 0.33 --birnndrop 0.33 --decdrop 0.33 --tokens 500 --birnn 2")
main("--datafiles $engtrn $engdev --lmfile $englm --upos 50 --birnnhidden 150 --lmdrop 0.33 --birnndrop 0.33 --decdrop 0.33 --tokens 500 --birnn 2")
Tuple{Symbol,Any}[(:feat, 0), (:birnn, 2), (:birnndrop, 0.33), (:lmfile, "/scratch/users/okirnap/ud-treebanks-v2.2/chmodel_converted/english_chmodel.jld"), (:min, 2), (:birnncell, "LSTM"), (:labelunits, 100), (:xpos, 0), (:batchsize, 8), (:max, 128), (:decdrop, 0.33), (:upos, 50), (:backupdir, "Backups_2018-06-08T21:09:44.788"), (:beta1, 0.9), (:remb, 350), (:algo, "edmonds"), (:beta2, 0.999), (:lmdrop, 0.33), (:proj, true), (:tokens, 500), (:epochs, 100), (:recover, ""), (:birnnhidden, 150), (:optim, "Adam"), (:lr, 0.001), (:datafiles, Any["/scratch/users/okirnap/ud-treebanks-v2.2/UD_English-EWT/en_ewt-ud-train.conllu", "/scratch/users/okirnap/ud-treebanks-v2.2/UD_English-EWT/en_ewt-ud-dev.conllu"]), (:arcunits, 400)]
INFO: Loading data
21:09:44 /scratch/users/okirnap/ud-treebanks-v2.2/chmodel_converted/english_chmodel.jld 
21:09:47 /scratch/users/okirnap/ud-treebanks-v2.2/UD_English-EWT/en_ewt-ud-train.conllu 
21:09:47 /scratch/users/okirnap/ud-treebanks-v2.2/UD_English-EWT/en_ewt-ud-dev.conllu 
  C-c C-c^CERROR: InterruptException:
Stacktrace:
 [1] knetgc(::Int64) at /scratch/users/cgumeli/.julia/v0.6/Knet/src/kptr.jl:125
 [2] Knet.KnetPtr(::Int64) at /scratch/users/cgumeli/.julia/v0.6/Knet/src/kptr.jl:88
 [3] Type at /scratch/users/cgumeli/.julia/v0.6/Knet/src/karray.jl:104 [inlined]
 [4] Knet.KnetArray(::Type{Float32}, ::Tuple{Int64,Int64}) at /scratch/users/cgumeli/.julia/v0.6/Knet/src/karray.jl:116
 [5] broadcast#*(::Knet.KnetArray{Float32,2}, ::Knet.KnetArray{Float32,2}) at /scratch/users/cgumeli/.julia/v0.6/Knet/src/broadcast.jl:76
 [6] *(::AutoGrad.Broadcasted{Knet.KnetArray{Float32,2}}, ::AutoGrad.Broadcasted{Knet.KnetArray{Float32,2}}) at /scratch/users/cgumeli/.julia/v0.6/AutoGrad/src/unfuse.jl:49
 [7] broadcast(::Function, ::Knet.KnetArray{Float32,2}, ::Knet.KnetArray{Float32,2}) at /scratch/users/cgumeli/.julia/v0.6/Knet/src/unfuse.jl:9
 [8] #_lstm#9(::Knet.KnetArray{Float32,1}, ::Function, ::Knet.KnetArray{Float32,2}, ::Knet.KnetArray{Float32,2}, ::Knet.KnetArray{Float32,2}, ::Knet.KnetArray{Float32,2}, ::Knet.KnetArray{Float32,2}) at /scratch/users/cgumeli/Nubik/./ku-dependency-parser2/src/lm_utils.jl:159
 [9] (::#kw##_lstm)(::Array{Any,1}, ::#_lstm, ::Knet.KnetArray{Float32,2}, ::Knet.KnetArray{Float32,2}, ::Knet.KnetArray{Float32,2}, ::Knet.KnetArray{Float32,2}, ::Knet.KnetArray{Float32,2}) at ./<missing>:0
 [10] wordlstm(::Array{Knet.KnetArray{Float32,2},1}, ::Array{Array{Int64,1},1}, ::Array{Array{Float32,1},1}, ::Knet.KnetArray{Float32,2}) at /scratch/users/cgumeli/Nubik/./ku-dependency-parser2/src/lm_utils.jl:204
 [11] #fillvecs!#16(::Int64, ::Function, ::Array{Knet.KnetArray{Float32,2},1}, ::Array{Any,1}, ::Vocab) at /scratch/users/cgumeli/Nubik/./ku-dependency-parser2/src/lm_utils.jl:321
 [12] #depmain#46(::Bool, ::Function, ::String) at /scratch/users/cgumeli/Nubik/./ku-dependency-parser2/train.jl:55
 [13] load_data(::SubString{String}, ::SubString{String}, ::SubString{String}) at /scratch/users/cgumeli/Nubik/./data.jl:9
 [14] main(::String) at /scratch/users/cgumeli/Nubik/main2.jl:73

julia> include("main2.jl")
include("main2.jl")
INFO: Checking dependencies
INFO: All dependencies are installed
WARNING: redefining constant UPOSTAG
WARNING: redefining constant UDEPREL
WARNING: redefining constant lmdir
WARNING: redefining constant datadir
WARNING: redefining constant engtrn
WARNING: redefining constant engdev
WARNING: redefining constant englm
accuracy_bt

julia> main("--datafiles $engtrn $engdev --lmfile $englm --upos 50 --birnnhidden 150 --lmdrop 0.33 --birnndrop 0.33 --decdrop 0.33 --tokens 500 --birnn 2")  C-c C-c^C
julia> main("--datafiles $engtrn $engdev --lmfile $englm --upos 50 --birnnhidden 150 --lmdrop 0.33 --birnndrop 0.33 --decdrop 0.33 --tokens 500 --birnn 2")
main("--datafiles $engtrn $engdev --lmfile $englm --upos 50 --birnnhidden 150 --lmdrop 0.33 --birnndrop 0.33 --decdrop 0.33 --tokens 500 --birnn 2")
Tuple{Symbol,Any}[(:feat, 0), (:birnn, 2), (:birnndrop, 0.33), (:lmfile, "/scratch/users/okirnap/ud-treebanks-v2.2/chmodel_converted/english_chmodel.jld"), (:min, 2), (:birnncell, "LSTM"), (:labelunits, 100), (:xpos, 0), (:batchsize, 8), (:max, 128), (:decdrop, 0.33), (:upos, 50), (:backupdir, "Backups_2018-06-08T21:11:34.442"), (:beta1, 0.9), (:remb, 350), (:algo, "edmonds"), (:beta2, 0.999), (:lmdrop, 0.33), (:proj, true), (:tokens, 500), (:epochs, 100), (:recover, ""), (:birnnhidden, 150), (:optim, "Adam"), (:lr, 0.001), (:datafiles, Any["/scratch/users/okirnap/ud-treebanks-v2.2/UD_English-EWT/en_ewt-ud-train.conllu", "/scratch/users/okirnap/ud-treebanks-v2.2/UD_English-EWT/en_ewt-ud-dev.conllu"]), (:arcunits, 400)]
INFO: Loading data
21:11:35 /scratch/users/okirnap/ud-treebanks-v2.2/chmodel_converted/english_chmodel.jld 
21:11:36 /scratch/users/okirnap/ud-treebanks-v2.2/UD_English-EWT/en_ewt-ud-train.conllu 
21:11:37 /scratch/users/okirnap/ud-treebanks-v2.2/UD_English-EWT/en_ewt-ud-dev.conllu 
ERROR: cudart.cudaMemcpy error 77
Stacktrace:
 [1] macro expansion at /scratch/users/cgumeli/.julia/v0.6/Knet/src/gpu.jl:18 [inlined]
 [2] unsafe_copy!(::Knet.KnetArray{Int32,1}, ::Int64, ::Array{Int32,1}, ::Int64, ::Int64) at /scratch/users/cgumeli/.julia/v0.6/Knet/src/karray.jl:340
 [3] getindex(::Knet.KnetArray{Float32,2}, ::Colon, ::Array{Int64,1}) at /scratch/users/cgumeli/.julia/v0.6/Knet/src/karray.jl:652
 [4] charlstm(::Array{Knet.KnetArray{Float32,2},1}, ::Array{Array{Int64,1},1}, ::Array{Array{Float32,1},1}) at /scratch/users/cgumeli/Nubik/./ku-dependency-parser2/src/lm_utils.jl:180
 [5] #fillvecs!#138(::Int64, ::Function, ::Array{Knet.KnetArray{Float32,2},1}, ::Array{Any,1}, ::Vocab) at /scratch/users/cgumeli/Nubik/./ku-dependency-parser2/src/lm_utils.jl:308
 [6] #depmain#168(::Bool, ::Function, ::String) at /scratch/users/cgumeli/Nubik/./ku-dependency-parser2/train.jl:55
 [7] load_data(::SubString{String}, ::SubString{String}, ::SubString{String}) at /scratch/users/cgumeli/Nubik/./data.jl:9
 [8] main(::String) at /scratch/users/cgumeli/Nubik/main2.jl:73

julia> exit()
exit()
bash-4.2$ julia
julia
               _
   _       _ _(_)_     |  A fresh approach to technical computing
  (_)     | (_) (_)    |  Documentation: https://docs.julialang.org
   _ _   _| |_  __ _   |  Type "?help" for help.
  | | | | | | |/ _` |  |
  | | |_| | | | (_| |  |  Version 0.6.2 (2017-12-13 18:08 UTC)
 _/ |\__'_|_|_|\__'_|  |  Official http://julialang.org/ release
|__/                   |  x86_64-pc-linux-gnu

WARNING: Terminal not fully functional
julia> main("--datafiles $engtrn $engdev --lmfile $englm --upos 50 --birnnhidden 150 --lmdrop 0.33 --birnndrop 0.33 --decdrop 0.33 --tokens 500 --birnn 2")
main("--datafiles $engtrn $engdev --lmfile $englm --upos 50 --birnnhidden 150 --lmdrop 0.33 --birnndrop 0.33 --decdrop 0.33 --tokens 500 --birnn 2")
ERROR: UndefVarError: main not defined

julia> include("main2.jl")
include("main2.jl")
INFO: Checking dependencies
INFO: All dependencies are installed
accuracy_bt

julia> main("--datafiles $engtrn $engdev --lmfile $englm --upos 50 --birnnhidden 150 --lmdrop 0.33 --birnndrop 0.33 --decdrop 0.33 --tokens 500 --birnn 2")
main("--datafiles $engtrn $engdev --lmfile $englm --upos 50 --birnnhidden 150 --lmdrop 0.33 --birnndrop 0.33 --decdrop 0.33 --tokens 500 --birnn 2")
Tuple{Symbol,Any}[(:feat, 0), (:birnn, 2), (:birnndrop, 0.33), (:lmfile, "/scratch/users/okirnap/ud-treebanks-v2.2/chmodel_converted/english_chmodel.jld"), (:min, 2), (:birnncell, "LSTM"), (:labelunits, 100), (:xpos, 0), (:batchsize, 8), (:max, 128), (:decdrop, 0.33), (:upos, 50), (:backupdir, "Backups_2018-06-08T21:13:45.822"), (:beta1, 0.9), (:remb, 350), (:algo, "edmonds"), (:beta2, 0.999), (:lmdrop, 0.33), (:proj, true), (:tokens, 500), (:epochs, 100), (:recover, ""), (:birnnhidden, 150), (:optim, "Adam"), (:lr, 0.001), (:datafiles, Any["/scratch/users/okirnap/ud-treebanks-v2.2/UD_English-EWT/en_ewt-ud-train.conllu", "/scratch/users/okirnap/ud-treebanks-v2.2/UD_English-EWT/en_ewt-ud-dev.conllu"]), (:arcunits, 400)]
INFO: Loading data
21:13:49 /scratch/users/okirnap/ud-treebanks-v2.2/chmodel_converted/english_chmodel.jld 
21:13:54 /scratch/users/okirnap/ud-treebanks-v2.2/UD_English-EWT/en_ewt-ud-train.conllu 
21:13:55 /scratch/users/okirnap/ud-treebanks-v2.2/UD_English-EWT/en_ewt-ud-dev.conllu 
21:14:54 Caching lm vectors... 
INFO: Bucketing
INFO: Initializing Model and Optimizers
INFO: Transfering to gpu.
INFO: Setting momentums
INFO: Computing validation performance
ERROR: cudart.cudaMemcpy error 77
Stacktrace:
 [1] macro expansion at /scratch/users/cgumeli/.julia/v0.6/Knet/src/gpu.jl:18 [inlined]
 [2] unsafe_copy!(::Knet.KnetArray{Int32,1}, ::Int64, ::Array{Int32,1}, ::Int64, ::Int64) at /scratch/users/cgumeli/.julia/v0.6/Knet/src/karray.jl:340
 [3] getindex(::Knet.KnetArray{Float32,2}, ::Colon, ::Array{Int64,1}) at /scratch/users/cgumeli/.julia/v0.6/Knet/src/karray.jl:652
 [4] (::KnetModules.EmbeddingLookup)(::Array{Any,1}, ::Array{Int64,2}) at /scratch/users/cgumeli/.julia/v0.6/KnetModules/src/embedding.jl:75
 [5] ifexist(::KnetModules.EmbeddingLookup, ::##84#89{LMEncoder,Array{Any,1},Array{Any,1}}) at /scratch/users/cgumeli/Nubik/./encoders/./util.jl:41
 [6] (::LMEncoder)(::Array{Any,1}, ::Array{Any,1}, ::#prep_cavecs) at /scratch/users/cgumeli/Nubik/./encoders/lm.jl:40
 [7] (::FreshEncoder)(::Array{Any,1}, ::Array{Any,1}) at /scratch/users/cgumeli/Nubik/./encoders/fresh.jl:46
 [8] predict at /scratch/users/cgumeli/Nubik/main2.jl:13 [inlined]
 [9] macro expansion at /scratch/users/cgumeli/Nubik/main2.jl:150 [inlined]
 [10] macro expansion at ./util.jl:237 [inlined]
 [11] train() at /scratch/users/cgumeli/Nubik/main2.jl:148
 [12] main(::String) at /scratch/users/cgumeli/Nubik/main2.jl:83

julia>   C-c C-c^C
julia> exit()
exit()
bash-4.2$ julia
julia
               _
   _       _ _(_)_     |  A fresh approach to technical computing
  (_)     | (_) (_)    |  Documentation: https://docs.julialang.org
   _ _   _| |_  __ _   |  Type "?help" for help.
  | | | | | | |/ _` |  |
  | | |_| | | | (_| |  |  Version 0.6.2 (2017-12-13 18:08 UTC)
 _/ |\__'_|_|_|\__'_|  |  Official http://julialang.org/ release
|__/                   |  x86_64-pc-linux-gnu

WARNING: Terminal not fully functional
julia> include("main2.jl")
include("main2.jl")
INFO: Checking dependencies
INFO: All dependencies are installed
accuracy_bt

julia> main("--datafiles $engtrn $engdev --lmfile $englm --upos 50 --birnnhidden 300 --lmdrop 0.33 --birnndrop 0.33 --decdrop 0.33 --tokens 500 --birnn 1")
main("--datafiles $engtrn $engdev --lmfile $englm --upos 50 --birnnhidden 300 --lmdrop 0.33 --birnndrop 0.33 --decdrop 0.33 --tokens 500 --birnn 1")
Tuple{Symbol,Any}[(:feat, 0), (:birnn, 1), (:birnndrop, 0.33), (:lmfile, "/scratch/users/okirnap/ud-treebanks-v2.2/chmodel_converted/english_chmodel.jld"), (:min, 2), (:birnncell, "LSTM"), (:labelunits, 100), (:xpos, 0), (:batchsize, 8), (:max, 128), (:decdrop, 0.33), (:upos, 50), (:backupdir, "Backups_2018-06-08T21:17:33.203"), (:beta1, 0.9), (:remb, 350), (:algo, "edmonds"), (:beta2, 0.999), (:lmdrop, 0.33), (:proj, true), (:tokens, 500), (:epochs, 100), (:recover, ""), (:birnnhidden, 300), (:optim, "Adam"), (:lr, 0.001), (:datafiles, Any["/scratch/users/okirnap/ud-treebanks-v2.2/UD_English-EWT/en_ewt-ud-train.conllu", "/scratch/users/okirnap/ud-treebanks-v2.2/UD_English-EWT/en_ewt-ud-dev.conllu"]), (:arcunits, 400)]
INFO: Loading data
21:17:37 /scratch/users/okirnap/ud-treebanks-v2.2/chmodel_converted/english_chmodel.jld 
21:17:42 /scratch/users/okirnap/ud-treebanks-v2.2/UD_English-EWT/en_ewt-ud-train.conllu 
21:17:43 /scratch/users/okirnap/ud-treebanks-v2.2/UD_English-EWT/en_ewt-ud-dev.conllu 
21:18:40 Caching lm vectors... 
INFO: Bucketing
INFO: Initializing Model and Optimizers
INFO: Transfering to gpu.
INFO: Setting momentums
INFO: Computing validation performance
parsers/mst.py:24: RuntimeWarning: divide by zero encountered in divide
  new_root = tokens[np.argmax(root_scores / head_scores)]
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 58.668354 seconds (18.65 M allocations: 1.638 GiB, 0.60% gc time)
Validation loss: 85.27797
Unlabeled attachment score: 13.518045352922389
Labeled attachment score: 0.23954008304056212

Epoch 1
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
104.406084 seconds (85.97 M allocations: 10.848 GiB, 8.49% gc time)
Training loss: 33.29487


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
  7.952394 seconds (6.77 M allocations: 1.083 GiB, 5.74% gc time)
Validation loss: 14.37314
Unlabeled attachment score: 72.4768444586394
Labeled attachment score: 68.60427978281699
INFO: Backing up

Epoch 2
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 76.109721 seconds (81.16 M allocations: 10.584 GiB, 14.10% gc time)
Training loss: 22.934988


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
  8.349554 seconds (6.77 M allocations: 1.083 GiB, 5.30% gc time)
Validation loss: 12.484049
Unlabeled attachment score: 75.51900351325455
Labeled attachment score: 72.26525071862025
INFO: Backing up

Epoch 3
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 81.728660 seconds (81.20 M allocations: 10.585 GiB, 19.89% gc time)
Training loss: 20.654194


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
  8.800380 seconds (6.77 M allocations: 1.083 GiB, 5.41% gc time)
Validation loss: 11.873992
Unlabeled attachment score: 78.1499520919834
Labeled attachment score: 74.89220696263175
INFO: Backing up

Epoch 4
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 79.684753 seconds (81.18 M allocations: 10.584 GiB, 16.37% gc time)
Training loss: 19.211092


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
  8.951256 seconds (6.77 M allocations: 1.083 GiB, 5.38% gc time)
Validation loss: 11.673092
Unlabeled attachment score: 79.50734589587991
Labeled attachment score: 76.10188438198658
INFO: Backing up

Epoch 5
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 84.606756 seconds (81.20 M allocations: 10.585 GiB, 21.10% gc time)
Training loss: 18.655901


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
  8.917221 seconds (6.77 M allocations: 1.083 GiB, 5.16% gc time)
Validation loss: 11.426393
Unlabeled attachment score: 80.20600447141489
Labeled attachment score: 76.82848930054296
INFO: Backing up

Epoch 6
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 75.426251 seconds (81.20 M allocations: 10.585 GiB, 11.15% gc time)
Training loss: 17.666203


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
  9.446722 seconds (6.77 M allocations: 1.083 GiB, 4.86% gc time)
Validation loss: 11.240106
Unlabeled attachment score: 80.5533375918237
Labeled attachment score: 77.28760779303737
INFO: Backing up

Epoch 7
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 75.872690 seconds (81.20 M allocations: 10.585 GiB, 11.24% gc time)
Training loss: 17.19363


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
  9.679595 seconds (6.77 M allocations: 1.083 GiB, 4.90% gc time)
Validation loss: 11.174525
Unlabeled attachment score: 80.92063238581923
Labeled attachment score: 77.61897157457682
INFO: Backing up

Epoch 8
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 74.844262 seconds (81.20 M allocations: 10.585 GiB, 11.42% gc time)
Training loss: 16.835041


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 10.333830 seconds (6.77 M allocations: 1.083 GiB, 4.62% gc time)
Validation loss: 10.655674
Unlabeled attachment score: 81.47157457681251
Labeled attachment score: 78.40146917917598
INFO: Backing up

Epoch 9
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 77.252376 seconds (81.20 M allocations: 10.585 GiB, 11.40% gc time)
Training loss: 16.429518


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 10.282666 seconds (6.77 M allocations: 1.083 GiB, 4.63% gc time)
Validation loss: 10.764862
Unlabeled attachment score: 80.9086553816672
Labeled attachment score: 77.92238901309486
INFO: Backing up

Epoch 10
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 74.012757 seconds (81.20 M allocations: 10.585 GiB, 11.44% gc time)
Training loss: 16.283998


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
  9.903541 seconds (6.77 M allocations: 1.083 GiB, 4.55% gc time)
Validation loss: 10.711436
Unlabeled attachment score: 81.65122963909295
Labeled attachment score: 78.30565314595975
INFO: Backing up

Epoch 11
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 75.361564 seconds (81.20 M allocations: 10.585 GiB, 11.48% gc time)
Training loss: 15.978686


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 11.022495 seconds (6.77 M allocations: 1.083 GiB, 4.21% gc time)
Validation loss: 10.644215
Unlabeled attachment score: 82.04247844139253
Labeled attachment score: 78.72484829128074
INFO: Backing up

Epoch 12
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 79.282764 seconds (81.21 M allocations: 10.585 GiB, 15.25% gc time)
Training loss: 15.8840475


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 10.573444 seconds (6.77 M allocations: 1.083 GiB, 4.16% gc time)
Validation loss: 10.731536
Unlabeled attachment score: 82.07441711913127
Labeled attachment score: 78.64100926221654
INFO: Backing up

Epoch 13
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 79.027217 seconds (81.20 M allocations: 10.585 GiB, 15.45% gc time)
Training loss: 15.194529


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 10.939326 seconds (6.77 M allocations: 1.083 GiB, 4.22% gc time)
Validation loss: 10.525993
Unlabeled attachment score: 82.3299265410412
Labeled attachment score: 78.95241137016927
INFO: Backing up

Epoch 14
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 73.873997 seconds (81.22 M allocations: 10.585 GiB, 11.77% gc time)
Training loss: 15.063149


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 10.861746 seconds (6.77 M allocations: 1.083 GiB, 4.02% gc time)
Validation loss: 10.457076
Unlabeled attachment score: 82.0305014372405
Labeled attachment score: 78.6729479399553
INFO: Backing up

Epoch 15
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
  C-c C-c^CERROR: InterruptException:
Stacktrace:
 [1] cat(::Type{AutoGrad.Grad{45}}, ::Knet.KnetArray{Float32,2}, ::Knet.KnetArray{Float32,2}, ::Int64, ::AutoGrad.Rec{Knet.KnetArray{Float32,1}}, ::Vararg{AutoGrad.Rec{Knet.KnetArray{Float32,1}},N} where N) at /scratch/users/cgumeli/.julia/v0.6/Knet/src/karray.jl:0
 [2] backward_pass(::AutoGrad.Rec{Array{Any,1}}, ::AutoGrad.Rec{Float32}, ::Array{AutoGrad.Node,1}) at /scratch/users/cgumeli/.julia/v0.6/AutoGrad/src/core.jl:256
 [3] (::AutoGrad.##gradfun#4#6{#loss,Int64})(::Array{Any,1}, ::Function, ::Array{Any,1}, ::Vararg{Any,N} where N) at /scratch/users/cgumeli/.julia/v0.6/AutoGrad/src/core.jl:62
 [4] (::AutoGrad.#gradfun#5)(::Array{Any,1}, ::Vararg{Any,N} where N) at /scratch/users/cgumeli/.julia/v0.6/AutoGrad/src/core.jl:57
 [5] macro expansion at /scratch/users/cgumeli/Nubik/main2.jl:192 [inlined]
 [6] macro expansion at ./util.jl:237 [inlined]
 [7] train() at /scratch/users/cgumeli/Nubik/main2.jl:190
 [8] main(::String) at /scratch/users/cgumeli/Nubik/main2.jl:83

julia> main("--datafiles $engtrn $engdev --lmfile $englm --upos 150 --birnnhidden 300 --lmdrop 0.33 --birnndrop 0.33 --decdrop 0.33 --tokens 500 --birnn 1")
main("--datafiles $engtrn $engdev --lmfile $englm --upos 150 --birnnhidden 300 --lmdrop 0.33 --birnndrop 0.33 --decdrop 0.33 --tokens 500 --birnn 1")
Tuple{Symbol,Any}[(:feat, 0), (:birnn, 1), (:birnndrop, 0.33), (:lmfile, "/scratch/users/okirnap/ud-treebanks-v2.2/chmodel_converted/english_chmodel.jld"), (:min, 2), (:birnncell, "LSTM"), (:labelunits, 100), (:xpos, 0), (:batchsize, 8), (:max, 128), (:decdrop, 0.33), (:upos, 150), (:backupdir, "Backups_2018-06-08T21:41:13.426"), (:beta1, 0.9), (:remb, 350), (:algo, "edmonds"), (:beta2, 0.999), (:lmdrop, 0.33), (:proj, true), (:tokens, 500), (:epochs, 100), (:recover, ""), (:birnnhidden, 300), (:optim, "Adam"), (:lr, 0.001), (:datafiles, Any["/scratch/users/okirnap/ud-treebanks-v2.2/UD_English-EWT/en_ewt-ud-train.conllu", "/scratch/users/okirnap/ud-treebanks-v2.2/UD_English-EWT/en_ewt-ud-dev.conllu"]), (:arcunits, 400)]
INFO: Loading data
21:41:13 /scratch/users/okirnap/ud-treebanks-v2.2/chmodel_converted/english_chmodel.jld 
21:41:15 /scratch/users/okirnap/ud-treebanks-v2.2/UD_English-EWT/en_ewt-ud-train.conllu 
21:41:17 /scratch/users/okirnap/ud-treebanks-v2.2/UD_English-EWT/en_ewt-ud-dev.conllu 
21:43:06 Caching lm vectors... 
INFO: Bucketing
INFO: Initializing Model and Optimizers
INFO: Transfering to gpu.
INFO: Setting momentums
INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 12.242623 seconds (6.77 M allocations: 1.083 GiB, 12.98% gc time)
Validation loss: 86.61932
Unlabeled attachment score: 4.375598850207601
Labeled attachment score: 0.31539444267007344

Epoch 1
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 79.226613 seconds (81.20 M allocations: 10.586 GiB, 15.10% gc time)
Training loss: 34.679695


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 11.025178 seconds (6.77 M allocations: 1.083 GiB, 4.11% gc time)
Validation loss: 15.36127
Unlabeled attachment score: 70.1293516448419
Labeled attachment score: 66.00526988182689
INFO: Backing up

Epoch 2
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 77.130520 seconds (81.23 M allocations: 10.586 GiB, 11.68% gc time)
Training loss: 24.525906


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 11.190203 seconds (6.77 M allocations: 1.083 GiB, 4.03% gc time)
Validation loss: 13.168882
Unlabeled attachment score: 75.98211434046631
Labeled attachment score: 72.157457681252
INFO: Backing up

Epoch 3
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 74.288085 seconds (81.25 M allocations: 10.586 GiB, 11.66% gc time)
Training loss: 21.839046


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 11.267813 seconds (6.77 M allocations: 1.083 GiB, 3.79% gc time)
Validation loss: 12.416387
Unlabeled attachment score: 77.60699457042479
Labeled attachment score: 74.4770041520281
INFO: Backing up

Epoch 4
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 70.982360 seconds (81.25 M allocations: 10.586 GiB, 11.58% gc time)
Training loss: 19.989225


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 11.609681 seconds (6.77 M allocations: 1.083 GiB, 3.71% gc time)
Validation loss: 11.774062
Unlabeled attachment score: 79.79080166081124
Labeled attachment score: 76.65681890769721
INFO: Backing up

Epoch 5
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 72.590961 seconds (81.25 M allocations: 10.586 GiB, 11.77% gc time)
Training loss: 19.19612


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 12.233453 seconds (6.77 M allocations: 1.083 GiB, 3.74% gc time)
Validation loss: 11.992711
Unlabeled attachment score: 79.79878633024593
Labeled attachment score: 76.42925582880869
INFO: Backing up

Epoch 6
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 75.959373 seconds (81.25 M allocations: 10.586 GiB, 11.43% gc time)
Training loss: 18.653553


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 12.456392 seconds (6.77 M allocations: 1.083 GiB, 3.60% gc time)
Validation loss: 11.39632
Unlabeled attachment score: 79.56723091664006
Labeled attachment score: 76.55701053976365
INFO: Backing up

Epoch 7
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 73.864630 seconds (81.25 M allocations: 10.586 GiB, 11.83% gc time)
Training loss: 17.619995


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 12.691457 seconds (6.77 M allocations: 1.083 GiB, 3.55% gc time)
Validation loss: 11.209604
Unlabeled attachment score: 79.76684765250718
Labeled attachment score: 76.55301820504631
INFO: Backing up

Epoch 8
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
  C-c C-c^CERROR: InterruptException:
Stacktrace:
 [1] _collect(::UnitRange{Int64}, ::Base.Iterators.Flatten{Base.Iterators.Zip{Array{UInt8,1},Base.Iterators.Zip{Array{UInt8,1},Base.Iterators.Zip{Array{UInt8,1},Base.Iterators.Zip{Array{UInt8,1},Base.Iterators.Zip{Array{UInt8,1},Base.Iterators.Zip{Array{UInt8,1},Base.Iterators.Zip{Array{UInt8,1},Base.Iterators.Zip{Array{UInt8,1},Base.Iterators.Zip{Array{UInt8,1},Base.Iterators.Zip{Array{UInt8,1},Base.Iterators.Zip{Array{UInt8,1},Base.Iterators.Zip{Array{UInt8,1},Base.Iterators.Zip{Array{UInt8,1},Base.Iterators.Zip{Array{UInt8,1},Base.Iterators.Zip{Array{UInt8,1},Base.Iterators.Zip{Array{UInt8,1},Base.Iterators.Zip{Array{UInt8,1},Base.Iterators.Zip2{Array{UInt8,1},Array{UInt8,1}}}}}}}}}}}}}}}}}}}}, ::Base.HasEltype, ::Base.SizeUnknown) at ./array.jl:442
 [2] collect(::Base.Iterators.Flatten{Base.Iterators.Zip{Array{UInt8,1},Base.Iterators.Zip{Array{UInt8,1},Base.Iterators.Zip{Array{UInt8,1},Base.Iterators.Zip{Array{UInt8,1},Base.Iterators.Zip{Array{UInt8,1},Base.Iterators.Zip{Array{UInt8,1},Base.Iterators.Zip{Array{UInt8,1},Base.Iterators.Zip{Array{UInt8,1},Base.Iterators.Zip{Array{UInt8,1},Base.Iterators.Zip{Array{UInt8,1},Base.Iterators.Zip{Array{UInt8,1},Base.Iterators.Zip{Array{UInt8,1},Base.Iterators.Zip{Array{UInt8,1},Base.Iterators.Zip{Array{UInt8,1},Base.Iterators.Zip{Array{UInt8,1},Base.Iterators.Zip{Array{UInt8,1},Base.Iterators.Zip{Array{UInt8,1},Base.Iterators.Zip2{Array{UInt8,1},Array{UInt8,1}}}}}}}}}}}}}}}}}}}}) at ./array.jl:431
 [3] softloss(::BiaffineDecoder2, ::AutoGrad.Rec{Knet.KnetArray{Float32,3}}, ::AutoGrad.Rec{Knet.KnetArray{Float32,3}}, ::Array{Any,1}) at /scratch/users/cgumeli/Nubik/./decoders/biaffine2.jl:120
 [4] loss(::AutoGrad.Rec{Array{Any,1}}, ::FreshEncoder, ::BiaffineDecoder2, ::Array{Any,1}) at /scratch/users/cgumeli/Nubik/main2.jl:20
 [5] forward_pass(::Function, ::Tuple{Array{Any,1},FreshEncoder,BiaffineDecoder2,Array{Any,1}}, ::Array{Any,1}, ::Int64) at /scratch/users/cgumeli/.julia/v0.6/AutoGrad/src/core.jl:88
 [6] (::AutoGrad.##gradfun#4#6{#loss,Int64})(::Array{Any,1}, ::Function, ::Array{Any,1}, ::Vararg{Any,N} where N) at /scratch/users/cgumeli/.julia/v0.6/AutoGrad/src/core.jl:57
 [7] (::AutoGrad.#gradfun#5)(::Array{Any,1}, ::Vararg{Any,N} where N) at /scratch/users/cgumeli/.julia/v0.6/AutoGrad/src/core.jl:57
 [8] macro expansion at /scratch/users/cgumeli/Nubik/main2.jl:192 [inlined]
 [9] macro expansion at ./util.jl:237 [inlined]
 [10] train() at /scratch/users/cgumeli/Nubik/main2.jl:190
 [11] main(::String) at /scratch/users/cgumeli/Nubik/main2.jl:83

julia> 


julia> 


julia> 


julia> 


julia> exit()
exit()
bash-4.2$ julia
julia
               _
   _       _ _(_)_     |  A fresh approach to technical computing
  (_)     | (_) (_)    |  Documentation: https://docs.julialang.org
   _ _   _| |_  __ _   |  Type "?help" for help.
  | | | | | | |/ _` |  |
  | | |_| | | | (_| |  |  Version 0.6.2 (2017-12-13 18:08 UTC)
 _/ |\__'_|_|_|\__'_|  |  Official http://julialang.org/ release
|__/                   |  x86_64-pc-linux-gnu

WARNING: Terminal not fully functional
julia> main("--datafiles $engtrn $engdev --lmfile $englm --upos 150 --birnnhidden 300 --lmdrop 0.33 --birnndrop 0.33 --decdrop 0.33 --tokens 500 --birnn 1")  C-c C-c^C
julia> include("main2.jl")
include("main2.jl")
INFO: Checking dependencies
INFO: All dependencies are installed
accuracy_bt

julia> main("--datafiles $engtrn $engdev --lmfile $englm --upos 50 --birnnhidden 200 --lmdrop 0.33 --birnndrop 0.33 --decdrop 0.33 --tokens 500 --birnn 2")
main("--datafiles $engtrn $engdev --lmfile $englm --upos 50 --birnnhidden 200 --lmdrop 0.33 --birnndrop 0.33 --decdrop 0.33 --tokens 500 --birnn 2")
Tuple{Symbol,Any}[(:feat, 0), (:birnn, 2), (:birnndrop, 0.33), (:lmfile, "/scratch/users/okirnap/ud-treebanks-v2.2/chmodel_converted/english_chmodel.jld"), (:min, 2), (:birnncell, "LSTM"), (:labelunits, 100), (:xpos, 0), (:batchsize, 8), (:max, 128), (:decdrop, 0.33), (:upos, 50), (:backupdir, "Backups_2018-06-08T22:41:32.434"), (:beta1, 0.9), (:remb, 350), (:algo, "edmonds"), (:beta2, 0.999), (:lmdrop, 0.33), (:proj, true), (:tokens, 500), (:epochs, 100), (:recover, ""), (:birnnhidden, 200), (:optim, "Adam"), (:lr, 0.001), (:datafiles, Any["/scratch/users/okirnap/ud-treebanks-v2.2/UD_English-EWT/en_ewt-ud-train.conllu", "/scratch/users/okirnap/ud-treebanks-v2.2/UD_English-EWT/en_ewt-ud-dev.conllu"]), (:arcunits, 400)]
INFO: Loading data
22:41:36 /scratch/users/okirnap/ud-treebanks-v2.2/chmodel_converted/english_chmodel.jld 
22:41:41 /scratch/users/okirnap/ud-treebanks-v2.2/UD_English-EWT/en_ewt-ud-train.conllu 
22:41:42 /scratch/users/okirnap/ud-treebanks-v2.2/UD_English-EWT/en_ewt-ud-dev.conllu 
  C-c C-c^CERROR: InterruptException:
Stacktrace:
 [1] Knet.KnetPtr(::Int64) at /scratch/users/cgumeli/.julia/v0.6/Knet/src/kptr.jl:80
 [2] broadcast#*(::Knet.KnetArray{Float32,2}, ::Knet.KnetArray{Float32,2}) at /scratch/users/cgumeli/.julia/v0.6/Knet/src/broadcast.jl:72
 [3] *(::AutoGrad.Broadcasted{Knet.KnetArray{Float32,2}}, ::AutoGrad.Broadcasted{Knet.KnetArray{Float32,2}}) at /scratch/users/cgumeli/.julia/v0.6/AutoGrad/src/unfuse.jl:49
 [4] broadcast(::Function, ::Knet.KnetArray{Float32,2}, ::Knet.KnetArray{Float32,2}) at /scratch/users/cgumeli/.julia/v0.6/Knet/src/unfuse.jl:9
 [5] #_lstm#9(::Knet.KnetArray{Float32,1}, ::Function, ::Knet.KnetArray{Float32,2}, ::Knet.KnetArray{Float32,2}, ::Knet.KnetArray{Float32,2}, ::Knet.KnetArray{Float32,2}, ::Knet.KnetArray{Float32,2}) at /scratch/users/cgumeli/Nubik/./ku-dependency-parser2/src/lm_utils.jl:155
 [6] (::#kw##_lstm)(::Array{Any,1}, ::#_lstm, ::Knet.KnetArray{Float32,2}, ::Knet.KnetArray{Float32,2}, ::Knet.KnetArray{Float32,2}, ::Knet.KnetArray{Float32,2}, ::Knet.KnetArray{Float32,2}) at ./<missing>:0
 [7] charlstm(::Array{Knet.KnetArray{Float32,2},1}, ::Array{Array{Int64,1},1}, ::Array{Array{Float32,1},1}) at /scratch/users/cgumeli/Nubik/./ku-dependency-parser2/src/lm_utils.jl:180
 [8] #fillvecs!#16(::Int64, ::Function, ::Array{Knet.KnetArray{Float32,2},1}, ::Array{Any,1}, ::Vocab) at /scratch/users/cgumeli/Nubik/./ku-dependency-parser2/src/lm_utils.jl:308
 [9] #depmain#46(::Bool, ::Function, ::String) at /scratch/users/cgumeli/Nubik/./ku-dependency-parser2/train.jl:55
 [10] load_data(::SubString{String}, ::SubString{String}, ::SubString{String}) at /scratch/users/cgumeli/Nubik/./data.jl:9
 [11] main(::String) at /scratch/users/cgumeli/Nubik/main2.jl:73

julia> exit()
exit()
bash-4.2$ julia
julia
               _
   _       _ _(_)_     |  A fresh approach to technical computing
  (_)     | (_) (_)    |  Documentation: https://docs.julialang.org
   _ _   _| |_  __ _   |  Type "?help" for help.
  | | | | | | |/ _` |  |
  | | |_| | | | (_| |  |  Version 0.6.2 (2017-12-13 18:08 UTC)
 _/ |\__'_|_|_|\__'_|  |  Official http://julialang.org/ release
|__/                   |  x86_64-pc-linux-gnu

WARNING: Terminal not fully functional
julia> main("--datafiles $engtrn $engdev --lmfile $englm --upos 50 --birnnhidden 200 --lmdrop 0.33 --birnndrop 0.33 --decdrop 0.33 --tokens 500 --birnn 2")  C-c C-c^C
julia> include("main2.jl")
include("main2.jl")
INFO: Checking dependencies
INFO: All dependencies are installed
accuracy_bt

julia> main("--datafiles $engtrn $engdev --lmfile $englm --upos 50 --birnnhidden 200 --lmdrop 0.33 --birnndrop 0.33 --decdrop 0.33 --tokens 500 --birnn 2")
main("--datafiles $engtrn $engdev --lmfile $englm --upos 50 --birnnhidden 200 --lmdrop 0.33 --birnndrop 0.33 --decdrop 0.33 --tokens 500 --birnn 2")
Tuple{Symbol,Any}[(:feat, 0), (:birnn, 2), (:birnndrop, 0.33), (:lmfile, "/scratch/users/okirnap/ud-treebanks-v2.2/chmodel_converted/english_chmodel.jld"), (:min, 2), (:birnncell, "LSTM"), (:labelunits, 100), (:xpos, 0), (:batchsize, 8), (:max, 128), (:decdrop, 0.33), (:upos, 50), (:backupdir, "Backups_2018-06-08T22:43:13.03"), (:beta1, 0.9), (:remb, 350), (:algo, "edmonds"), (:beta2, 0.999), (:lmdrop, 0.33), (:proj, true), (:tokens, 500), (:epochs, 100), (:recover, ""), (:birnnhidden, 200), (:optim, "Adam"), (:lr, 0.001), (:datafiles, Any["/scratch/users/okirnap/ud-treebanks-v2.2/UD_English-EWT/en_ewt-ud-train.conllu", "/scratch/users/okirnap/ud-treebanks-v2.2/UD_English-EWT/en_ewt-ud-dev.conllu"]), (:arcunits, 400)]
INFO: Loading data
22:43:16 /scratch/users/okirnap/ud-treebanks-v2.2/chmodel_converted/english_chmodel.jld 
22:43:21 /scratch/users/okirnap/ud-treebanks-v2.2/UD_English-EWT/en_ewt-ud-train.conllu 
22:43:23 /scratch/users/okirnap/ud-treebanks-v2.2/UD_English-EWT/en_ewt-ud-dev.conllu 
22:44:20 Caching lm vectors... 
INFO: Bucketing
INFO: Initializing Model and Optimizers
ERROR: MethodError: no method matching _BiRNN(::Int64, ::Int64; dropout=0.33f0, numLayers=2, nhidden=200, ninput=400, Cell=KnetModules.LSTM)
Closest candidates are:
  _BiRNN(::Any, ::Any; dropout, numLayers, Cell) at /scratch/users/cgumeli/Nubik/./encoders/fresh.jl:12 got unsupported keyword arguments "nhidden", "ninput"
Stacktrace:
 [1] (::Core.#kw#Type)(::Array{Any,1}, ::Type{_BiRNN}, ::Int64, ::Int64) at ./<missing>:0
 [2] #FreshEncoder#101(::Int64, ::Bool, ::Int64, ::Int64, ::Int64, ::Int64, ::Float32, ::Type{T} where T, ::Dict{Any,Any}, ::Float32, ::Bool, ::Int64, ::Array{Any,1}, ::Type{T} where T, ::ExtendedVocab) at /scratch/users/cgumeli/Nubik/./encoders/fresh.jl:67
 [3] (::Core.#kw#Type)(::Array{Any,1}, ::Type{FreshEncoder}, ::ExtendedVocab) at ./<missing>:0
 [4] train() at /scratch/users/cgumeli/Nubik/main2.jl:94
 [5] main(::String) at /scratch/users/cgumeli/Nubik/main2.jl:83

julia> include("main2.jl")
include("main2.jl")
INFO: Checking dependencies
INFO: All dependencies are installed
WARNING: redefining constant UPOSTAG
WARNING: redefining constant UDEPREL
WARNING: redefining constant lmdir
WARNING: redefining constant datadir
WARNING: redefining constant engtrn
WARNING: redefining constant engdev
WARNING: redefining constant englm
accuracy_bt

julia> main("--datafiles $engtrn $engdev --lmfile $englm --upos 50 --birnnhidden 200 --lmdrop 0.33 --birnndrop 0.33 --decdrop 0.33 --tokens 500 --birnn 2")
main("--datafiles $engtrn $engdev --lmfile $englm --upos 50 --birnnhidden 200 --lmdrop 0.33 --birnndrop 0.33 --decdrop 0.33 --tokens 500 --birnn 2")
Tuple{Symbol,Any}[(:feat, 0), (:birnn, 2), (:birnndrop, 0.33), (:lmfile, "/scratch/users/okirnap/ud-treebanks-v2.2/chmodel_converted/english_chmodel.jld"), (:min, 2), (:birnncell, "LSTM"), (:labelunits, 100), (:xpos, 0), (:batchsize, 8), (:max, 128), (:decdrop, 0.33), (:upos, 50), (:backupdir, "Backups_2018-06-08T22:45:30.815"), (:beta1, 0.9), (:remb, 350), (:algo, "edmonds"), (:beta2, 0.999), (:lmdrop, 0.33), (:proj, true), (:tokens, 500), (:epochs, 100), (:recover, ""), (:birnnhidden, 200), (:optim, "Adam"), (:lr, 0.001), (:datafiles, Any["/scratch/users/okirnap/ud-treebanks-v2.2/UD_English-EWT/en_ewt-ud-train.conllu", "/scratch/users/okirnap/ud-treebanks-v2.2/UD_English-EWT/en_ewt-ud-dev.conllu"]), (:arcunits, 400)]
INFO: Loading data
22:45:31 /scratch/users/okirnap/ud-treebanks-v2.2/chmodel_converted/english_chmodel.jld 
22:45:32 /scratch/users/okirnap/ud-treebanks-v2.2/UD_English-EWT/en_ewt-ud-train.conllu 
22:45:34 /scratch/users/okirnap/ud-treebanks-v2.2/UD_English-EWT/en_ewt-ud-dev.conllu 
22:46:50 Caching lm vectors... 
INFO: Bucketing
INFO: Initializing Model and Optimizers
INFO: Transfering to gpu.
INFO: Setting momentums
INFO: Computing validation performance
ERROR: DimensionMismatch("((1000, 800), (400, 411), (1000, 411))")
Stacktrace:
 [1] gemm!(::Char, ::Char, ::Float32, ::Knet.KnetArray{Float32,2}, ::Knet.KnetArray{Float32,2}, ::Float32, ::Knet.KnetArray{Float32,2}) at /scratch/users/cgumeli/.julia/v0.6/Knet/src/linalg.jl:37
 [2] *(::Knet.KnetArray{Float32,2}, ::Knet.KnetArray{Float32,2}) at /scratch/users/cgumeli/.julia/v0.6/Knet/src/linalg.jl:10
 [3] (::KnetModules.Linear)(::Array{Any,1}, ::Knet.KnetArray{Float32,2}) at /scratch/users/cgumeli/.julia/v0.6/KnetModules/src/linear.jl:42
 [4] (::KnetModules.Sequential)(::Array{Any,1}, ::Knet.KnetArray{Float32,2}) at /scratch/users/cgumeli/.julia/v0.6/KnetModules/src/container.jl:55
 [5] #call#180(::Bool, ::BiaffineDecoder2, ::Array{Any,1}, ::Knet.KnetArray{Float32,3}) at /scratch/users/cgumeli/Nubik/./decoders/biaffine2.jl:70
 [6] macro expansion at /scratch/users/cgumeli/Nubik/main2.jl:149 [inlined]
 [7] macro expansion at ./util.jl:237 [inlined]
 [8] train() at /scratch/users/cgumeli/Nubik/main2.jl:147
 [9] main(::String) at /scratch/users/cgumeli/Nubik/main2.jl:83

julia> include("main2.jl")
include("main2.jl")
INFO: Checking dependencies
INFO: All dependencies are installed
WARNING: redefining constant UPOSTAG
WARNING: redefining constant UDEPREL
WARNING: redefining constant lmdir
WARNING: redefining constant datadir
WARNING: redefining constant engtrn
WARNING: redefining constant engdev
WARNING: redefining constant englm
accuracy_bt

julia> main("--datafiles $engtrn $engdev --lmfile $englm --upos 50 --birnnhidden 200 --lmdrop 0.33 --birnndrop 0.33 --decdrop 0.33 --tokens 500 --birnn 2")
main("--datafiles $engtrn $engdev --lmfile $englm --upos 50 --birnnhidden 200 --lmdrop 0.33 --birnndrop 0.33 --decdrop 0.33 --tokens 500 --birnn 2")
Tuple{Symbol,Any}[(:feat, 0), (:birnn, 2), (:birnndrop, 0.33), (:lmfile, "/scratch/users/okirnap/ud-treebanks-v2.2/chmodel_converted/english_chmodel.jld"), (:min, 2), (:birnncell, "LSTM"), (:labelunits, 100), (:xpos, 0), (:batchsize, 8), (:max, 128), (:decdrop, 0.33), (:upos, 50), (:backupdir, "Backups_2018-06-08T22:48:07.916"), (:beta1, 0.9), (:remb, 350), (:algo, "edmonds"), (:beta2, 0.999), (:lmdrop, 0.33), (:proj, true), (:tokens, 500), (:epochs, 100), (:recover, ""), (:birnnhidden, 200), (:optim, "Adam"), (:lr, 0.001), (:datafiles, Any["/scratch/users/okirnap/ud-treebanks-v2.2/UD_English-EWT/en_ewt-ud-train.conllu", "/scratch/users/okirnap/ud-treebanks-v2.2/UD_English-EWT/en_ewt-ud-dev.conllu"]), (:arcunits, 400)]
INFO: Loading data
22:48:08 /scratch/users/okirnap/ud-treebanks-v2.2/chmodel_converted/english_chmodel.jld 
22:48:10 /scratch/users/okirnap/ud-treebanks-v2.2/UD_English-EWT/en_ewt-ud-train.conllu 
22:48:11 /scratch/users/okirnap/ud-treebanks-v2.2/UD_English-EWT/en_ewt-ud-dev.conllu 
22:50:22 Caching lm vectors... 
INFO: Bucketing
INFO: Initializing Model and Optimizers
INFO: Transfering to gpu.
INFO: Setting momentums
INFO: Computing validation performance
parsers/mst.py:24: RuntimeWarning: divide by zero encountered in divide
  new_root = tokens[np.argmax(root_scores / head_scores)]
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 57.740031 seconds (17.64 M allocations: 1.583 GiB, 3.61% gc time)
Validation loss: 85.99534
Unlabeled attachment score: 7.320661682837601
Labeled attachment score: 0.5328455543184348

Epoch 1
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
111.488089 seconds (89.14 M allocations: 10.963 GiB, 8.71% gc time)
Training loss: 32.223866


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
  8.771886 seconds (6.90 M allocations: 1.087 GiB, 5.20% gc time)
Validation loss: 14.327023
Unlabeled attachment score: 70.4151423572451
Labeled attachment score: 66.8919993637665
INFO: Backing up

Epoch 2
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 79.923771 seconds (84.25 M allocations: 10.689 GiB, 11.35% gc time)
Training loss: 21.790596


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
  9.212721 seconds (6.90 M allocations: 1.087 GiB, 5.29% gc time)
Validation loss: 12.494417
Unlabeled attachment score: 72.85271194528391
Labeled attachment score: 70.02544933990775
INFO: Backing up

Epoch 3
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 86.702891 seconds (84.25 M allocations: 10.688 GiB, 14.28% gc time)
Training loss: 18.91831


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
  9.236710 seconds (6.90 M allocations: 1.087 GiB, 5.17% gc time)
Validation loss: 11.167566
Unlabeled attachment score: 76.94846508668681
Labeled attachment score: 74.27230793701288
INFO: Backing up

Epoch 4
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 87.288195 seconds (84.27 M allocations: 10.689 GiB, 15.15% gc time)
Training loss: 17.23173


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
  9.410853 seconds (6.90 M allocations: 1.087 GiB, 5.29% gc time)
Validation loss: 10.654734
Unlabeled attachment score: 78.69413074598377
Labeled attachment score: 76.1134086209639
INFO: Backing up

Epoch 5
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 80.694612 seconds (84.26 M allocations: 10.688 GiB, 12.40% gc time)
Training loss: 16.268053


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 10.001488 seconds (6.90 M allocations: 1.087 GiB, 4.90% gc time)
Validation loss: 10.426878
Unlabeled attachment score: 79.53714013042787
Labeled attachment score: 76.99220613965325
INFO: Backing up

Epoch 6
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 82.771964 seconds (84.30 M allocations: 10.689 GiB, 12.22% gc time)
Training loss: 15.413699


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 15.867464 seconds (7.14 M allocations: 1.092 GiB, 13.20% gc time)
Validation loss: 10.118663
Unlabeled attachment score: 80.45967870208366
Labeled attachment score: 77.95450930491491
INFO: Backing up

Epoch 7
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 75.796600 seconds (84.04 M allocations: 10.684 GiB, 9.31% gc time)
Training loss: 14.677598


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 15.154972 seconds (7.16 M allocations: 1.092 GiB, 13.37% gc time)
Validation loss: 9.897866
Unlabeled attachment score: 80.08589152218865
Labeled attachment score: 77.63639255606807
INFO: Backing up

Epoch 8
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 85.869055 seconds (84.28 M allocations: 10.689 GiB, 13.48% gc time)
Training loss: 14.016496


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 10.249341 seconds (6.90 M allocations: 1.087 GiB, 4.55% gc time)
Validation loss: 9.800752
Unlabeled attachment score: 82.78590742802608
Labeled attachment score: 80.19325592492444
INFO: Backing up

Epoch 9
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 82.706356 seconds (84.30 M allocations: 10.689 GiB, 11.50% gc time)
Training loss: 13.430708


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 10.754586 seconds (6.90 M allocations: 1.087 GiB, 4.83% gc time)
Validation loss: 9.585554
Unlabeled attachment score: 82.70240178145379
Labeled attachment score: 80.14156195323683
INFO: Backing up

Epoch 10
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 82.046452 seconds (84.23 M allocations: 10.688 GiB, 11.65% gc time)
Training loss: 13.126312


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 11.099094 seconds (6.90 M allocations: 1.087 GiB, 4.62% gc time)
Validation loss: 9.658713
Unlabeled attachment score: 83.49371719421028
Labeled attachment score: 80.91697152855097
INFO: Backing up

Epoch 11
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 83.347119 seconds (84.25 M allocations: 10.688 GiB, 11.01% gc time)
Training loss: 12.948024


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 11.016888 seconds (6.90 M allocations: 1.087 GiB, 4.72% gc time)
Validation loss: 9.760761
Unlabeled attachment score: 83.58517575950374
Labeled attachment score: 81.06410052489264
INFO: Backing up

Epoch 12
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 86.498962 seconds (84.27 M allocations: 10.689 GiB, 14.34% gc time)
Training loss: 12.45169


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 11.396843 seconds (6.90 M allocations: 1.087 GiB, 4.31% gc time)
Validation loss: 9.648134
Unlabeled attachment score: 83.15571814856052
Labeled attachment score: 80.67440750755527
INFO: Backing up

Epoch 13
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 81.716312 seconds (84.28 M allocations: 10.689 GiB, 11.86% gc time)
Training loss: 12.16117


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 17.062149 seconds (7.15 M allocations: 1.092 GiB, 13.03% gc time)
Validation loss: 9.722579
Unlabeled attachment score: 83.52155240973437
Labeled attachment score: 80.95673612215683
INFO: Backing up

Epoch 14
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 75.919435 seconds (84.04 M allocations: 10.685 GiB, 9.48% gc time)
Training loss: 11.971345


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 16.883821 seconds (7.17 M allocations: 1.092 GiB, 13.06% gc time)
Validation loss: 9.719349
Unlabeled attachment score: 84.01463337044696
Labeled attachment score: 81.48958167647527
INFO: Backing up

Epoch 15
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 87.357577 seconds (84.28 M allocations: 10.689 GiB, 12.94% gc time)
Training loss: 11.721267


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 12.004679 seconds (6.90 M allocations: 1.087 GiB, 3.91% gc time)
Validation loss: 9.680625
Unlabeled attachment score: 84.31286782249086
Labeled attachment score: 81.779863209798
INFO: Backing up

Epoch 16
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 81.737886 seconds (84.30 M allocations: 10.689 GiB, 12.28% gc time)
Training loss: 11.461455


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 12.850127 seconds (6.90 M allocations: 1.087 GiB, 3.93% gc time)
Validation loss: 9.781024
Unlabeled attachment score: 84.07825672021632
Labeled attachment score: 81.59296961985048
INFO: Backing up

Epoch 17
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 83.326021 seconds (84.24 M allocations: 10.688 GiB, 12.28% gc time)
Training loss: 11.493533


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 12.918118 seconds (6.90 M allocations: 1.087 GiB, 3.85% gc time)
Validation loss: 9.605635
Unlabeled attachment score: 84.53157308732305
Labeled attachment score: 82.03038014951487
INFO: Backing up

Epoch 18
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 84.896273 seconds (84.25 M allocations: 10.688 GiB, 11.31% gc time)
Training loss: 11.320264


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 12.658627 seconds (6.90 M allocations: 1.087 GiB, 3.95% gc time)
Validation loss: 10.037478
Unlabeled attachment score: 84.4639732781931
Labeled attachment score: 82.04230952759663
INFO: Backing up

Epoch 19
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 90.795244 seconds (84.27 M allocations: 10.689 GiB, 14.21% gc time)
Training loss: 11.094048


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 13.278235 seconds (6.90 M allocations: 1.087 GiB, 3.89% gc time)
Validation loss: 9.990187
Unlabeled attachment score: 84.46794973755368
Labeled attachment score: 82.0224272307937
INFO: Backing up

Epoch 20
  C-c C-c^CERROR: InterruptException:
Stacktrace:
 [1] permutedims(::Knet.KnetArray{Float32,3}, ::Tuple{Int64,Int64,Int64}) at /scratch/users/cgumeli/.julia/v0.6/Knet/src/linalg.jl:163
 [2] permutedims(::Type{AutoGrad.Grad{1}}, ::Knet.KnetArray{Float32,3}, ::Knet.KnetArray{Float32,3}, ::AutoGrad.Rec{Knet.KnetArray{Float32,3}}, ::Tuple{Int64,Int64,Int64}, ::Vararg{Tuple{Int64,Int64,Int64},N} where N) at ./<missing>:0
 [3] backward_pass(::AutoGrad.Rec{Array{Any,1}}, ::AutoGrad.Rec{Float32}, ::Array{AutoGrad.Node,1}) at /scratch/users/cgumeli/.julia/v0.6/AutoGrad/src/core.jl:256
 [4] (::AutoGrad.##gradfun#4#6{#loss,Int64})(::Array{Any,1}, ::Function, ::Array{Any,1}, ::Vararg{Any,N} where N) at /scratch/users/cgumeli/.julia/v0.6/AutoGrad/src/core.jl:62
 [5] (::AutoGrad.#gradfun#5)(::Array{Any,1}, ::Vararg{Any,N} where N) at /scratch/users/cgumeli/.julia/v0.6/AutoGrad/src/core.jl:57
 [6] macro expansion at /scratch/users/cgumeli/Nubik/main2.jl:194 [inlined]
 [7] macro expansion at ./util.jl:237 [inlined]
 [8] train() at /scratch/users/cgumeli/Nubik/main2.jl:192
 [9] main(::String) at /scratch/users/cgumeli/Nubik/main2.jl:83

julia> exit()
exit()
bash-4.2$ julia
julia
               _
   _       _ _(_)_     |  A fresh approach to technical computing
  (_)     | (_) (_)    |  Documentation: https://docs.julialang.org
   _ _   _| |_  __ _   |  Type "?help" for help.
  | | | | | | |/ _` |  |
  | | |_| | | | (_| |  |  Version 0.6.2 (2017-12-13 18:08 UTC)
 _/ |\__'_|_|_|\__'_|  |  Official http://julialang.org/ release
|__/                   |  x86_64-pc-linux-gnu

WARNING: Terminal not fully functional
julia> include("main2.jl")
include("main2.jl")
INFO: Checking dependencies
INFO: All dependencies are installed
accuracy_bt

julia> main("--datafiles $engtrn $engdev --lmfile $englm --upos 50 --birnnhidden 200 --lmdrop 0.33 --birnndrop 0.33 --decdrop 0.33 --tokens 500 --birnn 3")
main("--datafiles $engtrn $engdev --lmfile $englm --upos 50 --birnnhidden 200 --lmdrop 0.33 --birnndrop 0.33 --decdrop 0.33 --tokens 500 --birnn 3")
Tuple{Symbol,Any}[(:feat, 0), (:birnn, 3), (:birnndrop, 0.33), (:lmfile, "/scratch/users/okirnap/ud-treebanks-v2.2/chmodel_converted/english_chmodel.jld"), (:min, 2), (:birnncell, "LSTM"), (:labelunits, 100), (:xpos, 0), (:batchsize, 8), (:max, 128), (:decdrop, 0.33), (:upos, 50), (:backupdir, "Backups_2018-06-08T23:46:51.134"), (:beta1, 0.9), (:remb, 350), (:algo, "edmonds"), (:beta2, 0.999), (:lmdrop, 0.33), (:proj, true), (:tokens, 500), (:epochs, 100), (:recover, ""), (:birnnhidden, 200), (:optim, "Adam"), (:lr, 0.001), (:datafiles, Any["/scratch/users/okirnap/ud-treebanks-v2.2/UD_English-EWT/en_ewt-ud-train.conllu", "/scratch/users/okirnap/ud-treebanks-v2.2/UD_English-EWT/en_ewt-ud-dev.conllu"]), (:arcunits, 400)]
INFO: Loading data
23:46:55 /scratch/users/okirnap/ud-treebanks-v2.2/chmodel_converted/english_chmodel.jld 
23:47:00 /scratch/users/okirnap/ud-treebanks-v2.2/UD_English-EWT/en_ewt-ud-train.conllu 
23:47:01 /scratch/users/okirnap/ud-treebanks-v2.2/UD_English-EWT/en_ewt-ud-dev.conllu 
23:48:03 Caching lm vectors... 
INFO: Bucketing
INFO: Initializing Model and Optimizers
INFO: Transfering to gpu.
INFO: Setting momentums
INFO: Computing validation performance
parsers/mst.py:24: RuntimeWarning: divide by zero encountered in divide
  new_root = tokens[np.argmax(root_scores / head_scores)]
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 60.858759 seconds (19.03 M allocations: 1.651 GiB)
Validation loss: 86.067276
Unlabeled attachment score: 6.127723874662001
Labeled attachment score: 0.5924924447272149

Epoch 1
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
121.199128 seconds (94.56 M allocations: 11.141 GiB, 7.44% gc time)
Training loss: 32.114307


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 14.470003 seconds (7.38 M allocations: 1.099 GiB, 11.93% gc time)
Validation loss: 14.347013
Unlabeled attachment score: 70.24813106410052
Labeled attachment score: 66.7846349610307
INFO: Backing up

Epoch 2
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 88.028035 seconds (89.51 M allocations: 10.861 GiB, 9.96% gc time)
Training loss: 21.342594


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
  9.506134 seconds (7.13 M allocations: 1.094 GiB, 4.91% gc time)
Validation loss: 12.129358
Unlabeled attachment score: 75.44138698902498
Labeled attachment score: 72.75727692062988
INFO: Backing up

Epoch 3
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 90.882085 seconds (89.53 M allocations: 10.862 GiB, 9.78% gc time)
Training loss: 18.3621


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
  9.576611 seconds (7.12 M allocations: 1.093 GiB, 4.97% gc time)
Validation loss: 11.026646
Unlabeled attachment score: 79.0122474948306
Labeled attachment score: 76.17703197073325
INFO: Backing up

Epoch 4
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 98.647597 seconds (89.82 M allocations: 10.867 GiB, 12.37% gc time)
Training loss: 16.633842


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
  9.249844 seconds (7.13 M allocations: 1.094 GiB, 4.78% gc time)
Validation loss: 10.354971
Unlabeled attachment score: 80.62668999522825
Labeled attachment score: 77.80738030857324
INFO: Backing up

Epoch 5
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
100.189838 seconds (89.51 M allocations: 10.861 GiB, 20.42% gc time)
Training loss: 15.318124


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 10.047500 seconds (7.13 M allocations: 1.094 GiB, 5.18% gc time)
Validation loss: 9.990946
Unlabeled attachment score: 82.54731986639096
Labeled attachment score: 79.63257515508191
INFO: Backing up

Epoch 6
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 92.308657 seconds (89.53 M allocations: 10.861 GiB, 11.68% gc time)
Training loss: 14.264411


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 10.427880 seconds (7.12 M allocations: 1.093 GiB, 4.87% gc time)
Validation loss: 9.680254
Unlabeled attachment score: 82.5672021631939
Labeled attachment score: 79.8512804199141
INFO: Backing up

Epoch 7
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 92.337291 seconds (89.79 M allocations: 10.867 GiB, 10.62% gc time)
Training loss: 13.717319


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 10.563367 seconds (7.13 M allocations: 1.094 GiB, 4.47% gc time)
Validation loss: 9.4647455
Unlabeled attachment score: 82.53141402894862
Labeled attachment score: 80.05805630666454
INFO: Backing up

Epoch 8
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 89.231697 seconds (89.51 M allocations: 10.861 GiB, 10.77% gc time)
Training loss: 12.997394


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 10.968208 seconds (7.12 M allocations: 1.094 GiB, 4.34% gc time)
Validation loss: 9.516895
Unlabeled attachment score: 84.01860982980754
Labeled attachment score: 81.39017019246063
INFO: Backing up

Epoch 9
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 89.062764 seconds (89.54 M allocations: 10.862 GiB, 9.88% gc time)
Training loss: 12.333636


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 16.019268 seconds (7.37 M allocations: 1.098 GiB, 13.25% gc time)
Validation loss: 9.331334
Unlabeled attachment score: 84.93319548274216
Labeled attachment score: 82.31270876411644
INFO: Backing up

Epoch 10
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 88.948623 seconds (89.54 M allocations: 10.862 GiB, 9.67% gc time)
Training loss: 11.922591


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 11.272139 seconds (7.13 M allocations: 1.094 GiB, 3.96% gc time)
Validation loss: 9.449073
Unlabeled attachment score: 85.1161126133291
Labeled attachment score: 82.54334340703038
INFO: Backing up

Epoch 11
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
101.956527 seconds (89.52 M allocations: 10.861 GiB, 19.87% gc time)
Training loss: 11.602195


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 11.577352 seconds (7.12 M allocations: 1.093 GiB, 4.22% gc time)
Validation loss: 10.052319
Unlabeled attachment score: 84.70256083982822
Labeled attachment score: 82.32463814219818
INFO: Backing up

Epoch 12
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 99.457790 seconds (89.82 M allocations: 10.867 GiB, 12.50% gc time)
Training loss: 11.2856455


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 11.516194 seconds (7.13 M allocations: 1.094 GiB, 4.11% gc time)
Validation loss: 9.397597
Unlabeled attachment score: 85.47797041514235
Labeled attachment score: 83.01256561157945
INFO: Backing up

Epoch 13
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 89.522318 seconds (89.50 M allocations: 10.861 GiB, 10.28% gc time)
Training loss: 10.887655


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 12.345311 seconds (7.13 M allocations: 1.094 GiB, 3.80% gc time)
Validation loss: 9.309904
Unlabeled attachment score: 85.89152218864324
Labeled attachment score: 83.39032925083505
INFO: Backing up

Epoch 14
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 92.975353 seconds (89.51 M allocations: 10.861 GiB, 13.33% gc time)
Training loss: 10.783696


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 12.378204 seconds (7.12 M allocations: 1.093 GiB, 4.08% gc time)
Validation loss: 9.479384
Unlabeled attachment score: 85.74836965166216
Labeled attachment score: 83.12788293303642
INFO: Backing up

Epoch 15
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 94.591692 seconds (89.79 M allocations: 10.867 GiB, 10.88% gc time)
Training loss: 10.376858


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 12.230481 seconds (7.13 M allocations: 1.094 GiB, 3.63% gc time)
Validation loss: 9.552137
Unlabeled attachment score: 86.3090504215047
Labeled attachment score: 83.77604580881183
INFO: Backing up

Epoch 16
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 91.944317 seconds (89.52 M allocations: 10.861 GiB, 10.37% gc time)
Training loss: 10.466291


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 12.653240 seconds (7.12 M allocations: 1.094 GiB, 3.92% gc time)
Validation loss: 9.531883
Unlabeled attachment score: 85.99888659137903
Labeled attachment score: 83.47383489740734
INFO: Backing up

Epoch 17
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 86.699089 seconds (89.55 M allocations: 10.862 GiB, 10.17% gc time)
Training loss: 10.012232


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 17.711337 seconds (7.37 M allocations: 1.098 GiB, 12.59% gc time)
Validation loss: 9.527255
Unlabeled attachment score: 86.07443931923015
Labeled attachment score: 83.63686973119135
INFO: Backing up

Epoch 18
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 86.970954 seconds (89.54 M allocations: 10.862 GiB, 9.77% gc time)
Training loss: 10.165081


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 12.805194 seconds (7.13 M allocations: 1.094 GiB, 3.71% gc time)
Validation loss: 9.758236
Unlabeled attachment score: 86.34086209638937
Labeled attachment score: 83.95101002067759
INFO: Backing up

Epoch 19
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 95.642911 seconds (89.52 M allocations: 10.861 GiB, 16.40% gc time)
Training loss: 9.855978


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 13.596485 seconds (7.12 M allocations: 1.093 GiB, 3.47% gc time)
Validation loss: 10.254618
Unlabeled attachment score: 86.42039128360108
Labeled attachment score: 84.04246858597105
INFO: Backing up

Epoch 20
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 95.787414 seconds (89.83 M allocations: 10.867 GiB, 15.01% gc time)
Training loss: 9.613146


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 13.046603 seconds (7.13 M allocations: 1.094 GiB, 3.33% gc time)
Validation loss: 10.484807
Unlabeled attachment score: 86.34086209638937
Labeled attachment score: 83.93112772387467
INFO: Backing up

Epoch 21
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 89.838307 seconds (89.52 M allocations: 10.861 GiB, 10.32% gc time)
Training loss: 9.79251


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 14.446072 seconds (7.13 M allocations: 1.094 GiB, 3.41% gc time)
Validation loss: 10.042789
Unlabeled attachment score: 86.99697789088596
Labeled attachment score: 84.49578495307777
INFO: Backing up

Epoch 22
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 89.496260 seconds (89.52 M allocations: 10.861 GiB, 10.84% gc time)
Training loss: 9.47396


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 20.446093 seconds (7.37 M allocations: 1.098 GiB, 10.82% gc time)
Validation loss: 10.059598
Unlabeled attachment score: 86.74646095116908
Labeled attachment score: 84.29298552568793
INFO: Backing up

Epoch 23
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 92.416123 seconds (89.55 M allocations: 10.862 GiB, 10.07% gc time)
Training loss: 9.346756


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 14.357525 seconds (7.13 M allocations: 1.094 GiB, 3.42% gc time)
Validation loss: 10.287868
Unlabeled attachment score: 86.47208525528869
Labeled attachment score: 83.95896293939876
INFO: Backing up

Epoch 24
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 94.456851 seconds (89.50 M allocations: 10.861 GiB, 13.94% gc time)
Training loss: 9.065291


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 14.691944 seconds (7.12 M allocations: 1.094 GiB, 3.33% gc time)
Validation loss: 9.880903
Unlabeled attachment score: 86.75441386989024
Labeled attachment score: 84.40034992842374
INFO: Backing up

Epoch 25
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 94.459157 seconds (89.56 M allocations: 10.862 GiB, 10.37% gc time)
Training loss: 9.056832


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 22.090801 seconds (7.38 M allocations: 1.099 GiB, 11.63% gc time)
Validation loss: 10.424629
Unlabeled attachment score: 86.85382535390488
Labeled attachment score: 84.42420868458724
INFO: Backing up

Epoch 26
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 95.540675 seconds (89.55 M allocations: 10.862 GiB, 10.61% gc time)
Training loss: 9.12079


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 15.326975 seconds (7.13 M allocations: 1.094 GiB, 3.27% gc time)
Validation loss: 10.2975445
Unlabeled attachment score: 87.01686018768888
Labeled attachment score: 84.64291394941944
INFO: Backing up

Epoch 27
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 95.300550 seconds (89.53 M allocations: 10.862 GiB, 9.68% gc time)
Training loss: 9.0092535


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 16.343529 seconds (7.12 M allocations: 1.093 GiB, 3.07% gc time)
Validation loss: 10.421378
Unlabeled attachment score: 86.59137903610625
Labeled attachment score: 84.20152696039446
INFO: Backing up

Epoch 28
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380


200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
102.554879 seconds (89.83 M allocations: 10.867 GiB, 12.43% gc time)
Training loss: 8.958241


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 15.788458 seconds (7.13 M allocations: 1.094 GiB, 2.91% gc time)
Validation loss: 10.426478
Unlabeled attachment score: 87.21170669635757
Labeled attachment score: 84.73039605535232
INFO: Backing up

Epoch 29
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
105.667055 seconds (89.51 M allocations: 10.861 GiB, 19.92% gc time)
Training loss: 8.793116


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 16.306860 seconds (7.13 M allocations: 1.094 GiB, 3.23% gc time)
Validation loss: 10.740185
Unlabeled attachment score: 86.89756640687132
Labeled attachment score: 84.50771433115953
INFO: Backing up

Epoch 30
10 / 380
20 / 380
30 / 380
40 / 380
50 / 380
60 / 380
70 / 380
80 / 380
90 / 380
100 / 380
110 / 380
120 / 380
130 / 380
140 / 380
150 / 380
160 / 380
170 / 380
180 / 380
190 / 380
200 / 380
210 / 380
220 / 380
230 / 380
240 / 380
250 / 380
260 / 380
270 / 380
280 / 380
290 / 380
300 / 380
310 / 380
320 / 380
330 / 380
340 / 380
350 / 380
360 / 380
370 / 380
380 / 380
 99.179844 seconds (89.53 M allocations: 10.861 GiB, 11.35% gc time)
Training loss: 8.870018


INFO: Computing validation performance
10 / 83
20 / 83
30 / 83
40 / 83
50 / 83
60 / 83
70 / 83
80 / 83
 16.869854 seconds (7.12 M allocations: 1.093 GiB, 2.91% gc time)
Validation loss: 11.038578
Unlabeled attachment score: 87.07253061873708
Labeled attachment score: 84.56736122156832
INFO: Backing up

Epoch 31
  C-c C-c^CERROR: InterruptException:
Stacktrace:
 [1] knetgc(::Int64) at /scratch/users/cgumeli/.julia/v0.6/Knet/src/kptr.jl:125
 [2] Knet.KnetPtr(::Int64) at /scratch/users/cgumeli/.julia/v0.6/Knet/src/kptr.jl:88
 [3] Type at /scratch/users/cgumeli/.julia/v0.6/Knet/src/karray.jl:104 [inlined]
 [4] Type at /scratch/users/cgumeli/.julia/v0.6/Knet/src/karray.jl:116 [inlined]
 [5] similar at /scratch/users/cgumeli/.julia/v0.6/Knet/src/karray.jl:172 [inlined]
 [6] similar at /scratch/users/cgumeli/.julia/v0.6/Knet/src/karray.jl:176 [inlined]
 [7] At_mul_B(::Knet.KnetArray{Float32,2}, ::Knet.KnetArray{Float32,2}) at /scratch/users/cgumeli/.julia/v0.6/Knet/src/linalg.jl:18
 [8] *(::Type{AutoGrad.Grad{2}}, ::Knet.KnetArray{Float32,2}, ::Knet.KnetArray{Float32,2}, ::Knet.KnetArray{Float32,2}, ::AutoGrad.Rec{Knet.KnetArray{Float32,2}}) at ./<missing>:0
 [9] backward_pass(::AutoGrad.Rec{Array{Any,1}}, ::AutoGrad.Rec{Float32}, ::Array{AutoGrad.Node,1}) at /scratch/users/cgumeli/.julia/v0.6/AutoGrad/src/core.jl:256
 [10] (::AutoGrad.##gradfun#4#6{#loss,Int64})(::Array{Any,1}, ::Function, ::Array{Any,1}, ::Vararg{Any,N} where N) at /scratch/users/cgumeli/.julia/v0.6/AutoGrad/src/core.jl:62
 [11] (::AutoGrad.#gradfun#5)(::Array{Any,1}, ::Vararg{Any,N} where N) at /scratch/users/cgumeli/.julia/v0.6/AutoGrad/src/core.jl:57
 [12] macro expansion at /scratch/users/cgumeli/Nubik/main2.jl:194 [inlined]
 [13] macro expansion at ./util.jl:237 [inlined]
 [14] train() at /scratch/users/cgumeli/Nubik/main2.jl:192
 [15] main(::String) at /scratch/users/cgumeli/Nubik/main2.jl:83

julia> exit()
exit()
bash-4.2$ julia
julia
               _
   _       _ _(_)_     |  A fresh approach to technical computing
  (_)     | (_) (_)    |  Documentation: https://docs.julialang.org
   _ _   _| |_  __ _   |  Type "?help" for help.
  | | | | | | |/ _` |  |
  | | |_| | | | (_| |  |  Version 0.6.2 (2017-12-13 18:08 UTC)
 _/ |\__'_|_|_|\__'_|  |  Official http://julialang.org/ release
|__/                   |  x86_64-pc-linux-gnu

WARNING: Terminal not fully functional
julia> include("main2.jl")
include("main2.jl")
INFO: Checking dependencies
INFO: All dependencies are installed
accuracy_bt

julia> include("main2.jl")
include("main2.jl")
INFO: Checking dependencies
INFO: All dependencies are installed
WARNING: redefining constant UPOSTAG
WARNING: redefining constant UDEPREL
WARNING: redefining constant lmdir
WARNING: redefining constant datadir
WARNING: redefining constant engtrn
WARNING: redefining constant engdev
WARNING: redefining constant englm
accuracy_bt

julia> 


julia> 


julia> engtrn
engtrn
"/scratch/users/okirnap/ud-treebanks-v2.2/UD_English-EWT/en_ewt-ud-train.conllu"

julia> readdir("/scratch/users/okirnap/ud-treebanks-v2.2/")
readdir("/scratch/users/okirnap/ud-treebanks-v2.2/")
85-element Array{String,1}:
 "chmodel_converted"       
 "conll18_ud_eval.py"      
 "UD_Afrikaans-AfriBooms"  
 "UD_Ancient_Greek-Perseus"
 "UD_Ancient_Greek-PROIEL" 
 "UD_Arabic-PADT"          
 "UD_Armenian-ArmTDP"      
 "UD_Basque-BDT"           
 "UD_Breton-KEB"           
 "UD_Bulgarian-BTB"        
                          
 "UD_Swedish-PUD"          
 "UD_Swedish-Talbanken"    
 "UD_Thai-PUD"             
 "UD_Turkish-IMST"         
 "UD_Ukrainian-IU"         
 "UD_Upper_Sorbian-UFAL"   
 "UD_Urdu-UDTB"            
 "UD_Uyghur-UDT"           
 "UD_Vietnamese-VTB"       

julia> find("Tur", readdir("/scratch/users/okirnap/ud-treebanks-v2.2/"))
find("Tur", readdir("/scratch/users/okirnap/ud-treebanks-v2.2/"))
ERROR: MethodError: no method matching find(::String, ::Array{String,1})
Closest candidates are:
  find(::String, !Matched::Base.LibGit2.GitIndex) at libgit2/index.jl:113
  find(!Matched::Compat.Fix2{Base.#in,T} where T, ::Any) at /scratch/users/cgumeli/.julia/v0.6/Compat/src/Compat.jl:1565
  find(!Matched::Function, ::Any) at array.jl:1518
  ...

julia> cd("UD_Turkish-IMST")
cd("UD_Turkish-IMST")
ERROR: chdir UD_Turkish-IMST: no such file or directory (ENOENT)
Stacktrace:
 [1] uv_error at ./libuv.jl:68 [inlined]
 [2] cd(::String) at ./file.jl:50

julia> readdir("/scratch/users/okirnap/ud-treebanks-v2.2/UD_Turkish-IMST")
readdir("/scratch/users/okirnap/ud-treebanks-v2.2/UD_Turkish-IMST")
6-element Array{String,1}:
 "LICENSE.txt"            
 "README.txt"             
 "tr_imst-ud-dev.conllu"  
 "tr_imst-ud-dev.txt"     
 "tr_imst-ud-train.conllu"
 "tr_imst-ud-train.txt"   

julia> lmdir
lmdir
"/scratch/users/okirnap/ud-treebanks-v2.2/chmodel_converted"

julia> readdir(lmdir)
readdir(lmdir)
47-element Array{String,1}:
 "ancient_greek_chmodel.jld"
 "arabic_chmodel.jld"       
 "basque_chmodel.jld"       
 "bulgarian_chmodel.jld"    
 "catalan_chmodel.jld"      
 "chinese_chmodel.jld"      
 "croatian_chmodel.jld"     
 "czech_chmodel.jld"        
 "danish_chmodel.jld"       
 "dutch_chmodel.jld"        
                           
 "slovak_chmodel.jld"       
 "slovenian_chmodel.jld"    
 "spanish_chmodel.jld"      
 "swedish_chmodel.jld"      
 "turkish_chmodel.jld"      
 "ukranian_chmodel.jld"     
 "urdu_chmodel.jld"         
 "uyghur_chmodel.jld"       
 "vietnamese_chmodel.jld"   

julia> 


julia> include("main2.jl")
include("main2.jl")
INFO: Checking dependencies
INFO: All dependencies are installed
WARNING: redefining constant UPOSTAG
WARNING: redefining constant UDEPREL
WARNING: redefining constant lmdir
WARNING: redefining constant datadir
WARNING: redefining constant engtrn
WARNING: redefining constant engdev
WARNING: redefining constant englm
accuracy_bt

julia> main("--datafiles $trtrn $trdev --lmfile $trlm --upos 50 --birnnhidden 200 --lmdrop 0.33 --birnndrop 0.33 --decdrop 0.33 --tokens 500 --birnn 3")
main("--datafiles $trtrn $trdev --lmfile $trlm --upos 50 --birnnhidden 200 --lmdrop 0.33 --birnndrop 0.33 --decdrop 0.33 --tokens 500 --birnn 3")
Tuple{Symbol,Any}[(:feat, 0), (:birnn, 3), (:birnndrop, 0.33), (:lmfile, "/scratch/users/okirnap/ud-treebanks-v2.2/chmodel_converted/turkish_chmodel.jld"), (:min, 2), (:birnncell, "LSTM"), (:labelunits, 100), (:xpos, 0), (:batchsize, 8), (:max, 128), (:decdrop, 0.33), (:upos, 50), (:backupdir, "Backups_2018-06-09T01:02:50.23"), (:beta1, 0.9), (:remb, 350), (:algo, "edmonds"), (:beta2, 0.999), (:lmdrop, 0.33), (:proj, true), (:tokens, 500), (:epochs, 100), (:recover, ""), (:birnnhidden, 200), (:optim, "Adam"), (:lr, 0.001), (:datafiles, Any["/scratch/users/okirnap/ud-treebanks-v2.2/UD_Turkish-IMST/tr_imst-ud-train.conllu", "/scratch/users/okirnap/ud-treebanks-v2.2/UD_Turkish-IMST/tr_imst-ud-dev.conllu"]), (:arcunits, 400)]
INFO: Loading data
01:02:53 /scratch/users/okirnap/ud-treebanks-v2.2/chmodel_converted/turkish_chmodel.jld 
01:02:59 /scratch/users/okirnap/ud-treebanks-v2.2/UD_Turkish-IMST/tr_imst-ud-train.conllu 
01:03:00 /scratch/users/okirnap/ud-treebanks-v2.2/UD_Turkish-IMST/tr_imst-ud-dev.conllu 
01:03:35 Caching lm vectors... 
ERROR: KeyError: key "Aspect=DurPerf" not found
Stacktrace:
 [1] getindex at ./dict.jl:474 [inlined]
 [2] proc(::String, ::Dict{String,Int64}, ::Bool) at /scratch/users/cgumeli/Nubik/./ku-dependency-parser2/src/preprocess.jl:90
 [3] collect_to!(::Array{Array{Any,1},1}, ::Base.Generator{Array{Any,1},##272#274{ExtendedVocab}}, ::Int64, ::Int64) at ./array.jl:508
 [4] collect(::Base.Generator{Array{Any,1},##272#274{ExtendedVocab}}) at ./array.jl:476
 [5] extend_corpus!(::ExtendedVocab, ::Array{Any,1}) at /scratch/users/cgumeli/Nubik/./ku-dependency-parser2/src/preprocess.jl:78
 [6] main(::String) at /scratch/users/cgumeli/Nubik/main2.jl:76

julia> include("main2.jl")
include("main2.jl")
INFO: Checking dependencies
INFO: All dependencies are installed
WARNING: redefining constant UPOSTAG
WARNING: redefining constant UDEPREL
WARNING: redefining constant lmdir
WARNING: redefining constant datadir
WARNING: redefining constant engtrn
WARNING: redefining constant engdev
WARNING: redefining constant englm
WARNING: redefining constant trtrn
WARNING: redefining constant trdev
WARNING: redefining constant trlm
accuracy_bt

julia> main("--datafiles $trtrn $trdev --lmfile $trlm --upos 50 --birnnhidden 200 --lmdrop 0.33 --birnndrop 0.33 --decdrop 0.33 --tokens 500 --birnn 3")  C-c C-c^C
julia> 


julia> include("main2.jl")
include("main2.jl")
INFO: Checking dependencies
INFO: All dependencies are installed
WARNING: redefining constant UPOSTAG
WARNING: redefining constant UDEPREL
WARNING: redefining constant lmdir
WARNING: redefining constant datadir
WARNING: redefining constant engtrn
WARNING: redefining constant engdev
WARNING: redefining constant englm
WARNING: redefining constant trtrn
WARNING: redefining constant trdev
WARNING: redefining constant trlm
accuracy_bt

julia> main("--datafiles $trtrn $trdev --lmfile $trlm --upos 50 --birnnhidden 200 --lmdrop 0.33 --birnndrop 0.33 --decdrop 0.33 --tokens 500 --birnn 3")
main("--datafiles $trtrn $trdev --lmfile $trlm --upos 50 --birnnhidden 200 --lmdrop 0.33 --birnndrop 0.33 --decdrop 0.33 --tokens 500 --birnn 3")
Tuple{Symbol,Any}[(:feat, 0), (:birnn, 3), (:birnndrop, 0.33), (:lmfile, "/scratch/users/okirnap/ud-treebanks-v2.2/chmodel_converted/turkish_chmodel.jld"), (:min, 2), (:birnncell, "LSTM"), (:labelunits, 100), (:xpos, 0), (:batchsize, 8), (:max, 128), (:decdrop, 0.33), (:upos, 50), (:backupdir, "Backups_2018-06-09T01:13:37.77"), (:beta1, 0.9), (:remb, 350), (:algo, "edmonds"), (:beta2, 0.999), (:lmdrop, 0.33), (:proj, true), (:tokens, 500), (:epochs, 100), (:recover, ""), (:birnnhidden, 200), (:optim, "Adam"), (:lr, 0.001), (:datafiles, Any["/scratch/users/okirnap/ud-treebanks-v2.2/UD_Turkish-IMST/tr_imst-ud-train.conllu", "/scratch/users/okirnap/ud-treebanks-v2.2/UD_Turkish-IMST/tr_imst-ud-dev.conllu"]), (:arcunits, 400)]
INFO: Loading data
01:13:38 /scratch/users/okirnap/ud-treebanks-v2.2/chmodel_converted/turkish_chmodel.jld 
01:13:40 /scratch/users/okirnap/ud-treebanks-v2.2/UD_Turkish-IMST/tr_imst-ud-train.conllu 
01:13:41 /scratch/users/okirnap/ud-treebanks-v2.2/UD_Turkish-IMST/tr_imst-ud-dev.conllu 
01:13:59 Caching lm vectors... 
INFO: Bucketing
INFO: Initializing Model and Optimizers
INFO: Transfering to gpu.
INFO: Setting momentums
INFO: Computing validation performance
parsers/mst.py:24: RuntimeWarning: divide by zero encountered in divide
  new_root = tokens[np.argmax(root_scores / head_scores)]
10 / 49
20 / 49
30 / 49
40 / 49
 38.443992 seconds (11.79 M allocations: 931.351 MiB, 1.02% gc time)
Validation loss: 64.34174
Unlabeled attachment score: 7.971231645190291
Labeled attachment score: 0.12985715712716012

Epoch 1
10 / 55
20 / 55
30 / 55
40 / 55
50 / 55
 43.610734 seconds (20.63 M allocations: 2.684 GiB, 2.90% gc time)
Training loss: 33.502853


INFO: Computing validation performance
10 / 49
20 / 49
30 / 49
40 / 49
  2.947971 seconds (3.17 M allocations: 508.728 MiB, 5.10% gc time)
Validation loss: 28.220655
Unlabeled attachment score: 45.430026970332634
Labeled attachment score: 36.439916092298475
INFO: Backing up

Epoch 2
10 / 55
20 / 55
30 / 55
40 / 55
50 / 55
  9.156605 seconds (15.06 M allocations: 2.393 GiB, 12.53% gc time)
Training loss: 24.774616


INFO: Computing validation performance
10 / 49
20 / 49
30 / 49
40 / 49
  2.969319 seconds (3.17 M allocations: 508.731 MiB, 5.97% gc time)
Validation loss: 24.760027
Unlabeled attachment score: 48.306862451303566
Labeled attachment score: 40.53541104784737
INFO: Backing up

Epoch 3
10 / 55
20 / 55
30 / 55
40 / 55
50 / 55
  9.054176 seconds (15.09 M allocations: 2.394 GiB, 12.78% gc time)
Training loss: 22.231295


INFO: Computing validation performance
10 / 49
20 / 49
30 / 49
40 / 49
  2.950496 seconds (3.17 M allocations: 508.702 MiB, 5.90% gc time)
Validation loss: 22.81728
Unlabeled attachment score: 53.31135750674258
Labeled attachment score: 45.68974128458695
INFO: Backing up

Epoch 4
10 / 55
20 / 55
30 / 55
40 / 55
50 / 55
 12.289558 seconds (15.15 M allocations: 2.395 GiB, 12.62% gc time)
Training loss: 20.81374


INFO: Computing validation performance
10 / 49
20 / 49
30 / 49
40 / 49
  2.990971 seconds (3.17 M allocations: 508.715 MiB, 4.92% gc time)
Validation loss: 21.600325
Unlabeled attachment score: 54.97952252522226
Labeled attachment score: 48.09709319748277
INFO: Backing up

Epoch 5
10 / 55
20 / 55
30 / 55
40 / 55
50 / 55
  8.950542 seconds (15.09 M allocations: 2.393 GiB, 13.52% gc time)
Training loss: 19.782234


INFO: Computing validation performance
10 / 49
20 / 49
30 / 49
40 / 49
  2.973320 seconds (3.17 M allocations: 508.687 MiB, 5.06% gc time)
Validation loss: 21.451689
Unlabeled attachment score: 56.737588652482266
Labeled attachment score: 48.806313055638796
INFO: Backing up

Epoch 6
10 / 55
20 / 55
30 / 55
40 / 55
50 / 55
  9.140301 seconds (15.09 M allocations: 2.393 GiB, 15.78% gc time)
Training loss: 18.734549


INFO: Computing validation performance
10 / 49
20 / 49
30 / 49
40 / 49
  3.013583 seconds (3.17 M allocations: 508.684 MiB, 4.70% gc time)
Validation loss: 21.315697
Unlabeled attachment score: 52.082709020077914
Labeled attachment score: 45.85955449006093
INFO: Backing up

Epoch 7
10 / 55
20 / 55
30 / 55
40 / 55
50 / 55
  9.021595 seconds (15.09 M allocations: 2.393 GiB, 15.46% gc time)
Training loss: 17.958036


INFO: Computing validation performance
10 / 49
20 / 49
30 / 49
40 / 49
  3.035406 seconds (3.17 M allocations: 508.699 MiB, 5.18% gc time)
Validation loss: 21.041971
Unlabeled attachment score: 54.03056637698531
Labeled attachment score: 47.697532714014585
INFO: Backing up

Epoch 8
10 / 55
20 / 55
30 / 55
40 / 55
50 / 55
  9.097351 seconds (15.09 M allocations: 2.393 GiB, 15.23% gc time)
Training loss: 17.229614


INFO: Computing validation performance
10 / 49
20 / 49
30 / 49
40 / 49
  3.001604 seconds (3.17 M allocations: 508.690 MiB, 4.75% gc time)
Validation loss: 20.969255
Unlabeled attachment score: 50.744181400459496
Labeled attachment score: 44.7108181000899
INFO: Backing up

Epoch 9
10 / 55
20 / 55
30 / 55
40 / 55
50 / 55
  9.101399 seconds (15.09 M allocations: 2.393 GiB, 15.64% gc time)
Training loss: 16.74608


INFO: Computing validation performance
10 / 49
20 / 49
30 / 49
40 / 49
  3.071911 seconds (3.17 M allocations: 508.699 MiB, 5.28% gc time)
Validation loss: 20.764248
Unlabeled attachment score: 54.04055538907202
Labeled attachment score: 47.477774448107084
INFO: Backing up

Epoch 10
10 / 55
20 / 55
30 / 55
40 / 55
50 / 55
  9.208629 seconds (15.09 M allocations: 2.393 GiB, 16.17% gc time)
Training loss: 16.464624


INFO: Computing validation performance
10 / 49
20 / 49
30 / 49
40 / 49
  3.109197 seconds (3.17 M allocations: 508.690 MiB, 4.96% gc time)
Validation loss: 20.885883
Unlabeled attachment score: 55.97842373389272
Labeled attachment score: 49.255818599540504
INFO: Backing up

Epoch 11
10 / 55
20 / 55
30 / 55
40 / 55
50 / 55
  9.160947 seconds (15.09 M allocations: 2.393 GiB, 15.61% gc time)
Training loss: 16.098707


INFO: Computing validation performance
10 / 49
20 / 49
30 / 49
40 / 49
  3.056305 seconds (3.17 M allocations: 508.674 MiB, 4.78% gc time)
Validation loss: 21.59319
Unlabeled attachment score: 53.521126760563384
Labeled attachment score: 46.71860952951753
INFO: Backing up

Epoch 12
10 / 55
20 / 55
30 / 55
40 / 55
50 / 55
  8.907714 seconds (15.09 M allocations: 2.393 GiB, 15.53% gc time)
Training loss: 15.573423


INFO: Computing validation performance
10 / 49
20 / 49
30 / 49
40 / 49
  3.059718 seconds (3.17 M allocations: 508.687 MiB, 4.72% gc time)
Validation loss: 20.725658
Unlabeled attachment score: 56.27809409649386
Labeled attachment score: 49.96503845769654
INFO: Backing up

Epoch 13
10 / 55
20 / 55
30 / 55
40 / 55
50 / 55
  9.069308 seconds (15.09 M allocations: 2.393 GiB, 15.32% gc time)
Training loss: 14.922492


INFO: Computing validation performance
10 / 49
20 / 49
30 / 49
40 / 49
  3.077281 seconds (3.17 M allocations: 508.683 MiB, 4.70% gc time)
Validation loss: 21.172998
Unlabeled attachment score: 51.28358805314154
Labeled attachment score: 45.68974128458695
INFO: Backing up

Epoch 14
10 / 55
20 / 55
30 / 55
40 / 55
50 / 55
  9.014074 seconds (15.09 M allocations: 2.393 GiB, 15.64% gc time)
Training loss: 14.626795


INFO: Computing validation performance
10 / 49
20 / 49
30 / 49
40 / 49
  3.164644 seconds (3.17 M allocations: 508.684 MiB, 4.99% gc time)
Validation loss: 21.575579
Unlabeled attachment score: 54.08051143741884
Labeled attachment score: 48.007192088702425
INFO: Backing up

Epoch 15
10 / 55
20 / 55
30 / 55
40 / 55
50 / 55
  9.251183 seconds (15.09 M allocations: 2.393 GiB, 16.35% gc time)
Training loss: 14.47647


INFO: Computing validation performance
10 / 49
20 / 49
30 / 49
40 / 49
  3.145420 seconds (3.17 M allocations: 508.691 MiB, 4.79% gc time)
Validation loss: 21.695387
Unlabeled attachment score: 53.75087403855758
Labeled attachment score: 47.977225052442314
INFO: Backing up

Epoch 16
10 / 55
20 / 55
30 / 55
40 / 55
50 / 55
  9.191482 seconds (15.09 M allocations: 2.393 GiB, 15.50% gc time)
Training loss: 13.722428


INFO: Computing validation performance
10 / 49
20 / 49
30 / 49
40 / 49
  3.215236 seconds (3.17 M allocations: 508.692 MiB, 4.72% gc time)
Validation loss: 21.45818
Unlabeled attachment score: 55.66876435920487
Labeled attachment score: 49.60543402257517
INFO: Backing up

Epoch 17
10 / 55
20 / 55
30 / 55
40 / 55
50 / 55
  9.254900 seconds (15.09 M allocations: 2.393 GiB, 15.69% gc time)
Training loss: 13.363249


INFO: Computing validation performance
10 / 49
20 / 49
30 / 49
40 / 49
  3.245947 seconds (3.17 M allocations: 508.685 MiB, 4.80% gc time)
Validation loss: 21.69328
Unlabeled attachment score: 56.238138048147036
Labeled attachment score: 49.915093397263014
INFO: Backing up

Epoch 18
10 / 55
20 / 55
30 / 55
40 / 55
50 / 55
  9.099128 seconds (15.09 M allocations: 2.393 GiB, 15.43% gc time)
Training loss: 12.983008


INFO: Computing validation performance
10 / 49
20 / 49
30 / 49
40 / 49
  3.152541 seconds (3.17 M allocations: 508.686 MiB, 4.58% gc time)
Validation loss: 22.050503
Unlabeled attachment score: 57.27699530516432
Labeled attachment score: 50.60433523124563
INFO: Backing up

Epoch 19
10 / 55
20 / 55
30 / 55
40 / 55
50 / 55
  9.161704 seconds (15.09 M allocations: 2.393 GiB, 15.59% gc time)
Training loss: 12.763187


INFO: Computing validation performance
10 / 49
20 / 49
30 / 49
40 / 49
  3.243968 seconds (3.17 M allocations: 508.687 MiB, 4.62% gc time)
Validation loss: 22.840025
Unlabeled attachment score: 56.877434821696134
Labeled attachment score: 51.03386275097393
INFO: Backing up

Epoch 20
10 / 55
20 / 55
30 / 55
40 / 55
50 / 55
  9.221772 seconds (15.09 M allocations: 2.393 GiB, 16.00% gc time)
Training loss: 12.844133


INFO: Computing validation performance
10 / 49
20 / 49
30 / 49
40 / 49
  3.311176 seconds (3.17 M allocations: 508.692 MiB, 4.66% gc time)
Validation loss: 23.657915
Unlabeled attachment score: 56.32803915692738
Labeled attachment score: 50.524423134551995
INFO: Backing up

Epoch 21
10 / 55
20 / 55
30 / 55
40 / 55
50 / 55
  9.243705 seconds (15.10 M allocations: 2.394 GiB, 15.94% gc time)
Training loss: 12.124196


INFO: Computing validation performance
10 / 49
20 / 49
30 / 49
40 / 49
  3.267132 seconds (3.17 M allocations: 508.693 MiB, 4.42% gc time)
Validation loss: 22.448982
Unlabeled attachment score: 57.13714913595045
Labeled attachment score: 51.47337928278893
INFO: Backing up

Epoch 22
10 / 55
20 / 55
30 / 55
40 / 55
50 / 55
  9.363974 seconds (15.09 M allocations: 2.393 GiB, 15.82% gc time)
Training loss: 11.829282


INFO: Computing validation performance
10 / 49
20 / 49
30 / 49
40 / 49
  3.313773 seconds (3.17 M allocations: 508.690 MiB, 4.61% gc time)
Validation loss: 22.209352
Unlabeled attachment score: 58.82529217860353
Labeled attachment score: 52.42233543102587
INFO: Backing up

Epoch 23
10 / 55
20 / 55
30 / 55
40 / 55
50 / 55
  9.206678 seconds (15.09 M allocations: 2.393 GiB, 15.28% gc time)
Training loss: 11.5029545


INFO: Computing validation performance
10 / 49
20 / 49
30 / 49
40 / 49
  3.363667 seconds (3.17 M allocations: 508.682 MiB, 4.40% gc time)
Validation loss: 23.369204
Unlabeled attachment score: 58.02617121166717
Labeled attachment score: 52.0427529717311
INFO: Backing up

Epoch 24
10 / 55
20 / 55
30 / 55
40 / 55
50 / 55
  9.290895 seconds (15.09 M allocations: 2.393 GiB, 15.31% gc time)
Training loss: 11.121529


INFO: Computing validation performance
10 / 49
20 / 49
30 / 49
40 / 49
  3.384482 seconds (3.17 M allocations: 508.691 MiB, 4.42% gc time)
Validation loss: 23.71975
Unlabeled attachment score: 58.21596244131455
Labeled attachment score: 52.18259914094496
INFO: Backing up

Epoch 25
10 / 55
20 / 55
30 / 55
40 / 55
50 / 55
  9.326538 seconds (15.09 M allocations: 2.393 GiB, 16.05% gc time)
Training loss: 10.770585


INFO: Computing validation performance
10 / 49
20 / 49
30 / 49
40 / 49
  3.357716 seconds (3.17 M allocations: 508.707 MiB, 4.79% gc time)
Validation loss: 24.517569
Unlabeled attachment score: 55.77864349215862
Labeled attachment score: 50.134851663170515
INFO: Backing up

Epoch 26
10 / 55
20 / 55
30 / 55
40 / 55
50 / 55
  9.278202 seconds (15.09 M allocations: 2.394 GiB, 15.72% gc time)
Training loss: 10.361059


INFO: Computing validation performance
10 / 49
20 / 49
30 / 49
40 / 49
  3.361368 seconds (3.17 M allocations: 508.673 MiB, 4.59% gc time)
Validation loss: 24.894287
Unlabeled attachment score: 58.29587453800819
Labeled attachment score: 52.40235740685246
INFO: Backing up

Epoch 27
10 / 55
20 / 55
30 / 55
40 / 55
50 / 55
  9.154875 seconds (15.10 M allocations: 2.394 GiB, 15.81% gc time)
Training loss: 10.173552


INFO: Computing validation performance
10 / 49
20 / 49
30 / 49
40 / 49
  3.407125 seconds (3.17 M allocations: 508.695 MiB, 4.42% gc time)
Validation loss: 24.52131
Unlabeled attachment score: 59.13495155329138
Labeled attachment score: 53.00169813205474
INFO: Backing up

Epoch 28
10 / 55
20 / 55
30 / 55
40 / 55
50 / 55
  9.117187 seconds (15.10 M allocations: 2.394 GiB, 15.20% gc time)
Training loss: 9.921535


INFO: Computing validation performance
10 / 49
20 / 49
30 / 49
40 / 49
  3.396585 seconds (3.17 M allocations: 508.692 MiB, 4.39% gc time)
Validation loss: 25.73212
Unlabeled attachment score: 58.07611627210069
Labeled attachment score: 52.022774947557686
INFO: Backing up

Epoch 29
10 / 55
20 / 55
30 / 55
40 / 55
50 / 55
  9.244514 seconds (15.10 M allocations: 2.394 GiB, 15.37% gc time)
Training loss: 9.741039


INFO: Computing validation performance
10 / 49
20 / 49
30 / 49
40 / 49
  3.350272 seconds (3.17 M allocations: 508.694 MiB, 4.42% gc time)
Validation loss: 26.095537
Unlabeled attachment score: 59.47457796423934
Labeled attachment score: 53.680950953950656
INFO: Backing up

Epoch 30
10 / 55
20 / 55
30 / 55
40 / 55
50 / 55
  9.210234 seconds (15.10 M allocations: 2.394 GiB, 15.97% gc time)
Training loss: 9.530306


INFO: Computing validation performance
10 / 49
20 / 49
30 / 49
40 / 49
  3.446239 seconds (3.17 M allocations: 508.694 MiB, 4.45% gc time)
Validation loss: 25.956537
Unlabeled attachment score: 59.41464389171911
Labeled attachment score: 53.72090700229747
INFO: Backing up

Epoch 31
10 / 55
20 / 55
30 / 55
40 / 55
50 / 55
  9.250966 seconds (15.10 M allocations: 2.394 GiB, 15.72% gc time)
Training loss: 9.425995


INFO: Computing validation performance
10 / 49
20 / 49
30 / 49
40 / 49
  3.468094 seconds (3.17 M allocations: 508.696 MiB, 4.26% gc time)
Validation loss: 26.68343
Unlabeled attachment score: 59.02507242033763
Labeled attachment score: 53.101588252921786
INFO: Backing up

Epoch 32
10 / 55
20 / 55
30 / 55
40 / 55
50 / 55
  9.096256 seconds (15.10 M allocations: 2.394 GiB, 15.81% gc time)
Training loss: 8.980378


INFO: Computing validation performance
10 / 49
20 / 49
30 / 49
40 / 49
  3.485043 seconds (3.17 M allocations: 508.697 MiB, 4.61% gc time)
Validation loss: 27.664436
Unlabeled attachment score: 57.62661072819898
Labeled attachment score: 52.18259914094496
INFO: Backing up

Epoch 33
10 / 55
20 / 55
30 / 55
  C-c C-c^CERROR: InterruptException:
Stacktrace:
 [1] done at ./iterators.jl:244 [inlined] (repeats 3 times)
 [2] done at ./iterators.jl:734 [inlined]
 [3] _collect(::UnitRange{Int64}, ::Base.Iterators.Flatten{Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip2{Array{Int16,1},Array{Int16,1}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}, ::Base.HasEltype, ::Base.SizeUnknown) at ./array.jl:442
 [4] collect(::Base.Iterators.Flatten{Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip{Array{Int16,1},Base.Iterators.Zip2{Array{Int16,1},Array{Int16,1}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}) at ./array.jl:431
 [5] softloss(::BiaffineDecoder2, ::AutoGrad.Rec{Knet.KnetArray{Float32,3}}, ::AutoGrad.Rec{Knet.KnetArray{Float32,3}}, ::Array{Any,1}) at /scratch/users/cgumeli/Nubik/./decoders/biaffine2.jl:119
 [6] loss(::AutoGrad.Rec{Array{Any,1}}, ::FreshEncoder, ::BiaffineDecoder2, ::Array{Any,1}) at /scratch/users/cgumeli/Nubik/main2.jl:20
 [7] forward_pass(::Function, ::Tuple{Array{Any,1},FreshEncoder,BiaffineDecoder2,Array{Any,1}}, ::Array{Any,1}, ::Int64) at /scratch/users/cgumeli/.julia/v0.6/AutoGrad/src/core.jl:88
 [8] (::AutoGrad.##gradfun#4#6{#loss,Int64})(::Array{Any,1}, ::Function, ::Array{Any,1}, ::Vararg{Any,N} where N) at /scratch/users/cgumeli/.julia/v0.6/AutoGrad/src/core.jl:57
 [9] (::AutoGrad.#gradfun#5)(::Array{Any,1}, ::Vararg{Any,N} where N) at /scratch/users/cgumeli/.julia/v0.6/AutoGrad/src/core.jl:57
 [10] macro expansion at /scratch/users/cgumeli/Nubik/main2.jl:194 [inlined]
 [11] macro expansion at ./util.jl:237 [inlined]
 [12] train() at /scratch/users/cgumeli/Nubik/main2.jl:192
 [13] main(::String) at /scratch/users/cgumeli/Nubik/main2.jl:83

julia> exit()
exit()


bash-4.2$ bash-4.2$ 

bash-4.2$ julia
julia
               _
   _       _ _(_)_     |  A fresh approach to technical computing
  (_)     | (_) (_)    |  Documentation: https://docs.julialang.org
   _ _   _| |_  __ _   |  Type "?help" for help.
  | | | | | | |/ _` |  |
  | | |_| | | | (_| |  |  Version 0.6.2 (2017-12-13 18:08 UTC)
 _/ |\__'_|_|_|\__'_|  |  Official http://julialang.org/ release
|__/                   |  x86_64-pc-linux-gnu

WARNING: Terminal not fully functional
julia> include("main2.jl")
include("main2.jl")
INFO: Checking dependencies
INFO: All dependencies are installed
accuracy_bt

julia> main("--datafiles $trtrn $trdev --lmfile $trlm --upos 50 --birnnhidden 200 --lmdrop 0.33 --birnndrop 0.33 --decdrop 0.33 --tokens 100 --birnn 3")
main("--datafiles $trtrn $trdev --lmfile $trlm --upos 50 --birnnhidden 200 --lmdrop 0.33 --birnndrop 0.33 --decdrop 0.33 --tokens 100 --birnn 3")
Tuple{Symbol,Any}[(:feat, 0), (:birnn, 3), (:birnndrop, 0.33), (:lmfile, "/scratch/users/okirnap/ud-treebanks-v2.2/chmodel_converted/turkish_chmodel.jld"), (:min, 2), (:birnncell, "LSTM"), (:labelunits, 100), (:xpos, 0), (:batchsize, 8), (:max, 128), (:decdrop, 0.33), (:upos, 50), (:backupdir, "Backups_2018-06-09T01:24:56.562"), (:beta1, 0.9), (:remb, 350), (:algo, "edmonds"), (:beta2, 0.999), (:lmdrop, 0.33), (:proj, true), (:tokens, 100), (:epochs, 100), (:recover, ""), (:birnnhidden, 200), (:optim, "Adam"), (:lr, 0.001), (:datafiles, Any["/scratch/users/okirnap/ud-treebanks-v2.2/UD_Turkish-IMST/tr_imst-ud-train.conllu", "/scratch/users/okirnap/ud-treebanks-v2.2/UD_Turkish-IMST/tr_imst-ud-dev.conllu"]), (:arcunits, 400)]
INFO: Loading data
01:25:00 /scratch/users/okirnap/ud-treebanks-v2.2/chmodel_converted/turkish_chmodel.jld 
01:25:06 /scratch/users/okirnap/ud-treebanks-v2.2/UD_Turkish-IMST/tr_imst-ud-train.conllu 
01:25:06 /scratch/users/okirnap/ud-treebanks-v2.2/UD_Turkish-IMST/tr_imst-ud-dev.conllu 
01:25:43 Caching lm vectors... 
INFO: Bucketing
INFO: Initializing Model and Optimizers
INFO: Transfering to gpu.
INFO: Setting momentums
INFO: Computing validation performance
parsers/mst.py:24: RuntimeWarning: divide by zero encountered in divide
  new_root = tokens[np.argmax(root_scores / head_scores)]
10 / 149
20 / 149
30 / 149
40 / 149
50 / 149
60 / 149
70 / 149
80 / 149
90 / 149
100 / 149
110 / 149
120 / 149
130 / 149
140 / 149
 14.228460 seconds (6.12 M allocations: 427.400 MiB, 3.50% gc time)
Validation loss: 64.25259
Unlabeled attachment score: 11.107781440415543
Labeled attachment score: 0.2796923384277295

Epoch 1
10 / 442
20 / 442
30 / 442
40 / 442
50 / 442
60 / 442
70 / 442
80 / 442
90 / 442
100 / 442
110 / 442
120 / 442
130 / 442
140 / 442
150 / 442
160 / 442
170 / 442
180 / 442
190 / 442
200 / 442
210 / 442
220 / 442
230 / 442
240 / 442
250 / 442
260 / 442
270 / 442
280 / 442
290 / 442
300 / 442
310 / 442
320 / 442
330 / 442
340 / 442
350 / 442
360 / 442
370 / 442
380 / 442
390 / 442
400 / 442
410 / 442
420 / 442
430 / 442
440 / 442
 41.492776 seconds (26.50 M allocations: 1.506 GiB, 1.36% gc time)
Training loss: 31.944035


INFO: Computing validation performance
10 / 149
20 / 149
30 / 149
40 / 149
50 / 149
60 / 149
70 / 149
80 / 149
90 / 149
100 / 149
110 / 149
120 / 149
130 / 149
140 / 149
  3.698152 seconds (2.66 M allocations: 247.571 MiB, 1.79% gc time)
Validation loss: 25.968672
Unlabeled attachment score: 47.837378883228446
Labeled attachment score: 40.11587254020577
INFO: Backing up

Epoch 2
10 / 442
20 / 442
30 / 442
40 / 442
50 / 442
60 / 442
70 / 442
80 / 442
90 / 442
100 / 442
110 / 442
120 / 442
130 / 442
140 / 442
150 / 442
160 / 442
170 / 442
180 / 442
190 / 442
200 / 442
210 / 442
220 / 442
230 / 442
240 / 442
250 / 442
260 / 442
270 / 442
280 / 442
290 / 442
300 / 442
310 / 442
320 / 442
330 / 442
340 / 442
350 / 442
360 / 442
370 / 442
380 / 442
390 / 442
400 / 442
410 / 442
420 / 442
430 / 442
440 / 442
 22.801158 seconds (23.84 M allocations: 1.363 GiB, 3.29% gc time)
Training loss: 23.822262


INFO: Computing validation performance
10 / 149
20 / 149
30 / 149
40 / 149
50 / 149
60 / 149
70 / 149
80 / 149
90 / 149
100 / 149
110 / 149
120 / 149
130 / 149
140 / 149
  3.809991 seconds (2.65 M allocations: 247.434 MiB, 1.55% gc time)
Validation loss: 24.284702
Unlabeled attachment score: 51.68314853660973
Labeled attachment score: 44.38118070122865
INFO: Backing up

Epoch 3
10 / 442
20 / 442
30 / 442
40 / 442
50 / 442
60 / 442
70 / 442
80 / 442
90 / 442
100 / 442
110 / 442
120 / 442
130 / 442
140 / 442
150 / 442
160 / 442
170 / 442
180 / 442
190 / 442
200 / 442
210 / 442
220 / 442
230 / 442
240 / 442
250 / 442
260 / 442
270 / 442
280 / 442
290 / 442
300 / 442
310 / 442
320 / 442
330 / 442
340 / 442
350 / 442
360 / 442
370 / 442
380 / 442
390 / 442
400 / 442
410 / 442
420 / 442
430 / 442
440 / 442
 22.569596 seconds (23.84 M allocations: 1.363 GiB, 3.06% gc time)
Training loss: 22.118973


INFO: Computing validation performance
10 / 149
20 / 149
30 / 149
40 / 149
50 / 149
60 / 149
70 / 149
80 / 149
90 / 149
100 / 149
110 / 149
120 / 149
130 / 149
140 / 149
  3.982839 seconds (2.66 M allocations: 247.560 MiB, 1.74% gc time)
Validation loss: 22.503927
Unlabeled attachment score: 53.03166516831485
Labeled attachment score: 46.119268804315254
INFO: Backing up

Epoch 4
10 / 442
20 / 442
30 / 442
40 / 442
50 / 442
60 / 442
70 / 442
80 / 442
90 / 442
100 / 442
110 / 442
120 / 442
130 / 442
140 / 442
150 / 442
160 / 442
170 / 442
180 / 442
190 / 442
200 / 442
210 / 442
220 / 442
230 / 442
240 / 442
250 / 442
260 / 442
270 / 442
280 / 442
290 / 442
300 / 442
310 / 442
320 / 442
330 / 442
340 / 442
350 / 442
360 / 442
370 / 442
380 / 442
390 / 442
400 / 442
410 / 442
420 / 442
430 / 442
440 / 442
 22.084367 seconds (23.84 M allocations: 1.363 GiB, 2.13% gc time)
Training loss: 21.47266


INFO: Computing validation performance
10 / 149
20 / 149
30 / 149
40 / 149
50 / 149
60 / 149
70 / 149
80 / 149
90 / 149
100 / 149
110 / 149
120 / 149
130 / 149
140 / 149
  4.183279 seconds (2.66 M allocations: 247.457 MiB, 1.63% gc time)
Validation loss: 22.337055
Unlabeled attachment score: 53.32134651882929
Labeled attachment score: 46.46888422734992
INFO: Backing up

Epoch 5
10 / 442
20 / 442
30 / 442
40 / 442
50 / 442
60 / 442
70 / 442
80 / 442
90 / 442
100 / 442
110 / 442
120 / 442
130 / 442
140 / 442
150 / 442
160 / 442
170 / 442
180 / 442
190 / 442
200 / 442
210 / 442
220 / 442
230 / 442
240 / 442
250 / 442
260 / 442
270 / 442
280 / 442
290 / 442
300 / 442
310 / 442
320 / 442
330 / 442
340 / 442
350 / 442
360 / 442
370 / 442
380 / 442
390 / 442
400 / 442
410 / 442
420 / 442
430 / 442
440 / 442
 22.418787 seconds (23.83 M allocations: 1.363 GiB, 3.47% gc time)
Training loss: 20.350204


INFO: Computing validation performance
10 / 149
20 / 149
30 / 149
40 / 149
50 / 149
60 / 149
70 / 149
80 / 149
90 / 149
100 / 149
110 / 149
120 / 149
130 / 149
140 / 149
  4.292950 seconds (2.66 M allocations: 247.567 MiB, 1.56% gc time)
Validation loss: 22.176943
Unlabeled attachment score: 56.47787433822795
Labeled attachment score: 49.35570872040755
INFO: Backing up

Epoch 6
10 / 442
20 / 442
30 / 442
40 / 442
50 / 442
60 / 442
70 / 442
80 / 442
90 / 442
100 / 442
110 / 442
120 / 442
130 / 442
140 / 442
150 / 442
160 / 442
170 / 442
180 / 442
190 / 442
200 / 442
210 / 442
220 / 442
230 / 442
240 / 442
250 / 442
260 / 442
270 / 442
280 / 442
290 / 442
300 / 442
310 / 442
320 / 442
330 / 442
340 / 442
350 / 442
360 / 442
370 / 442
380 / 442
390 / 442
400 / 442
410 / 442
420 / 442
430 / 442
440 / 442
 22.008010 seconds (23.84 M allocations: 1.363 GiB, 2.11% gc time)
Training loss: 19.927107


INFO: Computing validation performance
10 / 149
20 / 149
30 / 149
40 / 149
50 / 149
60 / 149
70 / 149
80 / 149
90 / 149
100 / 149
110 / 149
120 / 149
130 / 149
140 / 149
  4.338559 seconds (2.66 M allocations: 247.456 MiB, 1.65% gc time)
Validation loss: 22.843054
Unlabeled attachment score: 56.188192987713514
Labeled attachment score: 48.57656577764459
INFO: Backing up

Epoch 7
10 / 442
20 / 442
30 / 442
40 / 442
50 / 442
60 / 442
70 / 442
80 / 442
90 / 442
100 / 442
110 / 442
120 / 442
130 / 442
140 / 442
150 / 442
160 / 442
170 / 442
180 / 442
190 / 442
200 / 442
210 / 442
220 / 442
230 / 442
240 / 442
250 / 442
260 / 442
270 / 442
280 / 442
290 / 442
300 / 442
310 / 442
320 / 442
330 / 442
340 / 442
350 / 442
360 / 442
370 / 442
380 / 442
390 / 442
400 / 442
410 / 442
420 / 442
430 / 442
440 / 442
 22.608112 seconds (23.83 M allocations: 1.363 GiB, 3.54% gc time)
Training loss: 19.203594


INFO: Computing validation performance
10 / 149
20 / 149
30 / 149
40 / 149
50 / 149
60 / 149
70 / 149
80 / 149
90 / 149
100 / 149
110 / 149
120 / 149
130 / 149
140 / 149
  4.629975 seconds (2.66 M allocations: 247.563 MiB, 1.95% gc time)
Validation loss: 22.051842
Unlabeled attachment score: 56.348017181100786
Labeled attachment score: 49.29577464788732
INFO: Backing up

Epoch 8
10 / 442
20 / 442
30 / 442
40 / 442
50 / 442
60 / 442
70 / 442
80 / 442
90 / 442
100 / 442
110 / 442
120 / 442
130 / 442
140 / 442
150 / 442
160 / 442
170 / 442
180 / 442
190 / 442
200 / 442
210 / 442
220 / 442
230 / 442
240 / 442
250 / 442
260 / 442
270 / 442
280 / 442
290 / 442
300 / 442
310 / 442
320 / 442
330 / 442
340 / 442
350 / 442
360 / 442
370 / 442
380 / 442
390 / 442
400 / 442
410 / 442
420 / 442
430 / 442
440 / 442
 22.149718 seconds (23.83 M allocations: 1.363 GiB, 2.17% gc time)
Training loss: 18.87791


INFO: Computing validation performance
10 / 149
20 / 149
30 / 149
40 / 149
50 / 149
60 / 149
70 / 149
80 / 149
90 / 149
100 / 149
110 / 149
120 / 149
130 / 149
140 / 149
  4.528661 seconds (2.66 M allocations: 247.457 MiB, 1.39% gc time)
Validation loss: 21.652521
Unlabeled attachment score: 59.035061432424335
Labeled attachment score: 51.14374188392768
INFO: Backing up

Epoch 9
10 / 442
20 / 442
30 / 442
40 / 442
50 / 442
60 / 442
70 / 442
80 / 442
90 / 442
100 / 442
110 / 442
120 / 442
130 / 442
140 / 442
150 / 442
160 / 442
170 / 442
180 / 442
190 / 442
200 / 442
210 / 442
220 / 442
230 / 442
240 / 442
250 / 442
260 / 442
270 / 442
280 / 442
290 / 442
300 / 442
310 / 442
320 / 442
330 / 442
340 / 442
350 / 442
360 / 442
370 / 442
380 / 442
390 / 442
400 / 442
410 / 442
420 / 442
430 / 442
440 / 442
 22.425008 seconds (23.83 M allocations: 1.363 GiB, 3.22% gc time)
Training loss: 18.381002


INFO: Computing validation performance
10 / 149
20 / 149
30 / 149
40 / 149
50 / 149
60 / 149
70 / 149
80 / 149
90 / 149
100 / 149
110 / 149
120 / 149
130 / 149
140 / 149
  4.907623 seconds (2.66 M allocations: 247.564 MiB, 1.46% gc time)
Validation loss: 21.928572
Unlabeled attachment score: 59.15492957746479
Labeled attachment score: 51.26361002896814
INFO: Backing up

Epoch 10
10 / 442
20 / 442
30 / 442
40 / 442
50 / 442
60 / 442
70 / 442
80 / 442
90 / 442
100 / 442
110 / 442
120 / 442
130 / 442
140 / 442
150 / 442
160 / 442
170 / 442
180 / 442
190 / 442
200 / 442
210 / 442
220 / 442
230 / 442
240 / 442
250 / 442
260 / 442
270 / 442
280 / 442
290 / 442
300 / 442
310 / 442
320 / 442
330 / 442
340 / 442
350 / 442
360 / 442
370 / 442
380 / 442
390 / 442
400 / 442
410 / 442
420 / 442
430 / 442
440 / 442
 22.271543 seconds (23.83 M allocations: 1.363 GiB, 2.16% gc time)
Training loss: 17.77963


INFO: Computing validation performance
10 / 149
20 / 149
30 / 149
40 / 149
50 / 149
60 / 149
70 / 149
80 / 149
90 / 149
100 / 149
110 / 149
120 / 149
130 / 149
140 / 149
  4.834679 seconds (2.65 M allocations: 247.454 MiB, 1.27% gc time)
Validation loss: 21.503115
Unlabeled attachment score: 59.85416042353411
Labeled attachment score: 52.03276395964439
INFO: Backing up

Epoch 11
10 / 442
20 / 442
30 / 442
40 / 442
50 / 442
60 / 442
70 / 442
80 / 442
90 / 442
100 / 442
110 / 442
120 / 442
130 / 442
140 / 442
150 / 442
160 / 442
170 / 442
180 / 442
190 / 442
200 / 442
210 / 442
220 / 442
230 / 442
240 / 442
250 / 442
260 / 442
270 / 442
280 / 442
290 / 442
300 / 442
310 / 442
320 / 442
330 / 442
340 / 442
350 / 442
360 / 442
370 / 442
380 / 442
390 / 442
400 / 442
410 / 442
420 / 442
430 / 442
440 / 442
 22.530577 seconds (23.83 M allocations: 1.363 GiB, 3.27% gc time)
Training loss: 17.441717


INFO: Computing validation performance
10 / 149
20 / 149
30 / 149
40 / 149
50 / 149
60 / 149
70 / 149
80 / 149
90 / 149
100 / 149
110 / 149
120 / 149
130 / 149
140 / 149
  5.082425 seconds (2.66 M allocations: 247.568 MiB, 1.50% gc time)
Validation loss: 22.416569
Unlabeled attachment score: 60.013984616921384
Labeled attachment score: 51.912895814603935
INFO: Backing up

Epoch 12
10 / 442
20 / 442
30 / 442
40 / 442
50 / 442
60 / 442
70 / 442
80 / 442
90 / 442
100 / 442
110 / 442
120 / 442
130 / 442
140 / 442
150 / 442
160 / 442
170 / 442
180 / 442
190 / 442
200 / 442
210 / 442
220 / 442
230 / 442
240 / 442
250 / 442
260 / 442
270 / 442
280 / 442
290 / 442
300 / 442
310 / 442
320 / 442
330 / 442
340 / 442
350 / 442
360 / 442
370 / 442
380 / 442
390 / 442
400 / 442
410 / 442
420 / 442
430 / 442
440 / 442
 21.980308 seconds (23.83 M allocations: 1.363 GiB, 2.15% gc time)
Training loss: 16.68571


INFO: Computing validation performance
10 / 149
20 / 149
30 / 149
40 / 149
50 / 149
60 / 149
70 / 149
80 / 149
90 / 149
100 / 149
110 / 149
120 / 149
130 / 149
140 / 149
  5.341143 seconds (2.65 M allocations: 247.455 MiB, 1.46% gc time)
Validation loss: 22.407137
Unlabeled attachment score: 58.83528119069024
Labeled attachment score: 51.553291379482566
INFO: Backing up

Epoch 13
10 / 442
20 / 442
30 / 442
40 / 442
50 / 442
60 / 442
70 / 442
80 / 442
90 / 442
100 / 442
110 / 442
120 / 442
130 / 442
140 / 442
150 / 442
  C-c C-c^CERROR: InterruptException:
Stacktrace:
 [1] (::AutoGrad.##rfun#7#10{Base.#getindex})(::Array{Any,1}, ::Function, ::AutoGrad.Rec{Knet.KnetArray{Float32,2}}, ::Vararg{Any,N} where N) at /scratch/users/cgumeli/.julia/v0.6/AutoGrad/src/core.jl:0
 [2] getindex(::AutoGrad.Rec{Knet.KnetArray{Float32,2}}, ::Colon, ::Int64) at ./<missing>:0
 [3] next at ./iterators.jl:724 [inlined]
 [4] grow_to!(::Array{AutoGrad.Rec{Knet.KnetArray{Float32,1}},1}, ::Base.Iterators.Flatten{Base.Generator{UnitRange{Int64},##57#63{Int64,Array{Int64,2}}}}, ::Tuple{Int64,Base.Generator{UnitRange{Int64},##58#64{Int64,Array{Int64,2}}},Int64}) at ./array.jl:532
 [5] grow_to!(::Array{Union{},1}, ::Base.Iterators.Flatten{Base.Generator{UnitRange{Int64},##57#63{Int64,Array{Int64,2}}}}, ::Tuple{Int64,Base.Generator{UnitRange{Int64},##58#64{Int64,Array{Int64,2}}},Int64}) at ./array.jl:540
 [6] grow_to!(::Array{Any,1}, ::Base.Iterators.Flatten{Base.Generator{UnitRange{Int64},##57#63{Int64,Array{Int64,2}}}}) at ./array.jl:525
 [7] #call#55(::Bool, ::BiaffineDecoder2, ::AutoGrad.Rec{Array{Any,1}}, ::AutoGrad.Rec{Knet.KnetArray{Float32,3}}) at /scratch/users/cgumeli/Nubik/./decoders/biaffine2.jl:95
 [8] loss(::AutoGrad.Rec{Array{Any,1}}, ::FreshEncoder, ::BiaffineDecoder2, ::Array{Any,1}) at /scratch/users/cgumeli/Nubik/main2.jl:19
 [9] forward_pass(::Function, ::Tuple{Array{Any,1},FreshEncoder,BiaffineDecoder2,Array{Any,1}}, ::Array{Any,1}, ::Int64) at /scratch/users/cgumeli/.julia/v0.6/AutoGrad/src/core.jl:88
 [10] (::AutoGrad.##gradfun#4#6{#loss,Int64})(::Array{Any,1}, ::Function, ::Array{Any,1}, ::Vararg{Any,N} where N) at /scratch/users/cgumeli/.julia/v0.6/AutoGrad/src/core.jl:57
 [11] (::AutoGrad.#gradfun#5)(::Array{Any,1}, ::Vararg{Any,N} where N) at /scratch/users/cgumeli/.julia/v0.6/AutoGrad/src/core.jl:57
 [12] macro expansion at /scratch/users/cgumeli/Nubik/main2.jl:194 [inlined]
 [13] macro expansion at ./util.jl:237 [inlined]
 [14] train() at /scratch/users/cgumeli/Nubik/main2.jl:192
 [15] main(::String) at /scratch/users/cgumeli/Nubik/main2.jl:83

julia> exit()
exit()
bash-4.2$ julia
julia
               _
   _       _ _(_)_     |  A fresh approach to technical computing
  (_)     | (_) (_)    |  Documentation: https://docs.julialang.org
   _ _   _| |_  __ _   |  Type "?help" for help.
  | | | | | | |/ _` |  |
  | | |_| | | | (_| |  |  Version 0.6.2 (2017-12-13 18:08 UTC)
 _/ |\__'_|_|_|\__'_|  |  Official http://julialang.org/ release
|__/                   |  x86_64-pc-linux-gnu

WARNING: Terminal not fully functional
julia> include("main2.jl")
include("main2.jl")
INFO: Checking dependencies
INFO: All dependencies are installed
accuracy_bt

julia> main("--datafiles $trtrn $trdev --lmfile $trlm --upos 50 --birnnhidden 200 --lmdrop 0.66 --birnndrop 0.66 --decdrop 0.66 --tokens 100 --birnn 3")
main("--datafiles $trtrn $trdev --lmfile $trlm --upos 50 --birnnhidden 200 --lmdrop 0.66 --birnndrop 0.66 --decdrop 0.66 --tokens 100 --birnn 3")
Tuple{Symbol,Any}[(:feat, 0), (:birnn, 3), (:birnndrop, 0.66), (:lmfile, "/scratch/users/okirnap/ud-treebanks-v2.2/chmodel_converted/turkish_chmodel.jld"), (:min, 2), (:birnncell, "LSTM"), (:labelunits, 100), (:xpos, 0), (:batchsize, 8), (:max, 128), (:decdrop, 0.66), (:upos, 50), (:backupdir, "Backups_2018-06-09T01:33:49.808"), (:beta1, 0.9), (:remb, 350), (:algo, "edmonds"), (:beta2, 0.999), (:lmdrop, 0.66), (:proj, true), (:tokens, 100), (:epochs, 100), (:recover, ""), (:birnnhidden, 200), (:optim, "Adam"), (:lr, 0.001), (:datafiles, Any["/scratch/users/okirnap/ud-treebanks-v2.2/UD_Turkish-IMST/tr_imst-ud-train.conllu", "/scratch/users/okirnap/ud-treebanks-v2.2/UD_Turkish-IMST/tr_imst-ud-dev.conllu"]), (:arcunits, 400)]
INFO: Loading data
01:33:53 /scratch/users/okirnap/ud-treebanks-v2.2/chmodel_converted/turkish_chmodel.jld 
01:33:59 /scratch/users/okirnap/ud-treebanks-v2.2/UD_Turkish-IMST/tr_imst-ud-train.conllu 
01:33:59 /scratch/users/okirnap/ud-treebanks-v2.2/UD_Turkish-IMST/tr_imst-ud-dev.conllu 
01:34:35 Caching lm vectors... 
INFO: Bucketing
INFO: Initializing Model and Optimizers
INFO: Transfering to gpu.
INFO: Setting momentums
INFO: Computing validation performance
parsers/mst.py:24: RuntimeWarning: divide by zero encountered in divide
  new_root = tokens[np.argmax(root_scores / head_scores)]
10 / 149
20 / 149
30 / 149
40 / 149
50 / 149
60 / 149
70 / 149
80 / 149
90 / 149
100 / 149
110 / 149
120 / 149
130 / 149
140 / 149
 14.279487 seconds (6.11 M allocations: 427.274 MiB, 3.54% gc time)
Validation loss: 64.39403
Unlabeled attachment score: 10.3685945459994
Labeled attachment score: 0.1997802417340925

Epoch 1
  C-c C-c^CERROR: InterruptException:
Stacktrace:
 [1] +(::AutoGrad.Broadcasted{AutoGrad.Rec{Knet.KnetArray{Float32,3}}}, ::AutoGrad.Broadcasted{AutoGrad.Rec{Knet.KnetArray{Float32,3}}}) at /scratch/users/cgumeli/.julia/v0.6/AutoGrad/src/unfuse.jl:49
 [2] broadcast(::Function, ::AutoGrad.Rec{Knet.KnetArray{Float32,3}}, ::AutoGrad.Rec{Knet.KnetArray{Float32,3}}) at /scratch/users/cgumeli/.julia/v0.6/AutoGrad/src/unfuse.jl:37
 [3] #call#55(::Bool, ::BiaffineDecoder2, ::AutoGrad.Rec{Array{Any,1}}, ::AutoGrad.Rec{Knet.KnetArray{Float32,3}}) at /scratch/users/cgumeli/Nubik/./decoders/biaffine2.jl:84
 [4] loss(::AutoGrad.Rec{Array{Any,1}}, ::FreshEncoder, ::BiaffineDecoder2, ::Array{Any,1}) at /scratch/users/cgumeli/Nubik/main2.jl:19
 [5] forward_pass(::Function, ::Tuple{Array{Any,1},FreshEncoder,BiaffineDecoder2,Array{Any,1}}, ::Array{Any,1}, ::Int64) at /scratch/users/cgumeli/.julia/v0.6/AutoGrad/src/core.jl:88
 [6] (::AutoGrad.##gradfun#4#6{#loss,Int64})(::Array{Any,1}, ::Function, ::Array{Any,1}, ::Vararg{Any,N} where N) at /scratch/users/cgumeli/.julia/v0.6/AutoGrad/src/core.jl:57
 [7] (::AutoGrad.#gradfun#5)(::Array{Any,1}, ::Vararg{Any,N} where N) at /scratch/users/cgumeli/.julia/v0.6/AutoGrad/src/core.jl:57
 [8] macro expansion at /scratch/users/cgumeli/Nubik/main2.jl:194 [inlined]
 [9] macro expansion at ./util.jl:237 [inlined]
 [10] train() at /scratch/users/cgumeli/Nubik/main2.jl:192
 [11] main(::String) at /scratch/users/cgumeli/Nubik/main2.jl:83

julia> main("--datafiles $trtrn $trdev --lmfile $trlm --upos 50 --birnnhidden 200 --lmdrop 0.66 --birnndrop 0.66 --decdrop 0.66 --tokens 500 --birnn 3")
main("--datafiles $trtrn $trdev --lmfile $trlm --upos 50 --birnnhidden 200 --lmdrop 0.66 --birnndrop 0.66 --decdrop 0.66 --tokens 500 --birnn 3")
Tuple{Symbol,Any}[(:feat, 0), (:birnn, 3), (:birnndrop, 0.66), (:lmfile, "/scratch/users/okirnap/ud-treebanks-v2.2/chmodel_converted/turkish_chmodel.jld"), (:min, 2), (:birnncell, "LSTM"), (:labelunits, 100), (:xpos, 0), (:batchsize, 8), (:max, 128), (:decdrop, 0.66), (:upos, 50), (:backupdir, "Backups_2018-06-09T01:35:13.428"), (:beta1, 0.9), (:remb, 350), (:algo, "edmonds"), (:beta2, 0.999), (:lmdrop, 0.66), (:proj, true), (:tokens, 500), (:epochs, 100), (:recover, ""), (:birnnhidden, 200), (:optim, "Adam"), (:lr, 0.001), (:datafiles, Any["/scratch/users/okirnap/ud-treebanks-v2.2/UD_Turkish-IMST/tr_imst-ud-train.conllu", "/scratch/users/okirnap/ud-treebanks-v2.2/UD_Turkish-IMST/tr_imst-ud-dev.conllu"]), (:arcunits, 400)]
INFO: Loading data
01:35:13 /scratch/users/okirnap/ud-treebanks-v2.2/chmodel_converted/turkish_chmodel.jld 
01:35:15 /scratch/users/okirnap/ud-treebanks-v2.2/UD_Turkish-IMST/tr_imst-ud-train.conllu 
01:35:16 /scratch/users/okirnap/ud-treebanks-v2.2/UD_Turkish-IMST/tr_imst-ud-dev.conllu 
01:35:49 Caching lm vectors... 
INFO: Bucketing
INFO: Initializing Model and Optimizers
INFO: Transfering to gpu.
ERROR: cudnn.cudnnSetDropoutDescriptor error 8
Stacktrace:
 [1] macro expansion at /scratch/users/cgumeli/.julia/v0.6/Knet/src/gpu.jl:18 [inlined]
 [2] #DD#469(::Ptr{Void}, ::Float64, ::Int64, ::Array{Any,1}, ::Type{T} where T) at /scratch/users/cgumeli/.julia/v0.6/Knet/src/rnn.jl:28
 [3] (::Core.#kw#Type)(::Array{Any,1}, ::Type{Knet.DD}) at ./<missing>:0
 [4] #rnninit#486(::Ptr{Void}, ::Int64, ::Float64, ::Bool, ::Bool, ::Symbol, ::Type{T} where T, ::Int64, ::Int64, ::Knet.#xavier, ::Base.#zeros, ::Bool, ::Knet.#rnninit, ::Int32, ::Int32) at /scratch/users/cgumeli/.julia/v0.6/Knet/src/rnn.jl:367
 [5] (::Knet.#kw##rnninit)(::Array{Any,1}, ::Knet.#rnninit, ::Int32, ::Int32) at ./<missing>:0
 [6] convert_params!(::KnetModules.LSTM, ::Type{T} where T) at /scratch/users/cgumeli/.julia/v0.6/KnetModules/src/rnn.jl:68
 [7] gpu!(::FreshEncoder) at /scratch/users/cgumeli/.julia/v0.6/KnetModules/src/core.jl:238
 [8] train() at /scratch/users/cgumeli/Nubik/main2.jl:117
 [9] main(::String) at /scratch/users/cgumeli/Nubik/main2.jl:83

julia> exit()
exit()
bash-4.2$ 

bash-4.2$ julia
julia
               _
   _       _ _(_)_     |  A fresh approach to technical computing
  (_)     | (_) (_)    |  Documentation: https://docs.julialang.org
   _ _   _| |_  __ _   |  Type "?help" for help.
  | | | | | | |/ _` |  |
  | | |_| | | | (_| |  |  Version 0.6.2 (2017-12-13 18:08 UTC)
 _/ |\__'_|_|_|\__'_|  |  Official http://julialang.org/ release
|__/                   |  x86_64-pc-linux-gnu

WARNING: Terminal not fully functional
julia> include("main2.jl")
include("main2.jl")
INFO: Checking dependencies
INFO: All dependencies are installed
accuracy_bt

julia> main("--datafiles $trtrn $trdev --lmfile $trlm --upos 50 --birnnhidden 200 --lmdrop 0.66 --birnndrop 0.66 --decdrop 0.66 --tokens 500 --birnn 3")
main("--datafiles $trtrn $trdev --lmfile $trlm --upos 50 --birnnhidden 200 --lmdrop 0.66 --birnndrop 0.66 --decdrop 0.66 --tokens 500 --birnn 3")
Tuple{Symbol,Any}[(:feat, 0), (:birnn, 3), (:birnndrop, 0.66), (:lmfile, "/scratch/users/okirnap/ud-treebanks-v2.2/chmodel_converted/turkish_chmodel.jld"), (:min, 2), (:birnncell, "LSTM"), (:labelunits, 100), (:xpos, 0), (:batchsize, 8), (:max, 128), (:decdrop, 0.66), (:upos, 50), (:backupdir, "Backups_2018-06-09T01:36:46.554"), (:beta1, 0.9), (:remb, 350), (:algo, "edmonds"), (:beta2, 0.999), (:lmdrop, 0.66), (:proj, true), (:tokens, 500), (:epochs, 100), (:recover, ""), (:birnnhidden, 200), (:optim, "Adam"), (:lr, 0.001), (:datafiles, Any["/scratch/users/okirnap/ud-treebanks-v2.2/UD_Turkish-IMST/tr_imst-ud-train.conllu", "/scratch/users/okirnap/ud-treebanks-v2.2/UD_Turkish-IMST/tr_imst-ud-dev.conllu"]), (:arcunits, 400)]
INFO: Loading data
01:36:50 /scratch/users/okirnap/ud-treebanks-v2.2/chmodel_converted/turkish_chmodel.jld 
01:36:56 /scratch/users/okirnap/ud-treebanks-v2.2/UD_Turkish-IMST/tr_imst-ud-train.conllu 
01:36:56 /scratch/users/okirnap/ud-treebanks-v2.2/UD_Turkish-IMST/tr_imst-ud-dev.conllu 
01:37:33 Caching lm vectors... 
INFO: Bucketing
INFO: Initializing Model and Optimizers
INFO: Transfering to gpu.
INFO: Setting momentums
INFO: Computing validation performance
parsers/mst.py:24: RuntimeWarning: divide by zero encountered in divide
  new_root = tokens[np.argmax(root_scores / head_scores)]
10 / 49
20 / 49
30 / 49
40 / 49
 38.462293 seconds (11.79 M allocations: 930.312 MiB, 1.80% gc time)
Validation loss: 64.544525
Unlabeled attachment score: 7.331934871641194
Labeled attachment score: 0.09989012086704625

Epoch 1
10 / 55
20 / 55
30 / 55
40 / 55
50 / 55
 43.686003 seconds (20.63 M allocations: 2.683 GiB, 2.64% gc time)
Training loss: 43.787674


INFO: Computing validation performance
10 / 49
20 / 49
30 / 49
40 / 49
  3.023223 seconds (3.17 M allocations: 508.743 MiB, 4.86% gc time)
Validation loss: 37.48809
Unlabeled attachment score: 32.57416841474378
Labeled attachment score: 23.17450804115473
INFO: Backing up

Epoch 2
10 / 55
20 / 55
30 / 55
40 / 55
50 / 55
  8.991435 seconds (15.08 M allocations: 2.393 GiB, 11.77% gc time)
Training loss: 34.287216


INFO: Computing validation performance
10 / 49
20 / 49
30 / 49
40 / 49
  2.847547 seconds (3.17 M allocations: 508.718 MiB, 5.29% gc time)
Validation loss: 33.117832
Unlabeled attachment score: 37.88832284487064
Labeled attachment score: 30.296673658975127
INFO: Backing up

Epoch 3
10 / 55
20 / 55
30 / 55
40 / 55
50 / 55
 10.774652 seconds (15.09 M allocations: 2.394 GiB, 8.76% gc time)
Training loss: 31.494312


INFO: Computing validation performance
10 / 49
20 / 49
30 / 49
40 / 49
  2.998915 seconds (3.17 M allocations: 508.734 MiB, 5.05% gc time)
Validation loss: 30.389795
Unlabeled attachment score: 40.80511437418839
Labeled attachment score: 33.443212466287086
INFO: Backing up

Epoch 4
10 / 55
20 / 55
30 / 55
40 / 55
50 / 55
  8.948444 seconds (15.10 M allocations: 2.394 GiB, 12.45% gc time)
Training loss: 29.86794


INFO: Computing validation performance
10 / 49
20 / 49
30 / 49
40 / 49
  2.867379 seconds (3.17 M allocations: 508.722 MiB, 5.81% gc time)
Validation loss: 29.776093
Unlabeled attachment score: 43.16252122665068
Labeled attachment score: 36.2900809109979
INFO: Backing up

Epoch 5
10 / 55
20 / 55
30 / 55
40 / 55
50 / 55
  9.210054 seconds (15.11 M allocations: 2.394 GiB, 15.45% gc time)
Training loss: 28.870365


INFO: Computing validation performance
10 / 49
20 / 49
30 / 49
40 / 49
  2.886683 seconds (3.17 M allocations: 508.719 MiB, 4.70% gc time)
Validation loss: 28.841375
Unlabeled attachment score: 44.99051043851763
Labeled attachment score: 37.918289881130754
INFO: Backing up

Epoch 6
10 / 55
20 / 55
30 / 55
40 / 55
50 / 55
  9.366391 seconds (15.07 M allocations: 2.393 GiB, 16.72% gc time)
Training loss: 27.96037


INFO: Computing validation performance
10 / 49
20 / 49
30 / 49
40 / 49
  2.916695 seconds (3.17 M allocations: 508.721 MiB, 5.13% gc time)
Validation loss: 27.860422
Unlabeled attachment score: 50.90400559384677
Labeled attachment score: 42.41334532014784
INFO: Backing up

Epoch 7
10 / 55
20 / 55
30 / 55
40 / 55
50 / 55
  9.311827 seconds (15.10 M allocations: 2.394 GiB, 15.70% gc time)
Training loss: 27.461775


INFO: Computing validation performance
10 / 49
20 / 49
30 / 49
40 / 49
  2.977051 seconds (3.17 M allocations: 508.714 MiB, 4.65% gc time)
Validation loss: 27.418089
Unlabeled attachment score: 50.12486265108381
Labeled attachment score: 41.794026570772154
INFO: Backing up

Epoch 8
10 / 55
20 / 55
  C-c C-c^CERROR: InterruptException:
Stacktrace:
 [1] backward_pass(::AutoGrad.Rec{Array{Any,1}}, ::AutoGrad.Rec{Float32}, ::Array{AutoGrad.Node,1}) at /scratch/users/cgumeli/.julia/v0.6/AutoGrad/src/core.jl:256
 [2] (::AutoGrad.##gradfun#4#6{#loss,Int64})(::Array{Any,1}, ::Function, ::Array{Any,1}, ::Vararg{Any,N} where N) at /scratch/users/cgumeli/.julia/v0.6/AutoGrad/src/core.jl:62
 [3] (::AutoGrad.#gradfun#5)(::Array{Any,1}, ::Vararg{Any,N} where N) at /scratch/users/cgumeli/.julia/v0.6/AutoGrad/src/core.jl:57
 [4] macro expansion at /scratch/users/cgumeli/Nubik/main2.jl:194 [inlined]
 [5] macro expansion at ./util.jl:237 [inlined]
 [6] train() at /scratch/users/cgumeli/Nubik/main2.jl:192
 [7] main(::String) at /scratch/users/cgumeli/Nubik/main2.jl:83

julia> include("main2.jl")
include("main2.jl")
INFO: Checking dependencies
INFO: All dependencies are installed
WARNING: redefining constant UPOSTAG
WARNING: redefining constant UDEPREL
WARNING: redefining constant lmdir
WARNING: redefining constant datadir
WARNING: redefining constant engtrn
WARNING: redefining constant engdev
WARNING: redefining constant englm
WARNING: redefining constant trtrn
WARNING: redefining constant trdev
WARNING: redefining constant trlm
accuracy_bt

julia> main("--datafiles $trtrn $trdev --lmfile $trlm --upos 50 --birnnhidden 200 --lmdrop 0.33 --birnndrop 0.33 --decdrop 0.33 --tokens 500 --birnn 3 --arcweight 0.5")
main("--datafiles $trtrn $trdev --lmfile $trlm --upos 50 --birnnhidden 200 --lmdrop 0.33 --birnndrop 0.33 --decdrop 0.33 --tokens 500 --birnn 3 --arcweight 0.5")
Tuple{Symbol,Any}[(:feat, 0), (:birnn, 3), (:birnndrop, 0.33), (:lmfile, "/scratch/users/okirnap/ud-treebanks-v2.2/chmodel_converted/turkish_chmodel.jld"), (:min, 2), (:birnncell, "LSTM"), (:labelunits, 100), (:xpos, 0), (:batchsize, 8), (:max, 128), (:decdrop, 0.33), (:upos, 50), (:backupdir, "Backups_2018-06-09T01:48:21.003"), (:beta1, 0.9), (:remb, 350), (:algo, "edmonds"), (:beta2, 0.999), (:lmdrop, 0.33), (:proj, true), (:arcweight, 0.5), (:tokens, 500), (:epochs, 100), (:recover, ""), (:birnnhidden, 200), (:optim, "Adam"), (:lr, 0.001), (:datafiles, Any["/scratch/users/okirnap/ud-treebanks-v2.2/UD_Turkish-IMST/tr_imst-ud-train.conllu", "/scratch/users/okirnap/ud-treebanks-v2.2/UD_Turkish-IMST/tr_imst-ud-dev.conllu"]), (:arcunits, 400)]
INFO: Loading data
01:48:21 /scratch/users/okirnap/ud-treebanks-v2.2/chmodel_converted/turkish_chmodel.jld 
01:48:23 /scratch/users/okirnap/ud-treebanks-v2.2/UD_Turkish-IMST/tr_imst-ud-train.conllu 
01:48:24 /scratch/users/okirnap/ud-treebanks-v2.2/UD_Turkish-IMST/tr_imst-ud-dev.conllu 
  C-c C-c^CERROR: InterruptException:
Stacktrace:
 [1] Knet.KnetPtr(::Int64) at /scratch/users/cgumeli/.julia/v0.6/Knet/src/kptr.jl:84
 [2] broadcast#sigm(::Knet.KnetArray{Float32,2}) at /scratch/users/cgumeli/.julia/v0.6/Knet/src/unary.jl:83
 [3] sigm(::AutoGrad.Broadcasted{Knet.KnetArray{Float32,2}}) at /scratch/users/cgumeli/.julia/v0.6/Knet/src/unfuse.jl:24
 [4] broadcast(::Function, ::Knet.KnetArray{Float32,2}) at /scratch/users/cgumeli/.julia/v0.6/Knet/src/unfuse.jl:9
 [5] #_lstm#134(::Knet.KnetArray{Float32,1}, ::Function, ::Knet.KnetArray{Float32,2}, ::Knet.KnetArray{Float32,2}, ::Knet.KnetArray{Float32,2}, ::Knet.KnetArray{Float32,2}, ::Knet.KnetArray{Float32,2}) at /scratch/users/cgumeli/Nubik/./ku-dependency-parser2/src/lm_utils.jl:151
 [6] (::#kw##_lstm)(::Array{Any,1}, ::#_lstm, ::Knet.KnetArray{Float32,2}, ::Knet.KnetArray{Float32,2}, ::Knet.KnetArray{Float32,2}, ::Knet.KnetArray{Float32,2}, ::Knet.KnetArray{Float32,2}) at ./<missing>:0
 [7] charlstm(::Array{Knet.KnetArray{Float32,2},1}, ::Array{Array{Int64,1},1}, ::Array{Array{Float32,1},1}) at /scratch/users/cgumeli/Nubik/./ku-dependency-parser2/src/lm_utils.jl:180
 [8] #fillvecs!#141(::Int64, ::Function, ::Array{Knet.KnetArray{Float32,2},1}, ::Array{Any,1}, ::Vocab) at /scratch/users/cgumeli/Nubik/./ku-dependency-parser2/src/lm_utils.jl:308
 [9] #depmain#171(::Bool, ::Function, ::String) at /scratch/users/cgumeli/Nubik/./ku-dependency-parser2/train.jl:55
 [10] load_data(::SubString{String}, ::SubString{String}, ::SubString{String}) at /scratch/users/cgumeli/Nubik/./data.jl:14
 [11] main(::String) at /scratch/users/cgumeli/Nubik/main2.jl:74

julia> exit()
exit()
bash-4.2$ exit
exit
exit
[cgumeli@login03 cgumeli]$ 